[
  {
    "objectID": "visualizations.html",
    "href": "visualizations.html",
    "title": "Quick Visualizations",
    "section": "",
    "text": "Visualizing data is a key step in any analysis. R provides powerful and flexible graphing tools, whether you are just starting to investigate the structure of your data, or polishing off the perfect figure for publication in a ‘tabloid’ journal like Science (https://www.science.org/journal/science) or Nature (https://www.nature.com/).\nIn this tutorial, you will learn how to make quick graphs with the ggplot() function from the ggplot2 package. We will go over some options for customizing the look and layout that will allow you to produce professional-grade graphs. In the next chapter you will learn some additional tricks and resources for developing your graphing skills even further.\nBy the time you are finished these two self-tutorials, you will have all the resources you need to make publication-ready graphics!\nBoth ggplot functions come from the ggplot2 package. It was developed by R Superstar Hadley Wickham and the team at posit.co, who also made R Studio, Quarto, shiny, dplyr, and the rest of the Tidyverse (https://www.tidyverse.org/) universe of helpful R packages.\n\nQuestion: What happened to ggplot1?\n\nOnce you have mastered these tutorials, you might want to continue to expand your ggplot repertoire by reading through additional examples in the ggplot2 documentation: http://docs.ggplot2.org/current/\nWARNING: There is a learning curve for graphing in R! Learning visualizations in R can feel like a struggle at first, and you may ask yourself: Is it worth my time?\nIf you already have experience making figures with point-and-click graphics programs, you may ask yourself: Why deal with all these coding errors when I can just generate a quick figure in a different program?\nThere are a few good reasons to invest the time to get over the learning curve and use R for all your graphing needs.\n\nYou will get much faster with practice.\nYou have much more control over every aspect of your figure.\nYour visualization will be reproducible, meaning anyone with the data and the code can reproduce every aspect of your figure, right down to each individual data point the formatting of each axis label.\n\nThe third point is worth some extra thought. Everybody makes mistakes, whether you are graphing with R or a point-and-click graphics program. If you make a mistake in a point-and-click program, you may produce a graph that is incorrect with no way to check! If you make an error in R you will either get an error message telling you, or you will have reproducible code that you can share with somebody who can check your work – even if that somebody is you, six months in the future. In this way, reproducibility is quality – an attractive but non-reproducible figure is a low-quality figure.\nIn addition to quality and scientific rigour, there is a more practical reason to value reproducible code. Consider what happens as you collect new data or find a mistake in your original data that needs to be corrected. With a point-and-click program you have to make the graph all over again. With R, you just rerun the code with the new input and get the updated figures and analyses! Later, we will learn how to incorporate code for figures along with statistical analysis into fully reproducible reports with output as .html (Website), .pdf (Adobe Acrobat), and .docx (Microsoft Word)\n\n\n\nBefore we dive into coding for visualizations, there are a few universal graphing concepts that are important to understand in order to create publication-ready graphics in R: file formats, pixel dimension, screen vs print colours, and accessibility.\n\n\nThere are many different file formats that you can use to save individual graphs. Each format has a different suffix or extension in the file name like .jpg, .png, or .pdf. Once saved, you can send these to graphics programs for minor tweaks, or you can send them directly to academic journals for final publication.\nImportantly, file formats for visualizations fall into two main classes: Raster and Vector.\nRaster files save graphs in a 2-dimensional grid of data corresponding to pixel location and colour. Imagine breaking up your screen into a large data.frame object with each pixel represented by a cell, and the value of each cell holding information about the colour and intensity of the pixel. You are probably quite familiar with this ‘pixel art’ format if you’ve ever worked with a digital photo or played a retro video game made before 1993. Some popular Raster file types include JPEG/JPG, PNG, TIFF, and BMP.\nVector files save information about shapes. Instead of tracking every single pixel, the data are encoded as coloured points mapped onto a two-dimensional plane, with points connected by straight or curved lines. To generate the image, the computer must plot out all of the points and lines, and then translate that information to pixels that display on your screen.\nIf you’ve ever drawn a shape in a program like Microsoft Powerpoint or Adobe Illustrator, you might have some sense of how this works. Some popular vector formats include SVG, PDF, EPS, AI, PS.\nSo, why does this matter?\nIn most cases, you should save your visualizations as vector files. SVG is a good choice, because it can be interpreted by web browsers and it is not proprietary. PDF and EPS are proprietary, from the Adobe company. But they are commonly used by publishers and can be viewed on most computers after installing free software.\nSaving your graphs as a vector format allows you (or the journal proof editor) to easily scale your image while maintaining crisp, clear lines. This is because the shapes themselves are tracked, so scaling just expands or contracts the x- and y-axes.\nIn contrast, if you expand a raster file, your computer has to figure out how to expand each pixel. This introduces blurriness or other artifacts. You have seen some images that look ‘pixelated’ – this happens when you try to expand the size of a lower-resolution figure. This also happens when a program compresses an image to save space – the computer program is trying to reduce the data content by reducing the dimensionality of the image.\nIn summary, vector images are generally a better format to use when saving your figures because you can rescale to any size and the lines will always be clean and clear. There are a few important exceptions, however.\n\nPhotographs – Photographs captured by a camera are saved in the Raster format and cannot be converted to vector without significant loss of information.\nGrid Data – Raster data are convenient for plotting data that occurs in a grid. This includes most spatial data that is broken up into a geographic grid. However, you may often want to use the vector format for mapping/GIS data so that the overlapping geographical features (e.g. borders, lakes, rivers) remain in the vector format, even if an underlying data layer is a raster object (e.g. global surface temperature in 1 × 1 km squares).\nLarge Data – With some large data applications (e.g. ‘omics’ datasets) a graphing data set may have many millions or even billions of data points or lines. In these cases, the vector file would be too big to use in publication (e.g. several gigabytes) or even too big to open on standard laptop or desktop computers. In this case you might opt for a high-resolution Raster file. On the other hand, you could graph your data using a representative subset of data or using a density grid with colours corresponding to the density of points. In each case, you would keep the vector format to maintain clean lines for the graph axes and labels, even though some of the data is in the raster format.\n\nThe bottom line: you generally want to save your graphics as SVG or PDF files if you plan to publish them.\n\n\n\nRaster map of temperature (blue) overlain with vector drawings of sampling locations (red), scaled by population size, from the Global Garlic Mustard Field Survey\n\n\n\n\n\nIn cases where you do need to use raster images in a publication, pay careful attention to the image’s pixel dimension. You have probably heard about image resolution: For example, a 2 megapixel camera is better than a 1 megapixel camera; a 200 dpi (dots per inch) printer is better than 50 dpi. But when creating and saving raster images, it’s not just the resolution that matters, the image size also determines the quality of the final image. The size and resolution of an image jointly determine its pixel dimension.\n\nExample: An image with 200 dpi that is 1 x 3 inches will have the same pixel dimension of an image with 100 dpi that is 2 x 6 inches. These images will look exactly the same if they are printed at the same size. The pixel dimension, not the resolution per se determines how crisp or pixelated an image looks.\n\n\n\n\nAnother important consideration is the intended audience and whether they will likely view your figures on a computer screen or printed page (or both). Each pixel of your screen has tiny lights that determine the specific colour that is reproduced. The pixels emit different wavelengths from your screen, which overlap in our eyes to produce the different colours that we see. In contrast, printed images get their colour from combinations of ink, which absorb different wavelengths of colour. This is a key distinction! One important consequence of this difference is that your computer monitor can produce a broader range of colours and intensities than a printed page, and therefore some colours on your computer monitor can look very different in print. In print, the intensity of colours are limited by the intensity of the Cyan, Magenta and Yellow inks that are used to reproduce the images. This is called CMYK printing.\nSome programs like Adobe Illustrator have options to limit the screen to display only those colours that can be reproduced with CMYK printing.\n\n\n\nAnother important consideration about your choice of colours involves your audience. Remember that many cultures have particular intuitions about colours that can cause confusion if your choice does not match these expectations. For example, in Western European cultures, the red spectrum colours (red, orange, yellow) are often associated with ‘hot’ or ‘danger’ while blue spectrum (blue, cyan, purple) are more associated with ‘cold’ or ‘calm’. Given these biases, imagine how confusing it would be to look at a weather map that used blue for hot temperatures and red for cool temperatures.\nIn addition to cultural biases, a significant portion of the population has some form of colour-blindness that limits their ability to see certain colours. This article in Nature has a good explanation with tips for making inclusive figures: https://www.nature.com/articles/d41586-021-02696-z\nThe Simulated Colour Blind Palettes Figure shows a simulation of colour blindness, written in R. Note how certain reds and blue/purple are indistinguishable. A good strategy is to use different intensity as well as different spectra.\n\nNote: If you are reading a black-and-white version of this book, this link will take you to a colour version of the image:\n\nhttps://github.com/ColauttiLab/RCrashCourse_Book/blob/master/images/colorblind.png\n\n\n\nSimulated colourblind palettes.\n\n\nThere is also a more practical reason for this. It is common for scientists and students to print your manuscript or published article in black-and-white to read on public transit or during a group discussion. If you choose colours and shading that can be interpreted properly in black and white, then you will avoid confusion with this significant portion of your audience. The viridis package is a good tool for this. See: https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html\n\n\n\n\nInstall the ggplot2 package using the install.packages() function the first time you want to use it. This installs it on your local computer, so you only need to do it once – though it is a good idea to re-install periodically to update to the most recent version.\n\ninstall.packages(\"ggplot2\")\n\nOnce it is installed, you still need to load it with the library function if you want to use it in your code.\n\nlibrary(ggplot2)\n\n\n\nWe will again be working with the FallopiaData.csv dataset, which can be downloaded here: https://colauttilab.github.io/RCrashCourse/FallopiaData.csv\nSave this text file to a folder called Data inside your project folder. Be sure to save it as FallopiaData.csv and make sure the .csv is included at the end of the name.\nRemember that you can find your current working folder with the getwd() function. You may want to make a new R Project directory as discussed in the R Fundamentals chapter.\nNow load the .csv file into R as a data.frame object:\n\nMyData&lt;-read.csv(\"./Data/FallopiaData.csv\", header=T)\n\nAlternatively, you can load the file right from the internet:\n\nMyData&lt;-read.csv(\n  \"https://colauttilab.github.io/RCrashCourse/FallopiaData.csv\")\n\nThis dataset comes from the research group of Dr. Oliver Bossdorf at the University of Tübingen in Tübingen, Germany. It was published as part of a paper by Parepa and Bossdof Testing for allelopathy in invasive plants: it all depends on the substrate! (Biological Invasions, 2016), which you can find here: https://doi.org/10.1007/s10530-016-1189-z\n\nNote: Tübingen is a historic university in wonderful little town on the Neckar River. Let’s inspect the data.frame that was created from the data, to see what kind of data we are working with.\n\n\n\n\nPunting on the Neckar river in Tübingen\n\n\n\nstr(MyData)\n\n'data.frame':   123 obs. of  13 variables:\n $ PotNum      : int  1 2 3 5 6 7 8 9 10 11 ...\n $ Scenario    : chr  \"low\" \"low\" \"low\" \"low\" ...\n $ Nutrients   : chr  \"low\" \"low\" \"low\" \"low\" ...\n $ Taxon       : chr  \"japon\" \"japon\" \"japon\" \"japon\" ...\n $ Symphytum   : num  9.81 8.64 2.65 1.44 9.15 ...\n $ Silene      : num  36.4 29.6 36 21.4 23.9 ...\n $ Urtica      : num  16.08 5.59 17.09 12.39 5.19 ...\n $ Geranium    : num  4.68 5.75 5.13 5.37 0 9.05 3.51 9.64 7.3 6.36 ...\n $ Geum        : num  0.12 0.55 0.09 0.31 0.17 0.97 0.4 0.01 0.47 0.33 ...\n $ All_Natives : num  67 50.2 61 40.9 38.4 ...\n $ Fallopia    : num  0.01 0.04 0.09 0.77 3.4 0.54 2.05 0.26 0 0 ...\n $ Total       : num  67.1 50.2 61.1 41.7 41.8 ...\n $ Pct_Fallopia: num  0.01 0.08 0.15 1.85 8.13 1.12 3.7 0.61 0 0 ...\n\n\nThe data come from a plant competition experiment involving two invasive species from the genus Fallopia. These species were grown in planting pots in competition with several other species. The first four columns give information about the pot and treatments (Taxon = species of Fallopia. The rest give biomass measurements for each species. Each column name is the genus of a plant grown in the same pot.\n\n\n\n\nThink back to the R Fundamentals Chapter, and you will hopefully recall the different data types represented by the columns of our data. For graphing purposes, there are really just two main types of data: categorical and continuous. Putting these together in different combinations with ggplot() gives us different default graph types.\nEach ggplot() function requires two main components:\n\nThe ggplot() function defines the input data structure. This usually includes a nested aesthetic function aes() to define the plotting variables and a data= parameter to define the input data.\nthe geom_&lt;name&gt;() function defines the output geometry\n\nNote that the &lt;name&gt; denotes a variable name that differs depending on the geography used to map the data. A detailed example of a complex ggplot graph is covered in more detail in the next chapter. For now, we’ll look at the most common visualizations, organized by input data type.\nSpecifically, we’ll consider effective graphs for visualizing one or two variables, each of which may be categorical or continuous.\n\n\nUsually when we only have a single continuous variable to graph, then we are interested in looking at the frequency distribution of values. This is called a frequency histogram. The frequency histogram shows the distribution of data – how many observations (y-axis) for a particular range of values or bins (x-axis).\nIt is very common to plot histograms for all of your variables before running any kind of statistical model to check for outliers and the distribution type. This is covered in the book R STATS Crash Course for Biologists. Looking at the histograms is a good way to look for potential coding errors (e.g. outliers) and whether data are generally normal or should be transformed to meet the assumptions of our statistical models.\nHistogram\n\nggplot(aes(x=Total), data=MyData) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nNote the two main components of our code: the ggplot() function defines the input data and the geom_histogram() function provides the mapping function.\nAlso note the warning message about binwidth in stat_bin(). This is not a problem, it is just R telling us that it chose a default value that may not be idea. We can try different values of binwidth= in the geom_histogram() function to specify the width of the bins along the x-axis. We’ll look at this in more detail later.\nYou can also see we get a warning message about stat_bin(). With R we can distinguish warning messages from error messages.\nError messages represent bigger problems and generally occur when the function doesn’t run at all. For example, if we wrote total instead of Total, we would get an error because R is looking for a column in MyData called total, which is not the same as Total with a capital T.\nWarning messages don’t necessarily prevent the function from running, as in this case. However, it is still important to read the warning and understand if it is ok to ignore it. In this case, it is suggesting a different binwidth parameter. We’ll come back to this later when we explore some of the different parameter options.\nDensity\nA density geom is another way to graph the frequency distribution. Instead of bars, a smoothed curve is fit across the bins, and instead of ‘count’ data, an estimate of the probability is shown on the y-axis.\n\nggplot(aes(x=Total), data=MyData) + geom_density()\n\n\n\n\n\n\n\n\nNotice that the y-axis says ‘density’, not probability? This curve is called a probability density function and we can calculate the probability of observing a value between two points along the x-axis by calculating the area under the curve for those two points. For example, integrating the total area under the curve should equal to a probability of 1.\n\n\n\nIf we input one variable that is categorical rather than continuous, then we are often most interested in looking at the sample size for each group in the category. In a classic ANOVA for example, you want to make sure you have a similar number of observations for each group.\nInstead of geom_histogram() we use geom_bar()\nBar Graph\n\nggplot(aes(x=Scenario), data=MyData) + geom_bar()\n\n\n\n\n\n\n\n\n\n\n\nIf we have two continuous variables, we are most often interested in the classic bivariate plot, sometimes called a scatter plot.\nThe bivariate plot or scatter plot is the ‘meat and potatoes’ of data visualization. By plotting two variables we can see if they are independent (‘shotgun’ pattern) or have some degree of correlation (oval sloped up or down). We can also look for outliers, which would be seen as isolated points that are far away from the main ‘cloud’ of points.\n\nggplot(aes(x=Silene, y=Total), data=MyData) + geom_point()\n\n\n\n\n\n\n\n\n\n\n\nPlotting two categorical variables is not so useful. Instead, we are better off making a summary table as described in detail the Data Science Chapter. Or, we can use the length function with aggregate() as introduced in the previous chapter:\n\naggregate(Total ~ Nutrients:Scenario, data=MyData, length) \n\n  Nutrients     Scenario Total\n1      high      extreme    26\n2      high fluctuations    24\n3      high      gradual    25\n4      high         high    23\n5       low          low    25\n\n\nOR we can use the handy table() function if we want to summarize the variables as rows and columns:\n\ntable(MyData$Scenario,MyData$Nutrients)\n\n              \n               high low\n  extreme        26   0\n  fluctuations   24   0\n  gradual        25   0\n  high           23   0\n  low             0  25\n\n\nIn this case we can see that there is only one class of the “Low” Nutrient treatment, but four classes of “High” Nutrient treatments. In other words, all of the rows with “Low” Nutrient treatment also have the “Low” Scenario, and NONE of the rows with “High” Nutrient treatment have “Low” in the Scenario treatment. However, all groups have similar sample size of about 25. This is because the experiment compared low vs high nutrients, but also looked at different ways that high nutrients could be delivered.\n\n\n\nIf we have a categorical and a continuous variable, we usually want to see the distribution of points for the two variables. There are a few ways to do this:\n\n\nThe box plot is a handy way to quickly inspect a few important characteristics of the data:\n\nmedian: middle horizontal line (i.e. the 50th percentile)\nhinges: top and bottom of the boxes showing the 75th and 25th percentiles, respectively\nwhiskers: vertical lines showing the highest and lowest values (excluding outliers, if present)\noutliers: points showing outlier values more than 1.5 times the inter-quartile range (i.e. 1.5 times the distance from the 25th to 75th percentiles). Note that not all data sets will have outlier points.\n\n\nggplot(aes(x=Nutrients, y=Total), data=MyData) + geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\nViolin plots or Density strips are another popular way to plot these type of data. The ‘violin’ or ‘density’ refers to the smoothed frequency distribution, which is similar the the geom_histogram we saw above, but imagine fitting a smoothed line along the top of each bar, and then turning it on its side and mirroring the image.\nIt can be very helpful to include both the violin and boxplot geoms on the same graph:\n\nggplot(aes(x=Nutrients, y=Total), data=MyData) + \n  geom_violin() + geom_boxplot(width=0.2)\n\n\n\n\n\n\n\n\n\n\n\nThe dotplot stacks points of similar value. It’s particularly useful for smaller datasets where the smooting of the density function may be unreliable. There are a couple of options here:\n\nYou can use dotplots for individual variables\n\n\nggplot(aes(x=Total), data=MyData) + \n  geom_dotplot(binwidth=2)\n\n\n\n\n\n\n\n\n\nOr for continuous-by-categorical (but note the extra parameters)\n\n\nggplot(aes(x=Nutrients, y=Total), data=MyData) + \n  geom_dotplot(binaxis=\"y\", binwidth=2)\n\n\n\n\n\n\n\n\n\n\n\n\nThat’s it! That’s all you need to start exploring your data! Load your data frame, and plot different combinations of variables to look at the distribution of values or the relationship between your variables.\nOnce you are comfortable producing these plots with different data types, you might start thinking about how to improve the appearance of your graphs. Once you understand these basic data types, you can explore how to customize and improve their appearance."
  },
  {
    "objectID": "visualizations.html#introduction",
    "href": "visualizations.html#introduction",
    "title": "Quick Visualizations",
    "section": "",
    "text": "Visualizing data is a key step in any analysis. R provides powerful and flexible graphing tools, whether you are just starting to investigate the structure of your data, or polishing off the perfect figure for publication in a ‘tabloid’ journal like Science (https://www.science.org/journal/science) or Nature (https://www.nature.com/).\nIn this tutorial, you will learn how to make quick graphs with the ggplot() function from the ggplot2 package. We will go over some options for customizing the look and layout that will allow you to produce professional-grade graphs. In the next chapter you will learn some additional tricks and resources for developing your graphing skills even further.\nBy the time you are finished these two self-tutorials, you will have all the resources you need to make publication-ready graphics!\nBoth ggplot functions come from the ggplot2 package. It was developed by R Superstar Hadley Wickham and the team at posit.co, who also made R Studio, Quarto, shiny, dplyr, and the rest of the Tidyverse (https://www.tidyverse.org/) universe of helpful R packages.\n\nQuestion: What happened to ggplot1?\n\nOnce you have mastered these tutorials, you might want to continue to expand your ggplot repertoire by reading through additional examples in the ggplot2 documentation: http://docs.ggplot2.org/current/\nWARNING: There is a learning curve for graphing in R! Learning visualizations in R can feel like a struggle at first, and you may ask yourself: Is it worth my time?\nIf you already have experience making figures with point-and-click graphics programs, you may ask yourself: Why deal with all these coding errors when I can just generate a quick figure in a different program?\nThere are a few good reasons to invest the time to get over the learning curve and use R for all your graphing needs.\n\nYou will get much faster with practice.\nYou have much more control over every aspect of your figure.\nYour visualization will be reproducible, meaning anyone with the data and the code can reproduce every aspect of your figure, right down to each individual data point the formatting of each axis label.\n\nThe third point is worth some extra thought. Everybody makes mistakes, whether you are graphing with R or a point-and-click graphics program. If you make a mistake in a point-and-click program, you may produce a graph that is incorrect with no way to check! If you make an error in R you will either get an error message telling you, or you will have reproducible code that you can share with somebody who can check your work – even if that somebody is you, six months in the future. In this way, reproducibility is quality – an attractive but non-reproducible figure is a low-quality figure.\nIn addition to quality and scientific rigour, there is a more practical reason to value reproducible code. Consider what happens as you collect new data or find a mistake in your original data that needs to be corrected. With a point-and-click program you have to make the graph all over again. With R, you just rerun the code with the new input and get the updated figures and analyses! Later, we will learn how to incorporate code for figures along with statistical analysis into fully reproducible reports with output as .html (Website), .pdf (Adobe Acrobat), and .docx (Microsoft Word)"
  },
  {
    "objectID": "visualizations.html#graphical-concepts",
    "href": "visualizations.html#graphical-concepts",
    "title": "Quick Visualizations",
    "section": "",
    "text": "Before we dive into coding for visualizations, there are a few universal graphing concepts that are important to understand in order to create publication-ready graphics in R: file formats, pixel dimension, screen vs print colours, and accessibility.\n\n\nThere are many different file formats that you can use to save individual graphs. Each format has a different suffix or extension in the file name like .jpg, .png, or .pdf. Once saved, you can send these to graphics programs for minor tweaks, or you can send them directly to academic journals for final publication.\nImportantly, file formats for visualizations fall into two main classes: Raster and Vector.\nRaster files save graphs in a 2-dimensional grid of data corresponding to pixel location and colour. Imagine breaking up your screen into a large data.frame object with each pixel represented by a cell, and the value of each cell holding information about the colour and intensity of the pixel. You are probably quite familiar with this ‘pixel art’ format if you’ve ever worked with a digital photo or played a retro video game made before 1993. Some popular Raster file types include JPEG/JPG, PNG, TIFF, and BMP.\nVector files save information about shapes. Instead of tracking every single pixel, the data are encoded as coloured points mapped onto a two-dimensional plane, with points connected by straight or curved lines. To generate the image, the computer must plot out all of the points and lines, and then translate that information to pixels that display on your screen.\nIf you’ve ever drawn a shape in a program like Microsoft Powerpoint or Adobe Illustrator, you might have some sense of how this works. Some popular vector formats include SVG, PDF, EPS, AI, PS.\nSo, why does this matter?\nIn most cases, you should save your visualizations as vector files. SVG is a good choice, because it can be interpreted by web browsers and it is not proprietary. PDF and EPS are proprietary, from the Adobe company. But they are commonly used by publishers and can be viewed on most computers after installing free software.\nSaving your graphs as a vector format allows you (or the journal proof editor) to easily scale your image while maintaining crisp, clear lines. This is because the shapes themselves are tracked, so scaling just expands or contracts the x- and y-axes.\nIn contrast, if you expand a raster file, your computer has to figure out how to expand each pixel. This introduces blurriness or other artifacts. You have seen some images that look ‘pixelated’ – this happens when you try to expand the size of a lower-resolution figure. This also happens when a program compresses an image to save space – the computer program is trying to reduce the data content by reducing the dimensionality of the image.\nIn summary, vector images are generally a better format to use when saving your figures because you can rescale to any size and the lines will always be clean and clear. There are a few important exceptions, however.\n\nPhotographs – Photographs captured by a camera are saved in the Raster format and cannot be converted to vector without significant loss of information.\nGrid Data – Raster data are convenient for plotting data that occurs in a grid. This includes most spatial data that is broken up into a geographic grid. However, you may often want to use the vector format for mapping/GIS data so that the overlapping geographical features (e.g. borders, lakes, rivers) remain in the vector format, even if an underlying data layer is a raster object (e.g. global surface temperature in 1 × 1 km squares).\nLarge Data – With some large data applications (e.g. ‘omics’ datasets) a graphing data set may have many millions or even billions of data points or lines. In these cases, the vector file would be too big to use in publication (e.g. several gigabytes) or even too big to open on standard laptop or desktop computers. In this case you might opt for a high-resolution Raster file. On the other hand, you could graph your data using a representative subset of data or using a density grid with colours corresponding to the density of points. In each case, you would keep the vector format to maintain clean lines for the graph axes and labels, even though some of the data is in the raster format.\n\nThe bottom line: you generally want to save your graphics as SVG or PDF files if you plan to publish them.\n\n\n\nRaster map of temperature (blue) overlain with vector drawings of sampling locations (red), scaled by population size, from the Global Garlic Mustard Field Survey\n\n\n\n\n\nIn cases where you do need to use raster images in a publication, pay careful attention to the image’s pixel dimension. You have probably heard about image resolution: For example, a 2 megapixel camera is better than a 1 megapixel camera; a 200 dpi (dots per inch) printer is better than 50 dpi. But when creating and saving raster images, it’s not just the resolution that matters, the image size also determines the quality of the final image. The size and resolution of an image jointly determine its pixel dimension.\n\nExample: An image with 200 dpi that is 1 x 3 inches will have the same pixel dimension of an image with 100 dpi that is 2 x 6 inches. These images will look exactly the same if they are printed at the same size. The pixel dimension, not the resolution per se determines how crisp or pixelated an image looks.\n\n\n\n\nAnother important consideration is the intended audience and whether they will likely view your figures on a computer screen or printed page (or both). Each pixel of your screen has tiny lights that determine the specific colour that is reproduced. The pixels emit different wavelengths from your screen, which overlap in our eyes to produce the different colours that we see. In contrast, printed images get their colour from combinations of ink, which absorb different wavelengths of colour. This is a key distinction! One important consequence of this difference is that your computer monitor can produce a broader range of colours and intensities than a printed page, and therefore some colours on your computer monitor can look very different in print. In print, the intensity of colours are limited by the intensity of the Cyan, Magenta and Yellow inks that are used to reproduce the images. This is called CMYK printing.\nSome programs like Adobe Illustrator have options to limit the screen to display only those colours that can be reproduced with CMYK printing.\n\n\n\nAnother important consideration about your choice of colours involves your audience. Remember that many cultures have particular intuitions about colours that can cause confusion if your choice does not match these expectations. For example, in Western European cultures, the red spectrum colours (red, orange, yellow) are often associated with ‘hot’ or ‘danger’ while blue spectrum (blue, cyan, purple) are more associated with ‘cold’ or ‘calm’. Given these biases, imagine how confusing it would be to look at a weather map that used blue for hot temperatures and red for cool temperatures.\nIn addition to cultural biases, a significant portion of the population has some form of colour-blindness that limits their ability to see certain colours. This article in Nature has a good explanation with tips for making inclusive figures: https://www.nature.com/articles/d41586-021-02696-z\nThe Simulated Colour Blind Palettes Figure shows a simulation of colour blindness, written in R. Note how certain reds and blue/purple are indistinguishable. A good strategy is to use different intensity as well as different spectra.\n\nNote: If you are reading a black-and-white version of this book, this link will take you to a colour version of the image:\n\nhttps://github.com/ColauttiLab/RCrashCourse_Book/blob/master/images/colorblind.png\n\n\n\nSimulated colourblind palettes.\n\n\nThere is also a more practical reason for this. It is common for scientists and students to print your manuscript or published article in black-and-white to read on public transit or during a group discussion. If you choose colours and shading that can be interpreted properly in black and white, then you will avoid confusion with this significant portion of your audience. The viridis package is a good tool for this. See: https://cran.r-project.org/web/packages/viridis/vignettes/intro-to-viridis.html"
  },
  {
    "objectID": "visualizations.html#getting-started",
    "href": "visualizations.html#getting-started",
    "title": "Quick Visualizations",
    "section": "",
    "text": "Install the ggplot2 package using the install.packages() function the first time you want to use it. This installs it on your local computer, so you only need to do it once – though it is a good idea to re-install periodically to update to the most recent version.\n\ninstall.packages(\"ggplot2\")\n\nOnce it is installed, you still need to load it with the library function if you want to use it in your code.\n\nlibrary(ggplot2)\n\n\n\nWe will again be working with the FallopiaData.csv dataset, which can be downloaded here: https://colauttilab.github.io/RCrashCourse/FallopiaData.csv\nSave this text file to a folder called Data inside your project folder. Be sure to save it as FallopiaData.csv and make sure the .csv is included at the end of the name.\nRemember that you can find your current working folder with the getwd() function. You may want to make a new R Project directory as discussed in the R Fundamentals chapter.\nNow load the .csv file into R as a data.frame object:\n\nMyData&lt;-read.csv(\"./Data/FallopiaData.csv\", header=T)\n\nAlternatively, you can load the file right from the internet:\n\nMyData&lt;-read.csv(\n  \"https://colauttilab.github.io/RCrashCourse/FallopiaData.csv\")\n\nThis dataset comes from the research group of Dr. Oliver Bossdorf at the University of Tübingen in Tübingen, Germany. It was published as part of a paper by Parepa and Bossdof Testing for allelopathy in invasive plants: it all depends on the substrate! (Biological Invasions, 2016), which you can find here: https://doi.org/10.1007/s10530-016-1189-z\n\nNote: Tübingen is a historic university in wonderful little town on the Neckar River. Let’s inspect the data.frame that was created from the data, to see what kind of data we are working with.\n\n\n\n\nPunting on the Neckar river in Tübingen\n\n\n\nstr(MyData)\n\n'data.frame':   123 obs. of  13 variables:\n $ PotNum      : int  1 2 3 5 6 7 8 9 10 11 ...\n $ Scenario    : chr  \"low\" \"low\" \"low\" \"low\" ...\n $ Nutrients   : chr  \"low\" \"low\" \"low\" \"low\" ...\n $ Taxon       : chr  \"japon\" \"japon\" \"japon\" \"japon\" ...\n $ Symphytum   : num  9.81 8.64 2.65 1.44 9.15 ...\n $ Silene      : num  36.4 29.6 36 21.4 23.9 ...\n $ Urtica      : num  16.08 5.59 17.09 12.39 5.19 ...\n $ Geranium    : num  4.68 5.75 5.13 5.37 0 9.05 3.51 9.64 7.3 6.36 ...\n $ Geum        : num  0.12 0.55 0.09 0.31 0.17 0.97 0.4 0.01 0.47 0.33 ...\n $ All_Natives : num  67 50.2 61 40.9 38.4 ...\n $ Fallopia    : num  0.01 0.04 0.09 0.77 3.4 0.54 2.05 0.26 0 0 ...\n $ Total       : num  67.1 50.2 61.1 41.7 41.8 ...\n $ Pct_Fallopia: num  0.01 0.08 0.15 1.85 8.13 1.12 3.7 0.61 0 0 ...\n\n\nThe data come from a plant competition experiment involving two invasive species from the genus Fallopia. These species were grown in planting pots in competition with several other species. The first four columns give information about the pot and treatments (Taxon = species of Fallopia. The rest give biomass measurements for each species. Each column name is the genus of a plant grown in the same pot."
  },
  {
    "objectID": "visualizations.html#basic-graphs",
    "href": "visualizations.html#basic-graphs",
    "title": "Quick Visualizations",
    "section": "",
    "text": "Think back to the R Fundamentals Chapter, and you will hopefully recall the different data types represented by the columns of our data. For graphing purposes, there are really just two main types of data: categorical and continuous. Putting these together in different combinations with ggplot() gives us different default graph types.\nEach ggplot() function requires two main components:\n\nThe ggplot() function defines the input data structure. This usually includes a nested aesthetic function aes() to define the plotting variables and a data= parameter to define the input data.\nthe geom_&lt;name&gt;() function defines the output geometry\n\nNote that the &lt;name&gt; denotes a variable name that differs depending on the geography used to map the data. A detailed example of a complex ggplot graph is covered in more detail in the next chapter. For now, we’ll look at the most common visualizations, organized by input data type.\nSpecifically, we’ll consider effective graphs for visualizing one or two variables, each of which may be categorical or continuous.\n\n\nUsually when we only have a single continuous variable to graph, then we are interested in looking at the frequency distribution of values. This is called a frequency histogram. The frequency histogram shows the distribution of data – how many observations (y-axis) for a particular range of values or bins (x-axis).\nIt is very common to plot histograms for all of your variables before running any kind of statistical model to check for outliers and the distribution type. This is covered in the book R STATS Crash Course for Biologists. Looking at the histograms is a good way to look for potential coding errors (e.g. outliers) and whether data are generally normal or should be transformed to meet the assumptions of our statistical models.\nHistogram\n\nggplot(aes(x=Total), data=MyData) + geom_histogram()\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nNote the two main components of our code: the ggplot() function defines the input data and the geom_histogram() function provides the mapping function.\nAlso note the warning message about binwidth in stat_bin(). This is not a problem, it is just R telling us that it chose a default value that may not be idea. We can try different values of binwidth= in the geom_histogram() function to specify the width of the bins along the x-axis. We’ll look at this in more detail later.\nYou can also see we get a warning message about stat_bin(). With R we can distinguish warning messages from error messages.\nError messages represent bigger problems and generally occur when the function doesn’t run at all. For example, if we wrote total instead of Total, we would get an error because R is looking for a column in MyData called total, which is not the same as Total with a capital T.\nWarning messages don’t necessarily prevent the function from running, as in this case. However, it is still important to read the warning and understand if it is ok to ignore it. In this case, it is suggesting a different binwidth parameter. We’ll come back to this later when we explore some of the different parameter options.\nDensity\nA density geom is another way to graph the frequency distribution. Instead of bars, a smoothed curve is fit across the bins, and instead of ‘count’ data, an estimate of the probability is shown on the y-axis.\n\nggplot(aes(x=Total), data=MyData) + geom_density()\n\n\n\n\n\n\n\n\nNotice that the y-axis says ‘density’, not probability? This curve is called a probability density function and we can calculate the probability of observing a value between two points along the x-axis by calculating the area under the curve for those two points. For example, integrating the total area under the curve should equal to a probability of 1.\n\n\n\nIf we input one variable that is categorical rather than continuous, then we are often most interested in looking at the sample size for each group in the category. In a classic ANOVA for example, you want to make sure you have a similar number of observations for each group.\nInstead of geom_histogram() we use geom_bar()\nBar Graph\n\nggplot(aes(x=Scenario), data=MyData) + geom_bar()\n\n\n\n\n\n\n\n\n\n\n\nIf we have two continuous variables, we are most often interested in the classic bivariate plot, sometimes called a scatter plot.\nThe bivariate plot or scatter plot is the ‘meat and potatoes’ of data visualization. By plotting two variables we can see if they are independent (‘shotgun’ pattern) or have some degree of correlation (oval sloped up or down). We can also look for outliers, which would be seen as isolated points that are far away from the main ‘cloud’ of points.\n\nggplot(aes(x=Silene, y=Total), data=MyData) + geom_point()\n\n\n\n\n\n\n\n\n\n\n\nPlotting two categorical variables is not so useful. Instead, we are better off making a summary table as described in detail the Data Science Chapter. Or, we can use the length function with aggregate() as introduced in the previous chapter:\n\naggregate(Total ~ Nutrients:Scenario, data=MyData, length) \n\n  Nutrients     Scenario Total\n1      high      extreme    26\n2      high fluctuations    24\n3      high      gradual    25\n4      high         high    23\n5       low          low    25\n\n\nOR we can use the handy table() function if we want to summarize the variables as rows and columns:\n\ntable(MyData$Scenario,MyData$Nutrients)\n\n              \n               high low\n  extreme        26   0\n  fluctuations   24   0\n  gradual        25   0\n  high           23   0\n  low             0  25\n\n\nIn this case we can see that there is only one class of the “Low” Nutrient treatment, but four classes of “High” Nutrient treatments. In other words, all of the rows with “Low” Nutrient treatment also have the “Low” Scenario, and NONE of the rows with “High” Nutrient treatment have “Low” in the Scenario treatment. However, all groups have similar sample size of about 25. This is because the experiment compared low vs high nutrients, but also looked at different ways that high nutrients could be delivered.\n\n\n\nIf we have a categorical and a continuous variable, we usually want to see the distribution of points for the two variables. There are a few ways to do this:\n\n\nThe box plot is a handy way to quickly inspect a few important characteristics of the data:\n\nmedian: middle horizontal line (i.e. the 50th percentile)\nhinges: top and bottom of the boxes showing the 75th and 25th percentiles, respectively\nwhiskers: vertical lines showing the highest and lowest values (excluding outliers, if present)\noutliers: points showing outlier values more than 1.5 times the inter-quartile range (i.e. 1.5 times the distance from the 25th to 75th percentiles). Note that not all data sets will have outlier points.\n\n\nggplot(aes(x=Nutrients, y=Total), data=MyData) + geom_boxplot()\n\n\n\n\n\n\n\n\n\n\n\nViolin plots or Density strips are another popular way to plot these type of data. The ‘violin’ or ‘density’ refers to the smoothed frequency distribution, which is similar the the geom_histogram we saw above, but imagine fitting a smoothed line along the top of each bar, and then turning it on its side and mirroring the image.\nIt can be very helpful to include both the violin and boxplot geoms on the same graph:\n\nggplot(aes(x=Nutrients, y=Total), data=MyData) + \n  geom_violin() + geom_boxplot(width=0.2)\n\n\n\n\n\n\n\n\n\n\n\nThe dotplot stacks points of similar value. It’s particularly useful for smaller datasets where the smooting of the density function may be unreliable. There are a couple of options here:\n\nYou can use dotplots for individual variables\n\n\nggplot(aes(x=Total), data=MyData) + \n  geom_dotplot(binwidth=2)\n\n\n\n\n\n\n\n\n\nOr for continuous-by-categorical (but note the extra parameters)\n\n\nggplot(aes(x=Nutrients, y=Total), data=MyData) + \n  geom_dotplot(binaxis=\"y\", binwidth=2)\n\n\n\n\n\n\n\n\n\n\n\n\nThat’s it! That’s all you need to start exploring your data! Load your data frame, and plot different combinations of variables to look at the distribution of values or the relationship between your variables.\nOnce you are comfortable producing these plots with different data types, you might start thinking about how to improve the appearance of your graphs. Once you understand these basic data types, you can explore how to customize and improve their appearance."
  },
  {
    "objectID": "regex2.html",
    "href": "regex2.html",
    "title": "Regular Expressions II",
    "section": "",
    "text": "In this chapter, we look at specific applications and examples of regular expressions. This includes some tips for practicing regular expressions in R Studio, gathering data from online websites, and practice exercises with DNA data.\n\n\n\nAs noted above, there is a steep learning curve to understanding regular expressions. Getting clean Regex code to do what you want can often come down to trial-and-error. Sometimes, it’s impractical to run code and look at the output to see if your Regex code is doing what you want it to do. In R Studio, you test your regular expression in a text file, using the Find command from the Edit menu in R Studio (or Ctl + F on Windows or Cmd + F on MacOS).\nTry opening a text file in R Studio and select Find from the Edit Menu. You’ll see some check boxes at the bottom of the search bar. One of these boxes is Regex. If you check this box (and uncheck all other boxes) you can use the Find command to highlight the text that is selected for your Regex command. Just make sure to use one escape character (\\) for the search bar.\n\nRecall that the double escape characters (\\\\) are specific to the R search string parameter of the regex functions (e.g., sub, gsub)\n\n\n\n\nScraping is a method for collecting data from online sources. In R, we can use the functions readLines and curl(), both from the curl library, to copy data from websites. We can do this because websites with the .html or .xml extension are a special kind of text files.\nFor more advanced applications, we might want to use the rvest package, which is designed for html and xml files. However, we’ll focus here on curl() because we can apply regular expressions to any text files to extract information of interest. Here’s an example where we will scrape a record for the Green Fluorescent Protein (GFP) from the Protein Data Bank (PDB). Note that this is a file with the extension .pdb but this is a human-readable text file that can be opened in any text editor\nFirst, we’ll import the text into an R object.\n\nlibrary(curl) \n\nUsing libcurl 7.84.0 with Schannel\n\n\nYou will have to use install.packages(\"curl\") to download this package to your computer. You only need to do this once but you will have to use library(curl) whenever you want to use the functions, as explained in the R Fundamentals Chapter.\nNow we can download a file to play with.\n\nProt&lt;-readLines(curl(\"http://www.rcsb.org/pdb/files/1ema.pdb\"))\n\nDownload this link to your computer and open with a text file to see what it looks like.\nThis hint is a simple trick to understand what kind of file(s) you are working with.\nThis is a tab-delimited file, which we could import as a data frame using read.delim but we’ll keep it this way to practice our regular expressions.\nThe Prot object we have made is a simple vector of strings, with each cell corresponding to a different row of text:\n\nlength(Prot)\n\n[1] 2363\n\ngrep(\"TITLE\",Prot)\n\n[1] 2\n\n\nWe can pull out the amino acid sequences, which are rows that start with the word ‘ATOM’\n\nAAseq&lt;-Prot[grep(\"^ATOM\",Prot)]\nlength(AAseq)\n\n[1] 1717\n\n\n\nAAseq[1]\n\n\n\n[1] \"ATOM      1  N   SER A   2      28.888   9.409  52.301  1.00 85.05           N  \"\n\n\n\nChallenge: Try to apply what you have learned about regular expressions to isolate the 3-letter amino acid code.\n\nThere are several ways we could do this. Take the time to think about it and give it a try.\nHere’s one good option, since we know it’s a tab-delimited file with the amino acid in the 4th column:\n\ngsub(\"ATOM\\\\t\\\\w+\\\\t\\\\w+\\\\t(\\\\w+).*\",\"\\\\1\",AAseq[1])\n\n\n\n[1] \"ATOM      1  N   SER A   2      28.888   9.409  52.301  1.00 85.05           N  \"\n\n\nThat didn’t work. Sometimes the ‘tabs’ are actually just multiple ‘spaces’\n\nAAchain&lt;-gsub(\"ATOM\\\\s+\\\\w+\\\\s+\\\\w+\\\\s+(\\\\w+).*\",\"\\\\1\",AAseq)\nAAchain[1:100]\n\n  [1] \"SER\" \"SER\" \"SER\" \"SER\" \"SER\" \"SER\" \"LYS\" \"LYS\" \"LYS\" \"LYS\" \"LYS\" \"LYS\"\n [13] \"LYS\" \"LYS\" \"LYS\" \"GLY\" \"GLY\" \"GLY\" \"GLY\" \"GLU\" \"GLU\" \"GLU\" \"GLU\" \"GLU\"\n [25] \"GLU\" \"GLU\" \"GLU\" \"GLU\" \"GLU\" \"GLU\" \"GLU\" \"GLU\" \"GLU\" \"GLU\" \"LEU\" \"LEU\"\n [37] \"LEU\" \"LEU\" \"LEU\" \"LEU\" \"LEU\" \"LEU\" \"PHE\" \"PHE\" \"PHE\" \"PHE\" \"PHE\" \"PHE\"\n [49] \"PHE\" \"PHE\" \"PHE\" \"PHE\" \"PHE\" \"THR\" \"THR\" \"THR\" \"THR\" \"THR\" \"THR\" \"THR\"\n [61] \"GLY\" \"GLY\" \"GLY\" \"GLY\" \"VAL\" \"VAL\" \"VAL\" \"VAL\" \"VAL\" \"VAL\" \"VAL\" \"VAL\"\n [73] \"VAL\" \"VAL\" \"VAL\" \"VAL\" \"VAL\" \"VAL\" \"PRO\" \"PRO\" \"PRO\" \"PRO\" \"PRO\" \"PRO\"\n [85] \"PRO\" \"ILE\" \"ILE\" \"ILE\" \"ILE\" \"ILE\" \"ILE\" \"ILE\" \"ILE\" \"LEU\" \"LEU\" \"LEU\"\n [97] \"LEU\" \"LEU\" \"LEU\" \"LEU\"\n\n\nNow we have a handy vector of amino acids representing our protein.\n\n\n\nLet’s try practising with a couple of examples.\n\n\n\nRegular expressions are also useful with data objects\nImagine you have a repeated measures design. 3 transects (A-C) and 3 positions along each transect (1-3). We can simulate this data by generating dandom numbers in a data frame.\n\nTransect&lt;-data.frame(Species=letters[1:20],\n                     A1=rnorm(20), A2=rnorm(20), A3=rnorm(20),\n                     B1=rnorm(20), B2=rnorm(20) ,B3=rnorm(20),\n                     C1=rnorm(20), C2=rnorm(20), C3=rnorm(20))\nhead(Transect)\n\n  Species         A1         A2         A3          B1          B2          B3\n1       a -0.4762919 -0.8771969 -2.4134562  1.09793390 -0.22686803  0.93908458\n2       b  0.5931594  0.2868677 -1.0057155 -0.48245954 -1.26291078 -0.13815786\n3       c  0.5233956  0.3531411 -0.3127597  0.28584890 -0.03140898 -0.17422685\n4       d -0.1398728 -0.1236981 -0.1956281 -0.01232568  0.41402493  0.02370992\n5       e  0.8294952 -0.5536721  0.7487573 -0.43119837  0.90706270  2.10488473\n6       f  1.0231336  0.2370539 -0.4418742 -0.97054591 -0.49717235  0.38836569\n          C1         C2         C3\n1 -1.3588022 -0.3143200 -1.1155445\n2 -0.7981804  0.2417494  0.2025241\n3 -0.3029387 -1.2293938 -0.2851082\n4  0.2644057 -1.3592029 -0.5699135\n5 -0.1300611  0.3638732 -1.7787850\n6 -0.7249635 -0.2156055 -0.9796530\n\n\n\nTip: the object letters in the code above contains lower-case letter, while LETTERS contains upper case.\n\n\n\nUse your knowledge f regular expressions with subsetting data outlined in the R Fundamentals Chapter to do the following:\n\nSubset only the columns that have an “A” in their name\nSubset the rows of data for species “d”\n\nTake the time to do this on your own. It will take you a while and you might make a lot of mistakes along the way. That’s all part of the learning process. The longer you struggle, the faster you will learn.\nNow here is a more challenging example:\n\n\n\n\nHere is a line of code to import DNA from genbank. (The one line is broken up into three physical lines to make it easier to read)\n\nLythrum_18S&lt;-scan(\n  \"https://colauttilab.github.io/RCrashCourse/sequence.gb\",\n  what=\"character\",sep=\"\\n\")\n\nThis is the sequence of the 18S subunit from the ribosome gene of Lythrum salicaria (from Genbank)\n\nprint(Lythrum_18S)\n\n\n\n [1] \"LOCUS       AF206955                1740 bp    DNA     linear   PLN 18-APR-2003\"\n [2] \"DEFINITION  Lythrum salicaria 18S ribosomal RNA gene, complete sequence.\"       \n [3] \"ACCESSION   AF206955\"                                                           \n [4] \"VERSION     AF206955.1\"                                                         \n [5] \"KEYWORDS    .\"                                                                  \n [6] \"SOURCE      Lythrum salicaria\"                                                  \n [7] \"  ORGANISM  Lythrum salicaria\"                                                  \n [8] \"            Eukaryota; Viridiplantae; Streptophyta; Embryophyta; Tracheophyta;\" \n [9] \"            Spermatophyta; Magnoliopsida; eudicotyledons; Gunneridae;\"          \n[10] \"            Pentapetalae; rosids; malvids; Myrtales; Lythraceae; Lythrum.\"      \n[11] \"REFERENCE   1  (bases 1 to 1740)\"                                               \n[12] \"  AUTHORS   Soltis,P.S., Soltis,D.E. and Chase,M.W.\"                            \n[13] \"  TITLE     Direct Submission\"                                                  \n[14] \"  JOURNAL   Submitted (19-NOV-1999) School of Biological Sciences, Washington\"  \n[15] \"            State University, Pullman, WA 99164-4236, USA\"                      \n[16] \"FEATURES             Location/Qualifiers\"                                       \n[17] \"     source          1..1740\"                                                   \n[18] \"                     /organism=\\\"Lythrum salicaria\\\"\"                           \n[19] \"                     /mol_type=\\\"genomic DNA\\\"\"                                 \n[20] \"                     /db_xref=\\\"taxon:13129\\\"\"                                  \n[21] \"                     /note=\\\"Lythrum salicaria L.\\\"\"                            \n[22] \"     rRNA            1..1740\"                                                   \n[23] \"                     /product=\\\"18S ribosomal RNA\\\"\"                            \n[24] \"ORIGIN      \"                                                                   \n[25] \"        1 gtcatatgct tgtctcaaag attaagccat gcatgtgtaa gtatgaacaa attcagactg\"    \n[26] \"       61 tgaaactgcg aatggctcat taaatcagtt atagtttgtt tgatggtatc tgctactcgg\"    \n[27] \"      121 ataaccgtag taattctaga gctaatacgt gcaacaaacc ccgacttctg gaagggacgc\"    \n[28] \"      181 atttattaga taaaaggtcg acgcgggctt tgcccgatgc tctgatgatt catgataact\"    \n[29] \"      241 tgacggatcg cacggccatc gtgccggcga cgcatcattc aaatttctgc cctatcaact\"    \n[30] \"      301 ttcgatggta ggatagtggc ctaccatggt gtttacgggt aacggagaat tagggttcga\"    \n[31] \"      361 ttccggagag ggagcctgag aaacggctac cacatccaag gaaggcagca ggcgcgcaaa\"    \n[32] \"      421 ttacccaatc ctgacacggg gaggtagtga caataaataa caatactggg ctctttgagt\"    \n[33] \"      481 ctggtaattg gaatgagtac aatctaaatc ccttaacgag gatccattgg agggcaagtc\"    \n[34] \"      541 tggtgccagc agccgcggta attccagctc caatagcgta tatttaagtt gttgcagtta\"    \n[35] \"      601 aaaagctcgt agttggacct tgggttgggt cgaccggtcc gcctttggtg tgcaccgatc\"    \n[36] \"      661 ggctcgtccc ttctaccggc gatgcgcgcc tggccttaat tggccgggtc gttcctccgg\"    \n[37] \"      721 tgctgttact ttgaagaaat tagagtgctc aaagcaagca ttagctatga atacattagc\"    \n[38] \"      781 atgggataac attataggat tccgatccta ttatgttggc cttcgggatc ggagtaatga\"    \n[39] \"      841 ttaacaggga cagtcggggg cattcgtatt tcatagtcag aggtgaaatt cttggattta\"    \n[40] \"      901 tgaaagacga acaactgcga aagcatttgc caaggatgtt ttcattaatc aagaacgaaa\"    \n[41] \"      961 gttgggggct cgaagacgat cagataccgt cctagtctca accataaacg atgccgacca\"    \n[42] \"     1021 gggatcagcg aatgttactt ttaggacttc gctggcacct tatgagaaat caaagttttt\"    \n[43] \"     1081 gggttccggg gggagtatgg tcgcaaggct gaaacttaaa ggaattgacg gaagggcacc\"    \n[44] \"     1141 accaggagtg gagcctgcgg cttaatttga ctcaacacgg ggaaacttac caggtccaga\"    \n[45] \"     1201 catagtaagg attgacagac tgagagctct ttcttgattc tatgggtggt ggtgcatggc\"    \n[46] \"     1261 cgttcttagt tggtggagcg atttgtctgg ttaattccgt taacgaacga gacctcagcc\"    \n[47] \"     1321 tgctaactag ctatgtggag gtacacctcc acggccagct tcttagaggg actatggccg\"    \n[48] \"     1381 cttaggccaa ggaagtttga ggcaataaca ggtctgtgat gcccttagat gttctgggcc\"    \n[49] \"     1441 gcacgcgcgc tacactgatg tattcaacga gtctatagcc ttggccgaca ggcccgggta\"    \n[50] \"     1501 atctttgaaa tttcatcgtg atggggatag atcattgcaa ttgttggtct tcaacgagga\"    \n[51] \"     1561 attcctagta agcgcgagtc atcagctcgc gttgactacg tccctgccct ttgtacacac\"    \n[52] \"     1621 cgcccgtcgc tcctaccgat tgaatggtcc ggtgaaatgt tcggatcgcg gcgacgtggg\"    \n[53] \"     1681 cgcttcgtcg ccgacgacgt cgcgagaagt ccattgaacc ttatcattta gaggaaggag\"    \n[54] \"//\"                                                                             \n\n\nNotice that each line is read in as a separate cell in a vector, with sequences beginning with a number ending with 1. We can take advantage of this to extract just the sequence data\n\n\nBefore we move on, try to do the following:\n\nIsolate only the rows containing DNA sequences. This should include\n\n\n\nRemoving all of the characters that are not a, t, g, or c.\nCombining separate cells/lines into a single string. You can do this with using the paste() function with the collapse=\"\" parameter\n\n\n\nConvert lower-case to upper-case. To do this, you can use:\n\n\ngsub(\"([actg])\",\"\\\\U\\\\1\",Seq,perl=T)\n\nThe \\\\U\\\\\\1 means “paste brackets as upper-case”, and is only available as a Perl command, which is accessible in gsub() with the perl=T parameter.\n\nReplace start codons (ATG) with --&gt;START--&gt;ATG\nInsert &gt;--STOP--| after any stop codons (TAA or TAG or TGA).\n\nTake the time to struggle with this and try different combinations until you find a way through. The more you struggle, the faster you will learn.\nA cool thing about regular expressions is that there is rarely a single right answer, especially for complicated problems. When you are ready, Continue on to see one possible solution.\n\n\n\n\n\n\nSubset only transect A for the first 3 species:\n\nTransect[1:3,grep(\"A\",names(Transect))]\n\n          A1         A2         A3\n1 -0.4762919 -0.8771969 -2.4134562\n2  0.5931594  0.2868677 -1.0057155\n3  0.5233956  0.3531411 -0.3127597\n\n\nSubset the data for the species \"d\".\n\nTransect[grep(\"d\",Transect$Species),]\n\n  Species         A1         A2         A3          B1        B2         B3\n4       d -0.1398728 -0.1236981 -0.1956281 -0.01232568 0.4140249 0.02370992\n         C1        C2         C3\n4 0.2644057 -1.359203 -0.5699135\n\n\n\n\n\nFirst, isolate the DNA sequence:\n\nUse .* with () to delete everything before the DNA sequence\n\n\nSeq&lt;-gsub(\".*(1 [gatc])\",\"\",Lythrum_18S)\n\n\nUse the .* and space with + to eliminate all text before the sequence\n\n\nSeq&lt;-gsub(\".*ORIGIN +\",\"\",paste(Seq,collapse=\"\"))\n\n\nEliminate spaces and the two // at the end\n\n\nSeq&lt;-gsub(\" |//\",\"\",Seq)\n\n\nCapital letters look nicer, but requires a PERL qualifier \\\\U in the replacement string. This is not standard in R regular expressions, but can be accessed with perl=T.\n\n\nSeq&lt;-gsub(\"([actg])\",\"\\\\U\\\\1\",Seq,perl=T)\n\nNow that we have our formatted DNA sequence string, we can highlight start codons.\n\nORFs&lt;-gsub(\"ATG\",\"\\n &lt;ATG&gt;--&gt;START--&gt;\",Seq)\n\nAnd stop codons too.\n\nORFs&lt;-gsub(\"(TAA|TAG|TGA)\",\"&lt;\\\\1&gt;--STOP--| \\n\",Seq)\n\nNote the addition of the newline character (\\n) to make the output more readable. To see the final modified string, use the cat() function to print the string directly, rather than creating an object containing the string:\n\ncat(ORFs)\n\n(Output not shown)\n\n\n\n\nHere are some more exercises to practice your skills. No solutions are given, but you will know if you are correct if you get the desired output.\n\nEmail Spammer: Consider a vector of email addresses scraped from the internet.\n\n\nrobert ‘dot’ colautti ‘at’ queensu ‘dot’ ca\nchris.eckert[at]queensu.ca\nlonnie.aarssen at queensu.ca\n\nUse regular expressions to convert all email addresses to the standard format: name@queensu.ca\n\nGenetic Simulation: Start by creating a random sequence of DNA. Think way back to the R Fundamentals Chapter to find a function that can randomly sample from 4 base pairs to create vector of 1,000 bases.\n\nOnce you have your vector, try collapsing it into a single element (i.e., a string with 1,000 characters).\n\nHint: You can do this with the paste() command. You just need one special parameter.\n\nNow, try each of the following challenges:\n\nReplace T with U.\nFind all start codons (AUG) and stop codons (UAA, UAG, UGA).\nFind all open reading frames (hint: consider each sequence beginning with AUG and ending with a stop codon; how do you know if both sequences are in the same reading frame?).\nCount the length (number of bases) for all open reading frames.\n\n\nRegex Golf\n\nHave fun! https://alf.nu/RegexGolf"
  },
  {
    "objectID": "regex2.html#introduction",
    "href": "regex2.html#introduction",
    "title": "Regular Expressions II",
    "section": "",
    "text": "In this chapter, we look at specific applications and examples of regular expressions. This includes some tips for practicing regular expressions in R Studio, gathering data from online websites, and practice exercises with DNA data."
  },
  {
    "objectID": "regex2.html#regex-in-r-studio",
    "href": "regex2.html#regex-in-r-studio",
    "title": "Regular Expressions II",
    "section": "",
    "text": "As noted above, there is a steep learning curve to understanding regular expressions. Getting clean Regex code to do what you want can often come down to trial-and-error. Sometimes, it’s impractical to run code and look at the output to see if your Regex code is doing what you want it to do. In R Studio, you test your regular expression in a text file, using the Find command from the Edit menu in R Studio (or Ctl + F on Windows or Cmd + F on MacOS).\nTry opening a text file in R Studio and select Find from the Edit Menu. You’ll see some check boxes at the bottom of the search bar. One of these boxes is Regex. If you check this box (and uncheck all other boxes) you can use the Find command to highlight the text that is selected for your Regex command. Just make sure to use one escape character (\\) for the search bar.\n\nRecall that the double escape characters (\\\\) are specific to the R search string parameter of the regex functions (e.g., sub, gsub)"
  },
  {
    "objectID": "regex2.html#scraping",
    "href": "regex2.html#scraping",
    "title": "Regular Expressions II",
    "section": "",
    "text": "Scraping is a method for collecting data from online sources. In R, we can use the functions readLines and curl(), both from the curl library, to copy data from websites. We can do this because websites with the .html or .xml extension are a special kind of text files.\nFor more advanced applications, we might want to use the rvest package, which is designed for html and xml files. However, we’ll focus here on curl() because we can apply regular expressions to any text files to extract information of interest. Here’s an example where we will scrape a record for the Green Fluorescent Protein (GFP) from the Protein Data Bank (PDB). Note that this is a file with the extension .pdb but this is a human-readable text file that can be opened in any text editor\nFirst, we’ll import the text into an R object.\n\nlibrary(curl) \n\nUsing libcurl 7.84.0 with Schannel\n\n\nYou will have to use install.packages(\"curl\") to download this package to your computer. You only need to do this once but you will have to use library(curl) whenever you want to use the functions, as explained in the R Fundamentals Chapter.\nNow we can download a file to play with.\n\nProt&lt;-readLines(curl(\"http://www.rcsb.org/pdb/files/1ema.pdb\"))\n\nDownload this link to your computer and open with a text file to see what it looks like.\nThis hint is a simple trick to understand what kind of file(s) you are working with.\nThis is a tab-delimited file, which we could import as a data frame using read.delim but we’ll keep it this way to practice our regular expressions.\nThe Prot object we have made is a simple vector of strings, with each cell corresponding to a different row of text:\n\nlength(Prot)\n\n[1] 2363\n\ngrep(\"TITLE\",Prot)\n\n[1] 2\n\n\nWe can pull out the amino acid sequences, which are rows that start with the word ‘ATOM’\n\nAAseq&lt;-Prot[grep(\"^ATOM\",Prot)]\nlength(AAseq)\n\n[1] 1717\n\n\n\nAAseq[1]\n\n\n\n[1] \"ATOM      1  N   SER A   2      28.888   9.409  52.301  1.00 85.05           N  \"\n\n\n\nChallenge: Try to apply what you have learned about regular expressions to isolate the 3-letter amino acid code.\n\nThere are several ways we could do this. Take the time to think about it and give it a try.\nHere’s one good option, since we know it’s a tab-delimited file with the amino acid in the 4th column:\n\ngsub(\"ATOM\\\\t\\\\w+\\\\t\\\\w+\\\\t(\\\\w+).*\",\"\\\\1\",AAseq[1])\n\n\n\n[1] \"ATOM      1  N   SER A   2      28.888   9.409  52.301  1.00 85.05           N  \"\n\n\nThat didn’t work. Sometimes the ‘tabs’ are actually just multiple ‘spaces’\n\nAAchain&lt;-gsub(\"ATOM\\\\s+\\\\w+\\\\s+\\\\w+\\\\s+(\\\\w+).*\",\"\\\\1\",AAseq)\nAAchain[1:100]\n\n  [1] \"SER\" \"SER\" \"SER\" \"SER\" \"SER\" \"SER\" \"LYS\" \"LYS\" \"LYS\" \"LYS\" \"LYS\" \"LYS\"\n [13] \"LYS\" \"LYS\" \"LYS\" \"GLY\" \"GLY\" \"GLY\" \"GLY\" \"GLU\" \"GLU\" \"GLU\" \"GLU\" \"GLU\"\n [25] \"GLU\" \"GLU\" \"GLU\" \"GLU\" \"GLU\" \"GLU\" \"GLU\" \"GLU\" \"GLU\" \"GLU\" \"LEU\" \"LEU\"\n [37] \"LEU\" \"LEU\" \"LEU\" \"LEU\" \"LEU\" \"LEU\" \"PHE\" \"PHE\" \"PHE\" \"PHE\" \"PHE\" \"PHE\"\n [49] \"PHE\" \"PHE\" \"PHE\" \"PHE\" \"PHE\" \"THR\" \"THR\" \"THR\" \"THR\" \"THR\" \"THR\" \"THR\"\n [61] \"GLY\" \"GLY\" \"GLY\" \"GLY\" \"VAL\" \"VAL\" \"VAL\" \"VAL\" \"VAL\" \"VAL\" \"VAL\" \"VAL\"\n [73] \"VAL\" \"VAL\" \"VAL\" \"VAL\" \"VAL\" \"VAL\" \"PRO\" \"PRO\" \"PRO\" \"PRO\" \"PRO\" \"PRO\"\n [85] \"PRO\" \"ILE\" \"ILE\" \"ILE\" \"ILE\" \"ILE\" \"ILE\" \"ILE\" \"ILE\" \"LEU\" \"LEU\" \"LEU\"\n [97] \"LEU\" \"LEU\" \"LEU\" \"LEU\"\n\n\nNow we have a handy vector of amino acids representing our protein."
  },
  {
    "objectID": "regex2.html#examples",
    "href": "regex2.html#examples",
    "title": "Regular Expressions II",
    "section": "",
    "text": "Let’s try practising with a couple of examples."
  },
  {
    "objectID": "regex2.html#transect-data",
    "href": "regex2.html#transect-data",
    "title": "Regular Expressions II",
    "section": "",
    "text": "Regular expressions are also useful with data objects\nImagine you have a repeated measures design. 3 transects (A-C) and 3 positions along each transect (1-3). We can simulate this data by generating dandom numbers in a data frame.\n\nTransect&lt;-data.frame(Species=letters[1:20],\n                     A1=rnorm(20), A2=rnorm(20), A3=rnorm(20),\n                     B1=rnorm(20), B2=rnorm(20) ,B3=rnorm(20),\n                     C1=rnorm(20), C2=rnorm(20), C3=rnorm(20))\nhead(Transect)\n\n  Species         A1         A2         A3          B1          B2          B3\n1       a -0.4762919 -0.8771969 -2.4134562  1.09793390 -0.22686803  0.93908458\n2       b  0.5931594  0.2868677 -1.0057155 -0.48245954 -1.26291078 -0.13815786\n3       c  0.5233956  0.3531411 -0.3127597  0.28584890 -0.03140898 -0.17422685\n4       d -0.1398728 -0.1236981 -0.1956281 -0.01232568  0.41402493  0.02370992\n5       e  0.8294952 -0.5536721  0.7487573 -0.43119837  0.90706270  2.10488473\n6       f  1.0231336  0.2370539 -0.4418742 -0.97054591 -0.49717235  0.38836569\n          C1         C2         C3\n1 -1.3588022 -0.3143200 -1.1155445\n2 -0.7981804  0.2417494  0.2025241\n3 -0.3029387 -1.2293938 -0.2851082\n4  0.2644057 -1.3592029 -0.5699135\n5 -0.1300611  0.3638732 -1.7787850\n6 -0.7249635 -0.2156055 -0.9796530\n\n\n\nTip: the object letters in the code above contains lower-case letter, while LETTERS contains upper case.\n\n\n\nUse your knowledge f regular expressions with subsetting data outlined in the R Fundamentals Chapter to do the following:\n\nSubset only the columns that have an “A” in their name\nSubset the rows of data for species “d”\n\nTake the time to do this on your own. It will take you a while and you might make a lot of mistakes along the way. That’s all part of the learning process. The longer you struggle, the faster you will learn.\nNow here is a more challenging example:"
  },
  {
    "objectID": "regex2.html#genbank",
    "href": "regex2.html#genbank",
    "title": "Regular Expressions II",
    "section": "",
    "text": "Here is a line of code to import DNA from genbank. (The one line is broken up into three physical lines to make it easier to read)\n\nLythrum_18S&lt;-scan(\n  \"https://colauttilab.github.io/RCrashCourse/sequence.gb\",\n  what=\"character\",sep=\"\\n\")\n\nThis is the sequence of the 18S subunit from the ribosome gene of Lythrum salicaria (from Genbank)\n\nprint(Lythrum_18S)\n\n\n\n [1] \"LOCUS       AF206955                1740 bp    DNA     linear   PLN 18-APR-2003\"\n [2] \"DEFINITION  Lythrum salicaria 18S ribosomal RNA gene, complete sequence.\"       \n [3] \"ACCESSION   AF206955\"                                                           \n [4] \"VERSION     AF206955.1\"                                                         \n [5] \"KEYWORDS    .\"                                                                  \n [6] \"SOURCE      Lythrum salicaria\"                                                  \n [7] \"  ORGANISM  Lythrum salicaria\"                                                  \n [8] \"            Eukaryota; Viridiplantae; Streptophyta; Embryophyta; Tracheophyta;\" \n [9] \"            Spermatophyta; Magnoliopsida; eudicotyledons; Gunneridae;\"          \n[10] \"            Pentapetalae; rosids; malvids; Myrtales; Lythraceae; Lythrum.\"      \n[11] \"REFERENCE   1  (bases 1 to 1740)\"                                               \n[12] \"  AUTHORS   Soltis,P.S., Soltis,D.E. and Chase,M.W.\"                            \n[13] \"  TITLE     Direct Submission\"                                                  \n[14] \"  JOURNAL   Submitted (19-NOV-1999) School of Biological Sciences, Washington\"  \n[15] \"            State University, Pullman, WA 99164-4236, USA\"                      \n[16] \"FEATURES             Location/Qualifiers\"                                       \n[17] \"     source          1..1740\"                                                   \n[18] \"                     /organism=\\\"Lythrum salicaria\\\"\"                           \n[19] \"                     /mol_type=\\\"genomic DNA\\\"\"                                 \n[20] \"                     /db_xref=\\\"taxon:13129\\\"\"                                  \n[21] \"                     /note=\\\"Lythrum salicaria L.\\\"\"                            \n[22] \"     rRNA            1..1740\"                                                   \n[23] \"                     /product=\\\"18S ribosomal RNA\\\"\"                            \n[24] \"ORIGIN      \"                                                                   \n[25] \"        1 gtcatatgct tgtctcaaag attaagccat gcatgtgtaa gtatgaacaa attcagactg\"    \n[26] \"       61 tgaaactgcg aatggctcat taaatcagtt atagtttgtt tgatggtatc tgctactcgg\"    \n[27] \"      121 ataaccgtag taattctaga gctaatacgt gcaacaaacc ccgacttctg gaagggacgc\"    \n[28] \"      181 atttattaga taaaaggtcg acgcgggctt tgcccgatgc tctgatgatt catgataact\"    \n[29] \"      241 tgacggatcg cacggccatc gtgccggcga cgcatcattc aaatttctgc cctatcaact\"    \n[30] \"      301 ttcgatggta ggatagtggc ctaccatggt gtttacgggt aacggagaat tagggttcga\"    \n[31] \"      361 ttccggagag ggagcctgag aaacggctac cacatccaag gaaggcagca ggcgcgcaaa\"    \n[32] \"      421 ttacccaatc ctgacacggg gaggtagtga caataaataa caatactggg ctctttgagt\"    \n[33] \"      481 ctggtaattg gaatgagtac aatctaaatc ccttaacgag gatccattgg agggcaagtc\"    \n[34] \"      541 tggtgccagc agccgcggta attccagctc caatagcgta tatttaagtt gttgcagtta\"    \n[35] \"      601 aaaagctcgt agttggacct tgggttgggt cgaccggtcc gcctttggtg tgcaccgatc\"    \n[36] \"      661 ggctcgtccc ttctaccggc gatgcgcgcc tggccttaat tggccgggtc gttcctccgg\"    \n[37] \"      721 tgctgttact ttgaagaaat tagagtgctc aaagcaagca ttagctatga atacattagc\"    \n[38] \"      781 atgggataac attataggat tccgatccta ttatgttggc cttcgggatc ggagtaatga\"    \n[39] \"      841 ttaacaggga cagtcggggg cattcgtatt tcatagtcag aggtgaaatt cttggattta\"    \n[40] \"      901 tgaaagacga acaactgcga aagcatttgc caaggatgtt ttcattaatc aagaacgaaa\"    \n[41] \"      961 gttgggggct cgaagacgat cagataccgt cctagtctca accataaacg atgccgacca\"    \n[42] \"     1021 gggatcagcg aatgttactt ttaggacttc gctggcacct tatgagaaat caaagttttt\"    \n[43] \"     1081 gggttccggg gggagtatgg tcgcaaggct gaaacttaaa ggaattgacg gaagggcacc\"    \n[44] \"     1141 accaggagtg gagcctgcgg cttaatttga ctcaacacgg ggaaacttac caggtccaga\"    \n[45] \"     1201 catagtaagg attgacagac tgagagctct ttcttgattc tatgggtggt ggtgcatggc\"    \n[46] \"     1261 cgttcttagt tggtggagcg atttgtctgg ttaattccgt taacgaacga gacctcagcc\"    \n[47] \"     1321 tgctaactag ctatgtggag gtacacctcc acggccagct tcttagaggg actatggccg\"    \n[48] \"     1381 cttaggccaa ggaagtttga ggcaataaca ggtctgtgat gcccttagat gttctgggcc\"    \n[49] \"     1441 gcacgcgcgc tacactgatg tattcaacga gtctatagcc ttggccgaca ggcccgggta\"    \n[50] \"     1501 atctttgaaa tttcatcgtg atggggatag atcattgcaa ttgttggtct tcaacgagga\"    \n[51] \"     1561 attcctagta agcgcgagtc atcagctcgc gttgactacg tccctgccct ttgtacacac\"    \n[52] \"     1621 cgcccgtcgc tcctaccgat tgaatggtcc ggtgaaatgt tcggatcgcg gcgacgtggg\"    \n[53] \"     1681 cgcttcgtcg ccgacgacgt cgcgagaagt ccattgaacc ttatcattta gaggaaggag\"    \n[54] \"//\"                                                                             \n\n\nNotice that each line is read in as a separate cell in a vector, with sequences beginning with a number ending with 1. We can take advantage of this to extract just the sequence data\n\n\nBefore we move on, try to do the following:\n\nIsolate only the rows containing DNA sequences. This should include\n\n\n\nRemoving all of the characters that are not a, t, g, or c.\nCombining separate cells/lines into a single string. You can do this with using the paste() function with the collapse=\"\" parameter\n\n\n\nConvert lower-case to upper-case. To do this, you can use:\n\n\ngsub(\"([actg])\",\"\\\\U\\\\1\",Seq,perl=T)\n\nThe \\\\U\\\\\\1 means “paste brackets as upper-case”, and is only available as a Perl command, which is accessible in gsub() with the perl=T parameter.\n\nReplace start codons (ATG) with --&gt;START--&gt;ATG\nInsert &gt;--STOP--| after any stop codons (TAA or TAG or TGA).\n\nTake the time to struggle with this and try different combinations until you find a way through. The more you struggle, the faster you will learn.\nA cool thing about regular expressions is that there is rarely a single right answer, especially for complicated problems. When you are ready, Continue on to see one possible solution."
  },
  {
    "objectID": "regex2.html#solutions",
    "href": "regex2.html#solutions",
    "title": "Regular Expressions II",
    "section": "",
    "text": "Subset only transect A for the first 3 species:\n\nTransect[1:3,grep(\"A\",names(Transect))]\n\n          A1         A2         A3\n1 -0.4762919 -0.8771969 -2.4134562\n2  0.5931594  0.2868677 -1.0057155\n3  0.5233956  0.3531411 -0.3127597\n\n\nSubset the data for the species \"d\".\n\nTransect[grep(\"d\",Transect$Species),]\n\n  Species         A1         A2         A3          B1        B2         B3\n4       d -0.1398728 -0.1236981 -0.1956281 -0.01232568 0.4140249 0.02370992\n         C1        C2         C3\n4 0.2644057 -1.359203 -0.5699135\n\n\n\n\n\nFirst, isolate the DNA sequence:\n\nUse .* with () to delete everything before the DNA sequence\n\n\nSeq&lt;-gsub(\".*(1 [gatc])\",\"\",Lythrum_18S)\n\n\nUse the .* and space with + to eliminate all text before the sequence\n\n\nSeq&lt;-gsub(\".*ORIGIN +\",\"\",paste(Seq,collapse=\"\"))\n\n\nEliminate spaces and the two // at the end\n\n\nSeq&lt;-gsub(\" |//\",\"\",Seq)\n\n\nCapital letters look nicer, but requires a PERL qualifier \\\\U in the replacement string. This is not standard in R regular expressions, but can be accessed with perl=T.\n\n\nSeq&lt;-gsub(\"([actg])\",\"\\\\U\\\\1\",Seq,perl=T)\n\nNow that we have our formatted DNA sequence string, we can highlight start codons.\n\nORFs&lt;-gsub(\"ATG\",\"\\n &lt;ATG&gt;--&gt;START--&gt;\",Seq)\n\nAnd stop codons too.\n\nORFs&lt;-gsub(\"(TAA|TAG|TGA)\",\"&lt;\\\\1&gt;--STOP--| \\n\",Seq)\n\nNote the addition of the newline character (\\n) to make the output more readable. To see the final modified string, use the cat() function to print the string directly, rather than creating an object containing the string:\n\ncat(ORFs)\n\n(Output not shown)"
  },
  {
    "objectID": "regex2.html#more-exercises",
    "href": "regex2.html#more-exercises",
    "title": "Regular Expressions II",
    "section": "",
    "text": "Here are some more exercises to practice your skills. No solutions are given, but you will know if you are correct if you get the desired output.\n\nEmail Spammer: Consider a vector of email addresses scraped from the internet.\n\n\nrobert ‘dot’ colautti ‘at’ queensu ‘dot’ ca\nchris.eckert[at]queensu.ca\nlonnie.aarssen at queensu.ca\n\nUse regular expressions to convert all email addresses to the standard format: name@queensu.ca\n\nGenetic Simulation: Start by creating a random sequence of DNA. Think way back to the R Fundamentals Chapter to find a function that can randomly sample from 4 base pairs to create vector of 1,000 bases.\n\nOnce you have your vector, try collapsing it into a single element (i.e., a string with 1,000 characters).\n\nHint: You can do this with the paste() command. You just need one special parameter.\n\nNow, try each of the following challenges:\n\nReplace T with U.\nFind all start codons (AUG) and stop codons (UAA, UAG, UGA).\nFind all open reading frames (hint: consider each sequence beginning with AUG and ending with a stop codon; how do you know if both sequences are in the same reading frame?).\nCount the length (number of bases) for all open reading frames.\n\n\nRegex Golf\n\nHave fun! https://alf.nu/RegexGolf"
  },
  {
    "objectID": "preface.html",
    "href": "preface.html",
    "title": "Preface",
    "section": "",
    "text": "Think of a Biologist. Who do you see? Take a minute to write down some characteristics in your mind. Try to be specific: gender, skin, age, height, hair, clothes, personality. Who do you see?\nNow think of a computer programmer or data scientist. Write down their characteristics. How do these people differ in your mind? Can you imagine them being the same person? Can you picture yourself in both roles?\nThe goal of this book is to bridge these two worlds. In writing this book, I assume you are a practising biologist or a student of biology, or you are just motivated by biological phenomena. It doesn’t matter if you are a recent high school graduate entering into a biology undergraduate program, a graduate student embarking on an independent research dissertation, or a senior scientist with specialized expertise in the science of life. As long as you are interested in learning how to code, this book is written for you.\nThe goal of this book is to provide a ‘how-to’ guide to connect you to the world of data science. We focus on the fundamentals of the R programming language and its applications in biology. In writing this book, I assume you do not have much coding experience. Whether you are a new biology student or a seasoned professional, this book was written for you.\nThere are many great introductions to the R coding language available in print and online. But these tend to be general and abstract, sometimes going on tangents that are not so relevant to what you want to do as a biologist. What makes this book different, is that it is written with the biologist in mind. Specifically, my goal was to write the book that I wish I had had as an undergraduate student learning how to collect and analyze data. With the benefit of hindsight, I’ve tried to cut out all the programming details that haven’t been of much use to me as a data scientist, and focus on the most common methods. I’ve tried to connect to biological questions and examples as much as possible, without getting too side-tracked with biological details. This decision-making progress is based on my research and teaching experience in a range of topics in Biology and Health Sciences at Queen’s University – Environmental Science, Epidemiology, Genomics, Ecology, and Evolution.\nA comprehensive coding volume would require thousands of printed pages and take decades to master. In choosing the content for this book, I have focused on everything that I wish I knew when I first started learning to program in R. Many of the functions and packages included here were not available when I started, but have some exceptional functionality. I will continue to add new tricks and techniques that I find useful.\n\n\nMaybe you are curious about coding for data analysis but you aren’t sure if you want to invest the time and energy you will need to become competent in these methods. Many students in biology programs do not receive strong quantitative skills training in math, statistics, or computer science. In fact, many of us choose to go into biology programs because we are scared of the quantitative focus of the ‘hard’ sciences like physics and chemistry. Only much later do we realize how valuable these skills can be for investigating biological phenomena. Modern biology is defined by ‘big data’ sources including high-throughput sequencing, real-time environmental measurements, satellite imaging, animal tracking, and monitoring human health. Along with more traditional data types, these data are increasingly made available in online databases that are too big to navigate manually. Coding is not simply helpful to biologists – it’s becoming essential.\nTo help demonstrate the tremendous value of coding, I focus on examples drawn from real biological studies. I try to provide real-world examples of how one can apply programming tools and techniques to curate, analyze, and visualize biological data. These tend to be areas in which I have researched and published papers – opportunities that were presented to me because of my ability to analyze data in a reproducible and open framework. However, a key theme of this book is that these skills are highly transferable, not only across the biological sciences but to other disciplines.\nHere are a few examples of the diversity of data, analyses, and visualizations in my own collaborations, which all use data analysis and visualizations in R:\n\nA paper examining rapid evolution of flowering: https://doi.org/10.1126/science.1242121\nA de novo genome assembly: https://doi.org/10.1093/g3journal/jkab339\nA meta-analysis of evolution of invasive species: https://doi.org/10.1111/mec.13162\nTracking COVID-19 outbreaks using whole-genome sequencing: https://doi.org/10.1038/s41598-021-83355-1\nA study of metabolites in nasal swabs that can differentiate COVID-19 from other viral infections in human patients: https://www.nature.com/articles/s41598-022-14050-y\nAn analysis of 3,429 herbarium images and &gt;1 million weather records to reconstruct evolution of an invasive plant: https://www.pnas.org/doi/full/10.1073/pnas.2107584119\nA model of species range limits: https://royalsocietypublishing.org/doi/full/10.1098/rstb.2021.0020\n\n\n\n\nThis book was written at Queen’s University in Kingston, Ontario, Canada, originally known as Katarowki, part of the traditional lands of the Anishinaabe and Haudenosaunee. I am very grateful that fate has brought me to this land to learn the Teaching of the Seven Grandfathers. When you need a break from coding, I encourage you to look up the Anishinaabe tradition of the Teaching of the Seven Grandfathers. Remember that coding is a superpower, and as your coding skills improve, you will have the responsibility to use your powers for the good of others.\nThis book was written at Queen’s University, but it began in 2009, when I first learned to code in R and began to collect resources and make notes to help teach these tools to others. In 2015, I converted these personal notes into a course at the University of Tübingen in southern Germany. I’m grateful to my friend and colleague, Dr. Oliver Bossdorf for encouraging me to develop and deliver that course. In 2017 I added new modules and developed a website of self-tutorials for a fourth year at Queen’s University called Introduction to Computation and Big Data in Biology. Over the next four years this content was revised and refined for a third year course on biostatistics and three graduate-level courses. In 2022 I separated these notes into four books, the first of which became the R Crash Course for Biologists. Feedback from dozens of graduate and undergraduate students helped me to understand which concepts were most difficult to new learners. The following graduate provided especially detailed and helpful feedback: Mia Akbar, María José Gómez Quijano, Charlotte Ngo, Claire Smith, Mike Vermeulen, and Sherise Vialva. The courses I’ve taught require a lot of work from students in the form of weekly quizzes and assignments. As such, the Dr. Brian Cumming deserves much credit for supporting these courses with Teaching Assistants to help me develop and deliver the content effectively. A special thanks to my partner in life and academia, Dr. Sarah Yakimowski, who provided support and feedback on a wide range of aspects from basic teaching philosophy to the cover design and layout. And a final thanks to you, the reader, for your interest in this book. I hope you find it useful, and I hope you will let me know what you think via email robert.colautti@queensu.ca or social media."
  },
  {
    "objectID": "preface.html#why-this-book",
    "href": "preface.html#why-this-book",
    "title": "Preface",
    "section": "",
    "text": "Maybe you are curious about coding for data analysis but you aren’t sure if you want to invest the time and energy you will need to become competent in these methods. Many students in biology programs do not receive strong quantitative skills training in math, statistics, or computer science. In fact, many of us choose to go into biology programs because we are scared of the quantitative focus of the ‘hard’ sciences like physics and chemistry. Only much later do we realize how valuable these skills can be for investigating biological phenomena. Modern biology is defined by ‘big data’ sources including high-throughput sequencing, real-time environmental measurements, satellite imaging, animal tracking, and monitoring human health. Along with more traditional data types, these data are increasingly made available in online databases that are too big to navigate manually. Coding is not simply helpful to biologists – it’s becoming essential.\nTo help demonstrate the tremendous value of coding, I focus on examples drawn from real biological studies. I try to provide real-world examples of how one can apply programming tools and techniques to curate, analyze, and visualize biological data. These tend to be areas in which I have researched and published papers – opportunities that were presented to me because of my ability to analyze data in a reproducible and open framework. However, a key theme of this book is that these skills are highly transferable, not only across the biological sciences but to other disciplines.\nHere are a few examples of the diversity of data, analyses, and visualizations in my own collaborations, which all use data analysis and visualizations in R:\n\nA paper examining rapid evolution of flowering: https://doi.org/10.1126/science.1242121\nA de novo genome assembly: https://doi.org/10.1093/g3journal/jkab339\nA meta-analysis of evolution of invasive species: https://doi.org/10.1111/mec.13162\nTracking COVID-19 outbreaks using whole-genome sequencing: https://doi.org/10.1038/s41598-021-83355-1\nA study of metabolites in nasal swabs that can differentiate COVID-19 from other viral infections in human patients: https://www.nature.com/articles/s41598-022-14050-y\nAn analysis of 3,429 herbarium images and &gt;1 million weather records to reconstruct evolution of an invasive plant: https://www.pnas.org/doi/full/10.1073/pnas.2107584119\nA model of species range limits: https://royalsocietypublishing.org/doi/full/10.1098/rstb.2021.0020"
  },
  {
    "objectID": "preface.html#acknowledgements",
    "href": "preface.html#acknowledgements",
    "title": "Preface",
    "section": "",
    "text": "This book was written at Queen’s University in Kingston, Ontario, Canada, originally known as Katarowki, part of the traditional lands of the Anishinaabe and Haudenosaunee. I am very grateful that fate has brought me to this land to learn the Teaching of the Seven Grandfathers. When you need a break from coding, I encourage you to look up the Anishinaabe tradition of the Teaching of the Seven Grandfathers. Remember that coding is a superpower, and as your coding skills improve, you will have the responsibility to use your powers for the good of others.\nThis book was written at Queen’s University, but it began in 2009, when I first learned to code in R and began to collect resources and make notes to help teach these tools to others. In 2015, I converted these personal notes into a course at the University of Tübingen in southern Germany. I’m grateful to my friend and colleague, Dr. Oliver Bossdorf for encouraging me to develop and deliver that course. In 2017 I added new modules and developed a website of self-tutorials for a fourth year at Queen’s University called Introduction to Computation and Big Data in Biology. Over the next four years this content was revised and refined for a third year course on biostatistics and three graduate-level courses. In 2022 I separated these notes into four books, the first of which became the R Crash Course for Biologists. Feedback from dozens of graduate and undergraduate students helped me to understand which concepts were most difficult to new learners. The following graduate provided especially detailed and helpful feedback: Mia Akbar, María José Gómez Quijano, Charlotte Ngo, Claire Smith, Mike Vermeulen, and Sherise Vialva. The courses I’ve taught require a lot of work from students in the form of weekly quizzes and assignments. As such, the Dr. Brian Cumming deserves much credit for supporting these courses with Teaching Assistants to help me develop and deliver the content effectively. A special thanks to my partner in life and academia, Dr. Sarah Yakimowski, who provided support and feedback on a wide range of aspects from basic teaching philosophy to the cover design and layout. And a final thanks to you, the reader, for your interest in this book. I hope you find it useful, and I hope you will let me know what you think via email robert.colautti@queensu.ca or social media."
  },
  {
    "objectID": "multiplot.html",
    "href": "multiplot.html",
    "title": "Multi-plot Graphs",
    "section": "",
    "text": "In the Quick Visualizations Chapter, we explored how to make multiple graphs for different groups using the facet() function. Let’s review briefly and then look at more advanced multi-graph options.\n\n\n\nFirst, we’ll run the usual plotting code.\n\nlibrary(ggplot2)\nsource(\"http://bit.ly/theme_pub\")\ntheme_set(theme_pub())\n\nContinuing from the previous chapter, we’ll work with the selection data from Colautti & Lau (2015). We’ll also change the header names, remove missing values, replace \\(s\\) with \\(|s|\\) and add a random variable Rpoint, as we did in the previous chapter.\n\nSelData&lt;-read.csv(\n  \"https://colauttilab.github.io/RCrashCourse/Selection_Data.csv\")\n\nnames(SelData)&lt;-c(\"Collector\", \"Author\", \"Year\", \"Journal\", \n                  \"Vol\", \"Species\", \"Native\", \"N\", \n                  \"Fitness.measure\", \"Trait\", \"s\", \n                  \"s.SE\", \"s.P\", \"B\", \"B.SE\", \"B.P\")\nSelData&lt;-SelData[!is.na(SelData$s),]\nSelData$s&lt;-abs(SelData$s)\nSelData$Rpoint&lt;-rnorm(nrow(SelData))\n\n\n\n\nThere are three main facet functions, each with different options.\n\nfacet_null makes a single graph, this is the default for ggplot(), as we saw earlier\nfacet_grid lets us define a grid and set the vertical and horizontal variables\nfacet_wrap is a convenient option if only have one categorical variable but many categories\n\nRemember: one little tricky part of facets with ggplot is that we can either use the tilde notation (. ~ .) with the facet_grid function, or else we must define the variable for faceting with the vars() function. The vars() function indicate which categorical variables from the original data set should be used to subset the graphs.\nReturning to the BivPlot example above:\n\nBivPlot&lt;-ggplot(data=SelData, aes(x=log(s+1), y=Rpoint)) + \n  geom_point() \nBivPlot + facet_grid(Native ~ Collector)\n\n\n\n\n\n\n\n\n\nBivPlot&lt;-ggplot(data=SelData, aes(x=log(s+1), y=Rpoint)) + \n  geom_point() \nBivPlot + facet_wrap(vars(Year))\n\n\n\n\n\n\n\n\nNote that this large, multi-panel graph does not reproduce well in this textbook, but may look better if plotted to a large window on your computer, or output to an external file with the pdf() or svg() functions, as discussed in the Basic Customizations Chapter.\n\n\n\nFacets produce graphs that all have the same dimension and the same x- and y-axes. We might call these ‘homogeneous’ plots because they use a homogeneous format. For some advanced publications and reports, we might want to include ‘heterogeneous’ plots with different axes and different sizes. The gridExtra package provides options for this.\nRemember to install with install.packages(\"gridExtra\") before you try to load the library for the first time.\n\nlibrary(gridExtra)\n\nThe grid.arrange() funcrion from the gridExtra package allows for more complex multi-panel figures.\n\n\nUse this to combine heterogeneous ggplot objects into a single multi-panel plot.\nNote that this will print graphs down rows, then across columns, from top left to bottom right. You can use nrow and ncol to control the layout in a grid format.\nFirst, we’ll add two more graphs for plotting – a histogram and bar plot.\n\nHistPlot&lt;-ggplot(aes(x=s,colour=Native), data=SelData) + \n  geom_density()\nBarPlot&lt;-ggplot(aes(x=s,fill=Native), data=SelData) + \n  geom_histogram(binwidth=1/10)\n\nLet’s start by plotting across one row\n\ngrid.arrange(HistPlot,BivPlot,BarPlot,nrow=1)\n\n\n\n\n\n\n\n\nAlternatively, we can put the plots down one column\n\ngrid.arrange(HistPlot,BivPlot,BarPlot,ncol=1)\n\n\n\n\n\n\n\n\nNote that you might get some warnings based on missing values or wrong binwidth options. You will also see some weird things with different text sizes in the graphs. Normally, you would want to fix these for a final published figure but here we are just focused on showing what is possible with the layouts.\nWe can see that grid.arrange() allows us to combine multiple graphs with different axes, data, and geom_ geometries. However, the layout of the graphs all have the same dimension. What if we want to combine plots of different size? For example, maybe we want to have one graph that is narowwer but wider than the other two. Or maybe we would like to inset a smaller graph inside of a larger one. The grid package can handle this.\n\n\n\n\nWe can make more advanced multi-panel graphs using the grid package. This is part of the base installation of R so you don’t need to use install.packages() this time.\n\nlibrary(grid)\n\nFirst, we set up a new plotting area with grid.newpage().\n\ngrid.newpage() # Open a new page on grid device\n\nYou won’t see anything plotted yet. To insert a new graph on top (or inside) the current graph, we use pushViewport to set up an imaginary plotting grid. In this case, imagine breaking up the plotting space into 3 rows by 2 columns.\n\npushViewport(viewport(layout = grid.layout(3, 2))) \n\nAgain, there is nothing being plotted yet, we have only set up the plotting area. Next, we print each plotting object into the grid(s) space we would like it to go.\nAdd the first figure in row 3 and across columns 1:2\n\nprint(HistPlot, vp = viewport(layout.pos.row = 3, \n                              layout.pos.col = 1:2))\n\nAdd the next figure across rows 1 and 2 of column 1\n\nprint(BivPlot, vp = viewport(layout.pos.row = 1:2, \n                             layout.pos.col = 1))\n\nAdd the final figure across rows 1 and 2 of column 2\n\nprint(BarPlot, vp = viewport(layout.pos.row = 1:2, \n                             layout.pos.col = 2))\n\n\n\n\n\n\n\n\n\n\n\n\nWe can also use pushViewport to set up a grid for plotting on top of an existing graph or image. This can be used to generate a figure with an inset.\nFirst generate the ‘background’ plot. Note that you could alternatively load an image here to place in the background.\n\nHistPlot\n\nNext, overlay an invisible grid layout, with the number of cells that can be used to determine size and location of the inset graph. In this case, we’ll set up a 4-by-4 grid and then plot in the top, right corner.\n\npushViewport(viewport(layout = grid.layout(4, 4)))\n\nFinally, add the graph. In this case we want it only in the top two rows and the right-most two columns – i.e. the top-right corner.\n\nprint(BivPlot, vp = viewport(layout.pos.row = 1:2,\n                             layout.pos.col = 3:4))\n\nThe final product:\n\nHistPlot\npushViewport(viewport(layout = grid.layout(4, 4)))\nprint(BivPlot, vp = viewport(layout.pos.row = 1:2,\n                             layout.pos.col = 3:4))\n\n\n\n\n\n\n\n\n\n\n\n\nThe 2009 book ggplot2: Elegant Graphics for Data Analysis by Hadley Wickham is the definitive guide to all things ggplot.\nA physical copy is published by Springer: http://link.springer.com/book/10.1007%2F978-0-387-98141-3\nAnd there is a free ebook version: https://ggplot2-book.org/"
  },
  {
    "objectID": "multiplot.html#introduction",
    "href": "multiplot.html#introduction",
    "title": "Multi-plot Graphs",
    "section": "",
    "text": "In the Quick Visualizations Chapter, we explored how to make multiple graphs for different groups using the facet() function. Let’s review briefly and then look at more advanced multi-graph options."
  },
  {
    "objectID": "multiplot.html#setup",
    "href": "multiplot.html#setup",
    "title": "Multi-plot Graphs",
    "section": "",
    "text": "First, we’ll run the usual plotting code.\n\nlibrary(ggplot2)\nsource(\"http://bit.ly/theme_pub\")\ntheme_set(theme_pub())\n\nContinuing from the previous chapter, we’ll work with the selection data from Colautti & Lau (2015). We’ll also change the header names, remove missing values, replace \\(s\\) with \\(|s|\\) and add a random variable Rpoint, as we did in the previous chapter.\n\nSelData&lt;-read.csv(\n  \"https://colauttilab.github.io/RCrashCourse/Selection_Data.csv\")\n\nnames(SelData)&lt;-c(\"Collector\", \"Author\", \"Year\", \"Journal\", \n                  \"Vol\", \"Species\", \"Native\", \"N\", \n                  \"Fitness.measure\", \"Trait\", \"s\", \n                  \"s.SE\", \"s.P\", \"B\", \"B.SE\", \"B.P\")\nSelData&lt;-SelData[!is.na(SelData$s),]\nSelData$s&lt;-abs(SelData$s)\nSelData$Rpoint&lt;-rnorm(nrow(SelData))"
  },
  {
    "objectID": "multiplot.html#facets",
    "href": "multiplot.html#facets",
    "title": "Multi-plot Graphs",
    "section": "",
    "text": "There are three main facet functions, each with different options.\n\nfacet_null makes a single graph, this is the default for ggplot(), as we saw earlier\nfacet_grid lets us define a grid and set the vertical and horizontal variables\nfacet_wrap is a convenient option if only have one categorical variable but many categories\n\nRemember: one little tricky part of facets with ggplot is that we can either use the tilde notation (. ~ .) with the facet_grid function, or else we must define the variable for faceting with the vars() function. The vars() function indicate which categorical variables from the original data set should be used to subset the graphs.\nReturning to the BivPlot example above:\n\nBivPlot&lt;-ggplot(data=SelData, aes(x=log(s+1), y=Rpoint)) + \n  geom_point() \nBivPlot + facet_grid(Native ~ Collector)\n\n\n\n\n\n\n\n\n\nBivPlot&lt;-ggplot(data=SelData, aes(x=log(s+1), y=Rpoint)) + \n  geom_point() \nBivPlot + facet_wrap(vars(Year))\n\n\n\n\n\n\n\n\nNote that this large, multi-panel graph does not reproduce well in this textbook, but may look better if plotted to a large window on your computer, or output to an external file with the pdf() or svg() functions, as discussed in the Basic Customizations Chapter."
  },
  {
    "objectID": "multiplot.html#gridextra-package",
    "href": "multiplot.html#gridextra-package",
    "title": "Multi-plot Graphs",
    "section": "",
    "text": "Facets produce graphs that all have the same dimension and the same x- and y-axes. We might call these ‘homogeneous’ plots because they use a homogeneous format. For some advanced publications and reports, we might want to include ‘heterogeneous’ plots with different axes and different sizes. The gridExtra package provides options for this.\nRemember to install with install.packages(\"gridExtra\") before you try to load the library for the first time.\n\nlibrary(gridExtra)\n\nThe grid.arrange() funcrion from the gridExtra package allows for more complex multi-panel figures.\n\n\nUse this to combine heterogeneous ggplot objects into a single multi-panel plot.\nNote that this will print graphs down rows, then across columns, from top left to bottom right. You can use nrow and ncol to control the layout in a grid format.\nFirst, we’ll add two more graphs for plotting – a histogram and bar plot.\n\nHistPlot&lt;-ggplot(aes(x=s,colour=Native), data=SelData) + \n  geom_density()\nBarPlot&lt;-ggplot(aes(x=s,fill=Native), data=SelData) + \n  geom_histogram(binwidth=1/10)\n\nLet’s start by plotting across one row\n\ngrid.arrange(HistPlot,BivPlot,BarPlot,nrow=1)\n\n\n\n\n\n\n\n\nAlternatively, we can put the plots down one column\n\ngrid.arrange(HistPlot,BivPlot,BarPlot,ncol=1)\n\n\n\n\n\n\n\n\nNote that you might get some warnings based on missing values or wrong binwidth options. You will also see some weird things with different text sizes in the graphs. Normally, you would want to fix these for a final published figure but here we are just focused on showing what is possible with the layouts.\nWe can see that grid.arrange() allows us to combine multiple graphs with different axes, data, and geom_ geometries. However, the layout of the graphs all have the same dimension. What if we want to combine plots of different size? For example, maybe we want to have one graph that is narowwer but wider than the other two. Or maybe we would like to inset a smaller graph inside of a larger one. The grid package can handle this."
  },
  {
    "objectID": "multiplot.html#grid-package",
    "href": "multiplot.html#grid-package",
    "title": "Multi-plot Graphs",
    "section": "",
    "text": "We can make more advanced multi-panel graphs using the grid package. This is part of the base installation of R so you don’t need to use install.packages() this time.\n\nlibrary(grid)\n\nFirst, we set up a new plotting area with grid.newpage().\n\ngrid.newpage() # Open a new page on grid device\n\nYou won’t see anything plotted yet. To insert a new graph on top (or inside) the current graph, we use pushViewport to set up an imaginary plotting grid. In this case, imagine breaking up the plotting space into 3 rows by 2 columns.\n\npushViewport(viewport(layout = grid.layout(3, 2))) \n\nAgain, there is nothing being plotted yet, we have only set up the plotting area. Next, we print each plotting object into the grid(s) space we would like it to go.\nAdd the first figure in row 3 and across columns 1:2\n\nprint(HistPlot, vp = viewport(layout.pos.row = 3, \n                              layout.pos.col = 1:2))\n\nAdd the next figure across rows 1 and 2 of column 1\n\nprint(BivPlot, vp = viewport(layout.pos.row = 1:2, \n                             layout.pos.col = 1))\n\nAdd the final figure across rows 1 and 2 of column 2\n\nprint(BarPlot, vp = viewport(layout.pos.row = 1:2, \n                             layout.pos.col = 2))\n\n\n\n\n\n\n\n\n\n\n\n\nWe can also use pushViewport to set up a grid for plotting on top of an existing graph or image. This can be used to generate a figure with an inset.\nFirst generate the ‘background’ plot. Note that you could alternatively load an image here to place in the background.\n\nHistPlot\n\nNext, overlay an invisible grid layout, with the number of cells that can be used to determine size and location of the inset graph. In this case, we’ll set up a 4-by-4 grid and then plot in the top, right corner.\n\npushViewport(viewport(layout = grid.layout(4, 4)))\n\nFinally, add the graph. In this case we want it only in the top two rows and the right-most two columns – i.e. the top-right corner.\n\nprint(BivPlot, vp = viewport(layout.pos.row = 1:2,\n                             layout.pos.col = 3:4))\n\nThe final product:\n\nHistPlot\npushViewport(viewport(layout = grid.layout(4, 4)))\nprint(BivPlot, vp = viewport(layout.pos.row = 1:2,\n                             layout.pos.col = 3:4))"
  },
  {
    "objectID": "multiplot.html#further-reading",
    "href": "multiplot.html#further-reading",
    "title": "Multi-plot Graphs",
    "section": "",
    "text": "The 2009 book ggplot2: Elegant Graphics for Data Analysis by Hadley Wickham is the definitive guide to all things ggplot.\nA physical copy is published by Springer: http://link.springer.com/book/10.1007%2F978-0-387-98141-3\nAnd there is a free ebook version: https://ggplot2-book.org/"
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "Introduction",
    "section": "",
    "text": "In Biology, there are two dominant programming languages: Python and R. Thousands of hours have been wasted arguing the merits of one programming language over another. The truth is that there is a lot of similarity and it’s very easy to move from one to the other.\nThere are many other programming languages used in Biology. C/C#/C++ and Java are popular in computer science because they provide a high level of control, but this comes at a cost of abstraction and a steeper learning curve. Bash programming in Unix/Linux/GNU is all but necessary for high-performance computing on remote servers, but in biology it is most often used to automate file management and to run programs written in other languages. Julia is gaining momentum for mathematical modellers, but it is still in its infancy. PERL was popular for bioinformatics but it has been all but replaced by Python.\nWe focus on R because it is more commonly used in published statistical analyses in Biology, and it is a bit easier to learn. As you will see, it is very easy to walk through the fundamentals and generate graphs and statistical analyses with just a few hours of practice. This comes at a cost of slower run-times and less flexibility than Python, but this is usually not a problem for beginners. In fact, it is possible to use R to run Python code (or C++ or many other languages). More importantly, concepts like data objects, function and packages are conceptually very similar between R and Python, making it easy to move from one language to the other. The truth is that they are both good languages and anyone who tells you that language A is better than B is simply showing their ignorance about language B.\n\n\nIf you’ve completed a few years of any undergraduate program in biology, then you’ve probably developed a good approach to study various subjects in Biology. Maybe it involves reading the textbook, attending lectures, and making notes that you review before the big test. Coding is different.\nIf this is your first attempt to learn how to code, then it’s important to understand HOW to learn to code. You won’t learn by reading this textbook. You need to take participate and actively take control of your learning by typing along with the examples in this book.\nConsider that R is a programming language. When I teach this content at the senior undergraduate and junior graduate level, I often begin with a poll of students to see who has learned to speak more than one language. I then ask:\nQuestion: How did you become fluent in a second language?\nSome common themes in the answers tend to be:\n\nImmerse yourself\nStudy, read, listen\nTry something new, fail, correct errors, repeat\nPractice, practice, practice!\n\nHow do you become fluent in a programming language? Pretty much the same way:\n\nImmerse yourself\nStudy, read, and type everything out!\nTry something new, fail, correct errors, repeat\nPractice, practice, practice!\n\nLearning a new language is not easy. Learning a programming language is not easy either. Here are a few specific tips to become fluent in R:\n\nGet organized and PLAN. Use a personal calender and schedule sufficient time to deal with error messages. This is important to accept, though it can be difficult: troubleshooting your code often takes more time than planning and writing it, especially when you are starting to learn.\nApply what you learn. You will start to develop a toolbox of coding techniques from day one. Look for opportunities to apply them whenever you can. Try to re-frame small projects or tasks in terms of what you can address with your R toolkit. Even if it takes a lot longer to code than to use other methods, the extra time will reinforce your coding skills, saving time in the long run. Take time to think about what coding tools you can apply.\nExperiment. Try new things, make mistakes, solve problems.\nDevote time. Set aside large blocks of time (2+ hours), to immerse yourself in your coding lessons or project.\nFocus. Eliminate distractions. Turn off your notifications. Put your phone and computer on ‘airplane mode’. Do whatever it takes to work without interruption. Get some good headphones with white noise or instrumental sounds (no lyrics) to block out distractions. Here are some things I listen to, depending on mood:\n\n\nBaroque/Classical\nSmooth Jazz\nElectronic (ambient, house, lofi)\nhttps://coffitivity.com/\n\n\nLearn to Troubleshoot.\n\n\nIf you get stuck, Google: “How do I ______ in R”. Look for answers from a website called Stack Overflow: (https://stackoverflow.com/)\nIf you can’t figure out what an error means, paste it into Google. Again, look for answers from Stack Overflow.\n\n\nSocialize. Find a coding support group or find a few others to form your own group. Discuss problems and successes. Read other people’s code to see how they tackle problems. Rarely is there one single ‘right’ way to code something.\nGit ’er done. When you are starting out, the ‘right’ way to code is whatever it takes to get the code to do what you want. Don’t let perfection be the enemy of the good: messy code that works is 100% better than efficient code that never runs.\nImprove. As you get more comfortable you can start to think about cleaner, clearer, more efficient ways to code. As you advance, look for ways to do the same thing faster and with fewer lines of code.\nEmbrace Failure. I can’t stress this enough. Even after 10+ years of programming experience, I often make mistakes, and a decent amount of my time is spent dealing with error messages and unexpected output. Every error is a learning opportunity. This is time well spent.\nRead the documentation for the function or package you are using. Don’t worry if you don’t understand everything. Be sure to take the time to read it slowly and try to understand as much as you can. Try searching online for terms or phrases that are not familiar to you. You will come across these again in the future, so you are investing time now for future payoff. In addition to the built-in help in R, often the repository on The Comprehensive R Archinve Network (CRAN) (https://cran.r-project.org/) or Bioconductor (https://www.bioconductor.org/) will include vignettes or tutorials as pdf files with worked examples.\nUse AI Effectively. With the rise of Large Language Models (LLMs) and other forms of artificial intelligence (AI), it’s time re-think how we teach and learn coding. We’ll expand on this in the next section.\n\n\n\n\nThe rapid advancement of Artificial Intelligence (AI), particularly Large Language Models (LLMs) like GPT-4, is having a profound effect on the way that we work and learn. Just as LLMs can help with writing and research, they can also be used to write and edit code. Programs like ChatGPT, and Bing Copilot use the GPT-4 LLM, others like Code Llama, and StarCoder have different LLMS, but all can write effective code (. There is no question that LLMs and other forms of AI will continue to develop into indispensable tools for biological research. If you are reading this book to learn how to code, these tools can act like a personal tutor or to help you learn to code faster and more effectively, and potentially introduce you to new techniques. It’s likely these tools will only get better in the future, so it’s worth incorporating them into your learning process now.\nAs of this writing, there are two major issues with LLMs that can interfere with your learning. The first one is a recurring theme in this book: The best way to learn to code is to write code yourself, type everything out, try different things, make mistakes, and learn how to fix them. Struggling through is the best way to learn. This can be a slow and tedious proces, but it is an important baseline for learning. LLMs are not programmed to help you learn, they are just fancy statistical models that have no vested interest in your learning or career goals. You can ask a LLM to write code for you, but I strongly suggest you avoid that when you are first learning how to code. Instead, try to use LLMs like a tutor by asking questions that inform your own understanding. Some specific tips are provided below.\nApart from hindering your learning, the second major issue with LLMs is that they will often give you bullshit without warning. Computer scientists say these models ‘hallucinate’, but a more accurate term is ‘bullshit’. The word bullshit has a technical definition thanks to Harry G. Frankfurt in his 2005 philosophy book On Bullshit. Briefly, he points out that bullshit is similar to a lie except that there is no regard for the truth. Whereas a liar is aware of the truth and tries to direct people away from it, a bullshitter will happily mix truth with misinformation and disinformation to achieve their goals.\nIn Calling Bullshit: The Art of Skepticism in a Data-Driven World, Carl Bergstrom and Jevin West apply the concept to data analysis, showing how even accurate data can be misrepresented or misleading by choices made by the person analyzing and presenting them. They also provide strategies for detecting bullshit in data analysis in a very entertaining way with minimal jargon. I highly recommend this book. I also recommend Frankfurt’s book, which a small pocket-sized book. Both are easy to read.\nIn a 2023 article for Undark Magazine, Carl Bergstrom and Brandon Ogbunu argue that ‘bullshiting’ is a more accurate term than ‘hallucinating’ for ChatGPT:\n\n“When AI chatbots flood the world with false facts, confidently asserted, they’re not breaking down, glitching out, or hallucinating. No, they’re bullshitting.”\n\nDespite the issues with LLMs, it’s likely that these models will only improve, and future coders will use AI regularly in their workflows. Therefore, it’s worth learning how to use LLMs to write effective code. The key for you at this point is to use AI to save you time and effort, without it getting in the way of your learning by doing your hard work for you, or by teaching you bullshit. Here are some tips when you are first starting to learn to code:\n\nDO Try to figure it out yourself, ask AI for help only when you get stuck.\nDO Ask AI for help interpreting warning or error messages in your code.\nDO Read the R help yourself. It’s difficult at first, but it gets easier. Ask AI for help with terms or concepts that you don’t understand in the R help.\nDO Ask AI for feedback on your code, but don’t use code that you don’t understand, and read the code carefully with your bullshit detectors turned up to 11.\nDO Ask AI to explain code to you, as you would ask a tutor.\nDON’T Ask AI which packages or functions you should use. Unless you like to spend lots of time memorizing functions and package names, the best way to learn them is by forcing yourself to think about what you know and how you can apply it for a given problem.\nDON’T Ask AI to write code for you. Not only will it make it affect your ability to learn to write effective code, it may produce complete bullshit.\n\n\n\n\nAs you work through these self-tutorials, don’t just read them. I can’t stress this enough: take the time to type out the commands in your R (Studio) console and make sure you get the same output. The simple act of typing it out will send messages to your brain saying “hey, this is an important thing to remember.” If you get an error, even better! Read the error carefully, then compare what you typed to what is in the tutorial. Once you find what is different, you will learn what that error means.\nAbout 70-90% of coding time is dealing with errors, and the same is true for learning to code. This can be difficult for us to accept because our experience in a typical biology course is quite different.\n\n\n\nLearning to code is a lifelong journey. There is always more to learn and new ways to improve. The beginning of your journey might be broken up into three overlapping stages, depending on the level of training you have already received:\n\nUtter bewilderment – reading code is like reading a foreign language. All these letters and symbols are meaningless to you.\nUnderstanding – you can look at a function and have a decent idea of what it does and how to use it, but you don’t understand most of the parameters. You usually rely on default parameters.\nCompetence – you can write your own code from scratch, without needing to look up examples, and you are able to carefully review and apply parameters. You rarely trust default parameters, especially for more complicated functions.\nExpertise – you write your own functions and help others to troubleshoot code, analysis pipelines, etc. Maybe you even have your own published R package or algorithms.\n\nDon’t confuse understanding with competence – this is a common mistake that students make. It’s relatively easy to learn how to understand code that is shown to you, but it’s quite another skill to learn the names and parameters of useful functions and apply them to solve problems or answer questions. That doesn’t mean you need to memorize every function – though memorization can help. A good strategy to move from understanding to competence is to take the extra time and make the effort to type out the code that is shown to you, even when you can look at it and understand what it does. As noted earlier, the act of typing out the code is what will help to solidify it in your brain.\n\n\n\nThere is often a mismatch between the knowledge acquired through a university degree and the skills that employers need in their workforce. That is, newly minted university students have a lot of knowledge and skills for learning, but often struggle with goals laid out by employers or in entrepreneurial endeavours or thesis/dissertation research.\nIn the computing world, the disconnect between learning and application can happen when students have acquired knowledge of coding algorithms and tools, but learn to apply these tools only when working within a ‘sandbox’ created for teaching purposes. The sandbox is a clean and well-groomed programming environment with pre-loaded software and examples, curated by the educator. The sandbox lacks the messiness and ambiguity that define real-world applications, and the student never faces these uncomfortable but highly relevant challenges. The sandbox approach is commonly used in both university settings and online courses (e.g. Udemy, Coursera, Datacamp, Skillshare).\nA typical teaching sandbox will probably include pre-installed software with ‘clean’ data defined by a well-defined data structure without errors or missing observations. It will probably have a clear and singular path from problem to solution. This approach has the advantage of efficiency – both for the educator and for the learner. The learner can be guided to move efficiently through key learning objectives while minimizing unexpected bugs or problems that can slow progress and take significant time for educators to deal with. The sandbox creates a more homogeneous experience that is more efficient for tracking progress and assigning grades. The trade-off is that sandbox learning does a poor job preparing you for the messy realities of coding with real data in the real world.\nAn alternative to the sandbox approach is translational coding, which borrows the term from translational medicine. Translational medicine is a multidisciplinary hybrid between research and application that directly connects medical researchers to the needs of patients. By analogy, translational coding tries to directly connect coding skills and tools to the needs of potential employers.\nThis will not be pleasant for you, the learner, at first. The sandbox approach is popular with learners because it is relatively quick and painless with minimal time needed for researching, planning, debugging, and other forms of problem solving. There is value to learning to work quickly and efficiently, but there is also value in learning to deal with problems that arise in the real-world. This includes dealing with errors at every stage, from installing software to problems hidden among thousands of lines of data or code. This can be frustrating at first, and it will absolutely slow down your progress. There are three key things to remember when this happens:\n\nEvery error, problem, or roadblock is a learning opportunity. Every problem and assignment will have specific goals and challenges that are explicitly laid out by the tutorial, assignment or practice problem. These are the challenges that every learner must overcome to complete the task. In addition, there are implicit challenges that may be unique or shared by only a few learners – a particular typo in the code, an error importing or saving, an unidentified error in your dataset. These implicit problems may feel ‘unfair’ because not every learner has to deal with the same problems at the same time. Over time however, these will tend to average out so that everyone will make similar mistakes, albeit at different times.\nYou can learn to budget your time to deal with these implicit, unforeseen errors. And this is an important and highly-transferrable skill! Start a problem or assignment as soon as possible. Give yourself time to take a break and come back to a problem when you get stuck. When you estimate how long an assignment will take, don’t just look at the explicit goals. Remember to also add time for the implicit challenges, which will take much longer to complete.\nTime devoted to a new problem pays off in the future. Most of your time will be spent the first time you encounter a problem. If you take the time to read the error or warning, think about it, and investigate it, then you will know how to recognize and deal with it in the future. In this way, implicit challenges tend to balance out among learners over time. Some learners will encounter a problem early and struggle while others move ahead, until they encounter the same problem, evening the playing field.\n\nThe most important thing is to embrace the challenge! Don’t let yourself get discouraged.\nNow, let’s get set up to start coding in R."
  },
  {
    "objectID": "intro.html#advice",
    "href": "intro.html#advice",
    "title": "Introduction",
    "section": "",
    "text": "If you’ve completed a few years of any undergraduate program in biology, then you’ve probably developed a good approach to study various subjects in Biology. Maybe it involves reading the textbook, attending lectures, and making notes that you review before the big test. Coding is different.\nIf this is your first attempt to learn how to code, then it’s important to understand HOW to learn to code. You won’t learn by reading this textbook. You need to take participate and actively take control of your learning by typing along with the examples in this book.\nConsider that R is a programming language. When I teach this content at the senior undergraduate and junior graduate level, I often begin with a poll of students to see who has learned to speak more than one language. I then ask:\nQuestion: How did you become fluent in a second language?\nSome common themes in the answers tend to be:\n\nImmerse yourself\nStudy, read, listen\nTry something new, fail, correct errors, repeat\nPractice, practice, practice!\n\nHow do you become fluent in a programming language? Pretty much the same way:\n\nImmerse yourself\nStudy, read, and type everything out!\nTry something new, fail, correct errors, repeat\nPractice, practice, practice!\n\nLearning a new language is not easy. Learning a programming language is not easy either. Here are a few specific tips to become fluent in R:\n\nGet organized and PLAN. Use a personal calender and schedule sufficient time to deal with error messages. This is important to accept, though it can be difficult: troubleshooting your code often takes more time than planning and writing it, especially when you are starting to learn.\nApply what you learn. You will start to develop a toolbox of coding techniques from day one. Look for opportunities to apply them whenever you can. Try to re-frame small projects or tasks in terms of what you can address with your R toolkit. Even if it takes a lot longer to code than to use other methods, the extra time will reinforce your coding skills, saving time in the long run. Take time to think about what coding tools you can apply.\nExperiment. Try new things, make mistakes, solve problems.\nDevote time. Set aside large blocks of time (2+ hours), to immerse yourself in your coding lessons or project.\nFocus. Eliminate distractions. Turn off your notifications. Put your phone and computer on ‘airplane mode’. Do whatever it takes to work without interruption. Get some good headphones with white noise or instrumental sounds (no lyrics) to block out distractions. Here are some things I listen to, depending on mood:\n\n\nBaroque/Classical\nSmooth Jazz\nElectronic (ambient, house, lofi)\nhttps://coffitivity.com/\n\n\nLearn to Troubleshoot.\n\n\nIf you get stuck, Google: “How do I ______ in R”. Look for answers from a website called Stack Overflow: (https://stackoverflow.com/)\nIf you can’t figure out what an error means, paste it into Google. Again, look for answers from Stack Overflow.\n\n\nSocialize. Find a coding support group or find a few others to form your own group. Discuss problems and successes. Read other people’s code to see how they tackle problems. Rarely is there one single ‘right’ way to code something.\nGit ’er done. When you are starting out, the ‘right’ way to code is whatever it takes to get the code to do what you want. Don’t let perfection be the enemy of the good: messy code that works is 100% better than efficient code that never runs.\nImprove. As you get more comfortable you can start to think about cleaner, clearer, more efficient ways to code. As you advance, look for ways to do the same thing faster and with fewer lines of code.\nEmbrace Failure. I can’t stress this enough. Even after 10+ years of programming experience, I often make mistakes, and a decent amount of my time is spent dealing with error messages and unexpected output. Every error is a learning opportunity. This is time well spent.\nRead the documentation for the function or package you are using. Don’t worry if you don’t understand everything. Be sure to take the time to read it slowly and try to understand as much as you can. Try searching online for terms or phrases that are not familiar to you. You will come across these again in the future, so you are investing time now for future payoff. In addition to the built-in help in R, often the repository on The Comprehensive R Archinve Network (CRAN) (https://cran.r-project.org/) or Bioconductor (https://www.bioconductor.org/) will include vignettes or tutorials as pdf files with worked examples.\nUse AI Effectively. With the rise of Large Language Models (LLMs) and other forms of artificial intelligence (AI), it’s time re-think how we teach and learn coding. We’ll expand on this in the next section."
  },
  {
    "objectID": "intro.html#use-ai",
    "href": "intro.html#use-ai",
    "title": "Introduction",
    "section": "",
    "text": "The rapid advancement of Artificial Intelligence (AI), particularly Large Language Models (LLMs) like GPT-4, is having a profound effect on the way that we work and learn. Just as LLMs can help with writing and research, they can also be used to write and edit code. Programs like ChatGPT, and Bing Copilot use the GPT-4 LLM, others like Code Llama, and StarCoder have different LLMS, but all can write effective code (. There is no question that LLMs and other forms of AI will continue to develop into indispensable tools for biological research. If you are reading this book to learn how to code, these tools can act like a personal tutor or to help you learn to code faster and more effectively, and potentially introduce you to new techniques. It’s likely these tools will only get better in the future, so it’s worth incorporating them into your learning process now.\nAs of this writing, there are two major issues with LLMs that can interfere with your learning. The first one is a recurring theme in this book: The best way to learn to code is to write code yourself, type everything out, try different things, make mistakes, and learn how to fix them. Struggling through is the best way to learn. This can be a slow and tedious proces, but it is an important baseline for learning. LLMs are not programmed to help you learn, they are just fancy statistical models that have no vested interest in your learning or career goals. You can ask a LLM to write code for you, but I strongly suggest you avoid that when you are first learning how to code. Instead, try to use LLMs like a tutor by asking questions that inform your own understanding. Some specific tips are provided below.\nApart from hindering your learning, the second major issue with LLMs is that they will often give you bullshit without warning. Computer scientists say these models ‘hallucinate’, but a more accurate term is ‘bullshit’. The word bullshit has a technical definition thanks to Harry G. Frankfurt in his 2005 philosophy book On Bullshit. Briefly, he points out that bullshit is similar to a lie except that there is no regard for the truth. Whereas a liar is aware of the truth and tries to direct people away from it, a bullshitter will happily mix truth with misinformation and disinformation to achieve their goals.\nIn Calling Bullshit: The Art of Skepticism in a Data-Driven World, Carl Bergstrom and Jevin West apply the concept to data analysis, showing how even accurate data can be misrepresented or misleading by choices made by the person analyzing and presenting them. They also provide strategies for detecting bullshit in data analysis in a very entertaining way with minimal jargon. I highly recommend this book. I also recommend Frankfurt’s book, which a small pocket-sized book. Both are easy to read.\nIn a 2023 article for Undark Magazine, Carl Bergstrom and Brandon Ogbunu argue that ‘bullshiting’ is a more accurate term than ‘hallucinating’ for ChatGPT:\n\n“When AI chatbots flood the world with false facts, confidently asserted, they’re not breaking down, glitching out, or hallucinating. No, they’re bullshitting.”\n\nDespite the issues with LLMs, it’s likely that these models will only improve, and future coders will use AI regularly in their workflows. Therefore, it’s worth learning how to use LLMs to write effective code. The key for you at this point is to use AI to save you time and effort, without it getting in the way of your learning by doing your hard work for you, or by teaching you bullshit. Here are some tips when you are first starting to learn to code:\n\nDO Try to figure it out yourself, ask AI for help only when you get stuck.\nDO Ask AI for help interpreting warning or error messages in your code.\nDO Read the R help yourself. It’s difficult at first, but it gets easier. Ask AI for help with terms or concepts that you don’t understand in the R help.\nDO Ask AI for feedback on your code, but don’t use code that you don’t understand, and read the code carefully with your bullshit detectors turned up to 11.\nDO Ask AI to explain code to you, as you would ask a tutor.\nDON’T Ask AI which packages or functions you should use. Unless you like to spend lots of time memorizing functions and package names, the best way to learn them is by forcing yourself to think about what you know and how you can apply it for a given problem.\nDON’T Ask AI to write code for you. Not only will it make it affect your ability to learn to write effective code, it may produce complete bullshit."
  },
  {
    "objectID": "intro.html#learn-by-doing",
    "href": "intro.html#learn-by-doing",
    "title": "Introduction",
    "section": "",
    "text": "As you work through these self-tutorials, don’t just read them. I can’t stress this enough: take the time to type out the commands in your R (Studio) console and make sure you get the same output. The simple act of typing it out will send messages to your brain saying “hey, this is an important thing to remember.” If you get an error, even better! Read the error carefully, then compare what you typed to what is in the tutorial. Once you find what is different, you will learn what that error means.\nAbout 70-90% of coding time is dealing with errors, and the same is true for learning to code. This can be difficult for us to accept because our experience in a typical biology course is quite different."
  },
  {
    "objectID": "intro.html#what-to-expect",
    "href": "intro.html#what-to-expect",
    "title": "Introduction",
    "section": "",
    "text": "Learning to code is a lifelong journey. There is always more to learn and new ways to improve. The beginning of your journey might be broken up into three overlapping stages, depending on the level of training you have already received:\n\nUtter bewilderment – reading code is like reading a foreign language. All these letters and symbols are meaningless to you.\nUnderstanding – you can look at a function and have a decent idea of what it does and how to use it, but you don’t understand most of the parameters. You usually rely on default parameters.\nCompetence – you can write your own code from scratch, without needing to look up examples, and you are able to carefully review and apply parameters. You rarely trust default parameters, especially for more complicated functions.\nExpertise – you write your own functions and help others to troubleshoot code, analysis pipelines, etc. Maybe you even have your own published R package or algorithms.\n\nDon’t confuse understanding with competence – this is a common mistake that students make. It’s relatively easy to learn how to understand code that is shown to you, but it’s quite another skill to learn the names and parameters of useful functions and apply them to solve problems or answer questions. That doesn’t mean you need to memorize every function – though memorization can help. A good strategy to move from understanding to competence is to take the extra time and make the effort to type out the code that is shown to you, even when you can look at it and understand what it does. As noted earlier, the act of typing out the code is what will help to solidify it in your brain."
  },
  {
    "objectID": "intro.html#translational-coding",
    "href": "intro.html#translational-coding",
    "title": "Introduction",
    "section": "",
    "text": "There is often a mismatch between the knowledge acquired through a university degree and the skills that employers need in their workforce. That is, newly minted university students have a lot of knowledge and skills for learning, but often struggle with goals laid out by employers or in entrepreneurial endeavours or thesis/dissertation research.\nIn the computing world, the disconnect between learning and application can happen when students have acquired knowledge of coding algorithms and tools, but learn to apply these tools only when working within a ‘sandbox’ created for teaching purposes. The sandbox is a clean and well-groomed programming environment with pre-loaded software and examples, curated by the educator. The sandbox lacks the messiness and ambiguity that define real-world applications, and the student never faces these uncomfortable but highly relevant challenges. The sandbox approach is commonly used in both university settings and online courses (e.g. Udemy, Coursera, Datacamp, Skillshare).\nA typical teaching sandbox will probably include pre-installed software with ‘clean’ data defined by a well-defined data structure without errors or missing observations. It will probably have a clear and singular path from problem to solution. This approach has the advantage of efficiency – both for the educator and for the learner. The learner can be guided to move efficiently through key learning objectives while minimizing unexpected bugs or problems that can slow progress and take significant time for educators to deal with. The sandbox creates a more homogeneous experience that is more efficient for tracking progress and assigning grades. The trade-off is that sandbox learning does a poor job preparing you for the messy realities of coding with real data in the real world.\nAn alternative to the sandbox approach is translational coding, which borrows the term from translational medicine. Translational medicine is a multidisciplinary hybrid between research and application that directly connects medical researchers to the needs of patients. By analogy, translational coding tries to directly connect coding skills and tools to the needs of potential employers.\nThis will not be pleasant for you, the learner, at first. The sandbox approach is popular with learners because it is relatively quick and painless with minimal time needed for researching, planning, debugging, and other forms of problem solving. There is value to learning to work quickly and efficiently, but there is also value in learning to deal with problems that arise in the real-world. This includes dealing with errors at every stage, from installing software to problems hidden among thousands of lines of data or code. This can be frustrating at first, and it will absolutely slow down your progress. There are three key things to remember when this happens:\n\nEvery error, problem, or roadblock is a learning opportunity. Every problem and assignment will have specific goals and challenges that are explicitly laid out by the tutorial, assignment or practice problem. These are the challenges that every learner must overcome to complete the task. In addition, there are implicit challenges that may be unique or shared by only a few learners – a particular typo in the code, an error importing or saving, an unidentified error in your dataset. These implicit problems may feel ‘unfair’ because not every learner has to deal with the same problems at the same time. Over time however, these will tend to average out so that everyone will make similar mistakes, albeit at different times.\nYou can learn to budget your time to deal with these implicit, unforeseen errors. And this is an important and highly-transferrable skill! Start a problem or assignment as soon as possible. Give yourself time to take a break and come back to a problem when you get stuck. When you estimate how long an assignment will take, don’t just look at the explicit goals. Remember to also add time for the implicit challenges, which will take much longer to complete.\nTime devoted to a new problem pays off in the future. Most of your time will be spent the first time you encounter a problem. If you take the time to read the error or warning, think about it, and investigate it, then you will know how to recognize and deal with it in the future. In this way, implicit challenges tend to balance out among learners over time. Some learners will encounter a problem early and struggle while others move ahead, until they encounter the same problem, evening the playing field.\n\nThe most important thing is to embrace the challenge! Don’t let yourself get discouraged.\nNow, let’s get set up to start coding in R."
  },
  {
    "objectID": "intro.html#r",
    "href": "intro.html#r",
    "title": "Introduction",
    "section": "R",
    "text": "R\nBefore you begin these tutorials, you should install the latest version of R: (https://cran.r-project.org/)\nVersions are available for Windows, MacOS and Linux operating systems. Immediately we can see one of the advantages of learning to code in R – we can move code across computing platforms quite easily, as long as R is installed there."
  },
  {
    "objectID": "intro.html#r-studio",
    "href": "intro.html#r-studio",
    "title": "Introduction",
    "section": "R Studio",
    "text": "R Studio\nYou should also install R Studio: (https://rstudio.com/products/rstudio/download/#download)\nR Studio is an Integrated Development Environment (IDE). Once you install R Studio, go ahead and run the program.\nYou will see several helpful tabs, probably arranged across four windows. Several windows have more than one tab at the top, which you can click to access. Here is a quick overview of the more useful ones (some of this will make more sense after you work through the first few chapters of the tutorial):\n\nEnvironment keeps track of all of the objects in your programming environment.\nHistory keeps track of the code you have run.\nFiles similar to the Finder (MacOS) or File Explorer (Windows), starting with the working directory.\nPlots are where your plots are created.\nPackages show which packages you have installed, and which have been loaded.\nHelp provides documentation for R functions.\nConsole is important enough to get its own section."
  },
  {
    "objectID": "intro.html#console",
    "href": "intro.html#console",
    "title": "Introduction",
    "section": "Console",
    "text": "Console\nThe console is one of the most important tabs in R Studio. It’s usually the main tab that opens on the left when you first start R Studio. You’ll see a little chevron (&gt;) with a cursor after it. This is the R Console, which is the part of R Studio that actually runs the R program. Everything in this window shows you what would happen if you ran the code outside of R Studio, for example on a high perfomance computing cluster like the ones maintained by Compute Canada, Microsoft Azure, Amazon Web Services, or Queen’s University’s own Centre for Advanced Computing. Everything in R studio is built around helping you to perform tasks in R, as shown through the R Console.\n\nR Script\nTo run an R script, you can just type functions into the console. However, it is very hard to keep track of everything you do if you only use the console. In R Studio you can click File--&gt;New File--&gt;R Script. This will open a new tab window called Untitled. This is called a script, but it’s really just a text file, with a .R suffix, that you can use to keep track of your R program. Try typing something into your R script – don’t worry for now if it is just some random text. Note that you can Save this file.\nNothing happens (yet). To run the script, you have to send the text from the script tab to the console tab. There are a few ways you could do this:\n\nCopy and paste manually. This works fine, but there are more efficient options.\nHighlight the code you want to run and click the Run button on the top-right corner of the script tab. The run button sends the highlighted text from the script to the console.\nIf you click the Run button without highlighting text, it will send whatever text is on the same line as your cursor.\nIf you press Ctl + Enter (Windows) or Cmd + Return (Mac) it will do the same thing – this is the shortcut for the Run button.\nThere are other options if you press the tiny triangle next to the Run button, including Run All.\nCtl/Cmd + Shift + Enter/Return is a shortcut for Run All."
  },
  {
    "objectID": "intro.html#packages",
    "href": "intro.html#packages",
    "title": "Introduction",
    "section": "Packages",
    "text": "Packages\nPackages in R contain functions – small programs that contain functions you can use. A few are loaded automatically when you start R, including the stats and base packages. One really good package is called tidyverse. The tidyverse package contains a lot of useful functions for working with different types of data, including visualizations. You’ll need to make sure you are connected to the internet and that your connection to the internet won’t be interrupted during the download.\n\nWARNING! This may take a long time to run\n\nTo install the packages, open R Studio and look for the Console tab. Type this into your console:\n\ninstall.packages(\"tidyverse\")\n\nNext, install devtools:\n\ninstall.packages(\"devtools\")\n\nThe install.packages() function downloads the package and saves it on your computer. You only need to do this one time, though you may want to do it periodically to update to the latest version of the package.\nOnce a package is installed on your computer, it will be available to run in R with the library() command. You’ll see examples of this throughout the book.\nThat’s it!\nNow, let’s get coding…"
  },
  {
    "objectID": "graphics.html",
    "href": "graphics.html",
    "title": "Advanced Graphics",
    "section": "",
    "text": "Advanced Graphics\nNow that you have a solid foundation, it’s time to learn some advanced graphing techniques. You can see that there are three more chapters here, so we are going to go deep into graphics. By the time you finish this section, you should have no problem making professional, publication-grade figures for your manuscripts and reports."
  },
  {
    "objectID": "fundamentals.html",
    "href": "fundamentals.html",
    "title": "R Fundamentals",
    "section": "",
    "text": "This chapter provides a rapid breakdown of the core functionality of R. There is a lot to cover in a very short time. You may be tempted to skip over some of these sections, but this chapter forms the foundation of future chapters. If you don’t have a solid foundation, you will have trouble building your coding skills. Remember that you can only learn coding through repetition. Take the extra time and make the effort to type out each code and run it in your console.\nI can’t stress this enough: It is important that you physically participate and code along with the examples. Type everything out. The physical act of typing into R and troubleshooting any errors you get is a crucial part of the learning process.\nIt’s very likely you will sometimes get a different result, such as a warning or error message. Don’t get frustrated! Think of it as an opportunity to work on you debugging skills. Check to make sure you don’t have any typos, like the letter l and the number 1, or \\ vs /, or missing spaces or other changes that may be hard to spot visually. If you are getting a warning, read it carefully.\n\n\n\nMake comments inside your code with the hash mark #. When you type this character, it tells the R program to ignore everything that comes after it.\nDocumentation is an important part of coding. It takes a bit of extra time to write, but it will save you a lot of time. Careful documentation will be essential when coding collaboratively, even if your collaborator is you when you wrote code six months back.\nIt’s ok to play around with code to get it working, but once you have a piece you are happy with, be sure to go back and add documentation.\nLater, we will see how to use R markdown to provide more attractive documents for reproducible analysis. But for dedicated programs, you can get creative with characters to help make long documentation more readable:\n\n# Use hastags to make comments - not read by the R console\n# Use other characters and blank lines to improve readability:\n# ------------------------- \n# My first R script \n# Today's Date\n# -------------------------\n# Add a summary description of what the script does\n# This script will...\n# And annotate individual parts of the script\n\n\n\nYou can do basic mathematical equations in R. Many of us choose to become biologists because we aren’t comfortable with mathematical equations, only to find out later how important math is for biology! As we’ll see later, coding can help to demystify mathematical equations. Let’s start with some basics:\n\nYes, type these out and look at the output!\n\n\n10 + 2 # add\n10 - 2 # subtract\n10 * 2 # multiply\n10 / 2 # divide\n10 ^ 2 # exponent\n10 %% 2 # modulo\n\n\nQuestion: Did you type this out? If not, you missed something important. Go back to the beginning of the book and read more carefully.\n\nThe modulo %% is one you may not be familiar with, but it comes in really handy in a lot of coding contexts. The modulo is just the remainder of a division. So 10 %% 2 returns a zero because 2 divides into 10 five times, but 10 %% 3 returns a 1 because three divides into 10 three times with 1 remainder.\nThis can be useful to determine whether a number (x) is even (i.e. if x %% 2 returns zero).\n\nTip: To get more practice, use R instead of your calculator app whenever you need to calculate something. It seems silly to go through the trouble to open R Studio to calculate a few numbers, but it will get you comfortable using R and R Studio, which will pay off in the long run..\n\n\n\n\nObjects and functions are the bread and butter of the R programming language. An object can take many forms, but is generally assigned from an input or to an output. This could include a letter or a number, or a set of letters or numbers, or a set of letters and numbers, or more structured types of objects that link together more complex forms of information.\nObjects are manipulated with functions. Each function has a unique name followed by a set of parentheses, which are used to define input and parameters that are used by the function, including inputs and outputs.\nIn fact, there is a function called function(). Yes, there is a function in R called function, and you can use it to write your own custom functions, but we’ll save that for later.\nFor now, just remember that functions have brackets. Brackets are used to define input and parameters that the function uses to produce output.\n\nWarning: Do not put a space between the function name and the opening bracket ( or you will generate an error.\n\n\n\n\nThe concatenate function c() is a very simple yet important and common function in R. Use it to group items together.\n\nc(1,2,3,5)\n\n[1] 1 2 3 5\n\n\nIn this function, the numbers 1, 2, 3, and 5 are the input parameters. Each number is itself an object in R.\nThe output is a type of object called a vector that contains four elements. The c() function takes four separate objects (elements) and combines them into a new object (vector). If this seems weird, take a few minutes to think it through because this difference will be important later.\nThink of a vector as part of a row or column in a spreadsheet, and an element as one of the cells, as shown in Figure 1.1. We can also have more complex objects that are equivalent to entire spreadsheets, or a combination of multiple spreadsheets and other kinds of structured data.\n\n\n\nVectors contain elements\n\n\n\n\n\nHere are some functions for common mathematical calculations. Type these out and then try changing some of the numbers in brackets to get a feel for them:\n\nabs(-10) # absolute value\nsqrt(10-1) # square root (with subtraction)\nlog(10) # natural log\nlog10(10) # log base 10\nexp(1) # power of e\nsin(pi/2) # sine function\nasin(1) # inverse sine\ncos(pi) # cosine\nacos(-1) # inverse cosine\ntan(0) # tangent\natan(0) # inverse tangent\n\n\nNote that pi is a special object containing the digits of pi. Try typing pi in the R Console and pressing Enter.\n\n\n\n\nWe can use R for rounding and truncating numbers.\n\nround(pi, digits=3) # standard rounding to 3 digits\n\n[1] 3.142\n\nfloor(pi) # round down to closest whole number\n\n[1] 3\n\nceiling(pi) # round up to closest whole number\n\n[1] 4\n\nsignif(pi, digits=2) # round to keep 2 significant digits\n\n[1] 3.1\n\n\n\nPro-tip: round() with digits=3 is a great function to avoid clutter when generating output for your reports, manuscripts, theses, and other scientific documents.\n\nLater, we’ll look at how to generate reports that incorporate code (e.g. statistical analyses) that you can quickly update with new data. Rounding the output of your R code with round() makes for much cleaner, and more readable reports. More than three digits may be necessary in a few cases, but in most cases it just adds unnecessary clutter.\n\n\n\nQuestion: What’s the proper way to round a number that ends with 5 (e.g. 1.5, 2.5, 3.5, 4.5)?\nA Twitter by evolutionary entomologist Dr. Stephen B. Heard reveals some confusion about this rule, as shown in Figure 1.2.\n\n\n\nA Twitter poll from Stephen Heard showing confusion about rounding rules, which we can investigate in R\n\n\nAnswer: One convention is to round the nearest even number. But, this is not the only convention you’ll see.\nR has the answer:\n\nround(c(1.5,2.5,3.5,4.5))\n\n[1] 2 2 4 4\n\n\nRounding numbers can produce some unexpected results. For example:\n\nround(2.675,2)\n\n[1] 2.67\n\n\nWhy not 2.68? The reason is the way that programming languages store numbers with decimals. These are called float numbers and they way they are encoded in memory can cause very slight deviations in the numbers. In this case, 2.675 is stored as 2.67499999999999982236431605997495353221893310546875, which is close enough for most calculations. However, it’s just slightly smaller than 2.675, which causes it to round down to 2.7 instead of up to 2.8 – these small differences usually don’t matter much, but in more advanced calculations they can be important. For example, models that use probabilities often add log-probabilities rather than multiply raw probabilities to avoid multiplying very small numbers that can be problematic for computers. The key is to carefully review the output of your programs and double-check your calculations.\nNotice how we nested a function inside of another function. We wrote a concatenate c() function to generate a vector of four elements. We then put that function inside of the round() function. The round function applied the rounding algorithm separately to each of the four elements created by c(), generating a vector output containing four elements – one rounded number for each of the four input numbers. This may be a bit tricky to understand, but we’ll work through more examples in this book.\n\n\n\nAn operator is used to compare objects. We’ll use these a lot when we start writing our own custom programs and functions. It also comes in handy for sub-setting your data.\n\n1 &gt; 2 # greater than\n\n[1] FALSE\n\n1 &lt; 2 # less than\n\n[1] TRUE\n\n1 &lt;= 2 # less than or equal to\n\n[1] TRUE\n\n1 == 1 # equal to\n\n[1] TRUE\n\n1 == 2 | 1 == 1 # | means 'OR'\n\n[1] TRUE\n\n1 == 2 & 1 == 1 # & means 'AND' \n\n[1] FALSE\n\n1 == 1 & 1 == 1\n\n[1] TRUE\n\n\nWe can also use! as a negation/inverse operator\n\n1 != 1 # not equal to\n\n[1] FALSE\n\n\n\n\n\nInstead of the vertical bar character |, you can use %in% with c() to check a large number of values.\n\n1 %in% c(1,2,3,4,5,6,7,8,9,10)\n\n[1] TRUE\n\n\n\n\n\nBefore we move on to the next section, take a second to look back at all the coding skills you’ve already learned: documenting code, basic math, working with objects and functions, combining objects, some advanced math functions, and comparing objects. Well done!\nSeriously, you already know enough write your own R program! Try it!\n\nMake a new file: File--&gt;New File--&gt;R Script\nWrite some code – try to use as many concepts above as you can.\nDon’t forget your documentation!\nSave the file\nRun the file and look at the output\nDebug any errors and warning messages.\nShow off your program to your friends and family\n\nYou are a coder now! Let’s take your skills to the next level.\n\n\n\n\nWhenever you are learning a new function, you should use ? and carefully read about all the parameters and outputs. The explanations can be a bit technical, which is intimidating at first. But after enough practice you will start to understand more and more of the descriptions. Let’s break it down:\n\n?round\n\n\nNote: In R Studio, the help will open in a separate ‘Help’ tab (lower, right panel in the default view)\n\n\n\nThe description gives a general overview of the function. In this case, round() is one of a set of related functions, which are all described together in the same help file\n\n\n\nThis shows the general form of the function that is run in the R Console.\n\n\n\nThis explains the ‘arguments’ of the function, which are the input values and parameters. In the case of round the arguments include a numeric vector x as input and digits as a parameter.\n\n\n\nThis help doesn’t have a Value subheading, but more complex functions do. For example, try ?lm to see the help for linear models. Values are objects created by the function as output. For example, the model coefficients and residuals are separate objects of a linear model created by the lm() function.\n\n\n\nThis explains the function(s) in greater detail, and is worth reading the first few times you use a function.\n\n\n\nThis section gives examples as reproducible code, which you can copy-paste right into your terminal.\nTo conclude, always read the help carefully when you first use a function. It’s normal to keep referring to the help every time you use a function that you aren’t too familiar with. It’s also normal that you might not understand everything in the help file. Just do your best and be persistent and over time it will start to make more sense to you. You will find these get easier as you read about more functions and try to apply whatever you can understand.\n\n\n\n\nThe ability to quickly and efficiently generate random numbers has a lot of useful applications for biologists. What are some examples?\n\nGenerating random numbers as part of an experimental design.\nSimulating ‘noise’ or stochastic processes in a biological model.\nDeveloping a null model for statistical significance testing.\nExploring ‘parameter space’ in a Maximum Likelihood model or a Markov Chain Monte Carlo simulation.\n\nIt is very easy to generate some random numbers in R, from a variety of different sampling distributions.\nThese are covered in more detail in the Distributions Chapter of the book R STATS Crash Course for Biologists , which is part of a different book (R Stats Crash Course for Biologists). For now, we’ll just focus on generating random numbers.\n\n\nPerhaps the simplest random number is a whole number (i.e. no decimal) drawn from a uniform distribution, meaning that each number has an equal probability of being selected.\n\nrunif(n=10, min=0, max=1)\n\n [1] 0.1117100 0.6569272 0.8211328 0.2509286 0.3734155 0.6324161 0.3697382\n [8] 0.9881294 0.5996457 0.2274557\n\n\n\nNote that your randomly chosen numbers will be different from the ones randomly chosen here.\n\nThe runif() function here uses 3 parameters:\n\nn – the number of random values to generate\nmin – the minimum value that can be chosen\nmax – the maximum value that can be chosen.\n\nWe’ll talk more about parameters later.\n\n\n\nOne of the most common random distributions in biology is the Gaussian distribution with parameters for mean and sd (standard deviation). Rational numbers (i.e. with decimal) closer to the mean are more likely to be chosen, with sd defining probability of sampling a value far above or below the mean value.\n\nrnorm(10, mean=0, sd=1)\n\n [1]  0.03593779  1.28266635  1.08966908  0.74950488  1.21493109 -0.04426062\n [7]  1.32068115 -0.20525943 -0.01895848  1.02895996\n\n\n\nSide note: Look what we did here. We wrote 10 instead of n=10 and the function still works! In fact, we can get away with:\n\n\nrnorm(10,0,1)\n\n [1] -0.56314215  0.07072185 -0.44125894  0.15320699 -0.64631668 -1.36350796\n [7]  1.86823425  1.60473928  0.13220764 -0.87088755\n\n\nYou can figure out the order by reading the help (?) for the function. When you are starting out, it’s a good idea to type the extra characters to specify the parameter names to avoid bugs in your code. It also makes the code more readable to others.\n\n\n\nA poisson distribution includes only whole numbers with a parameter lambda, which is analogous to the mean in the normal distribution.\nPoisson distributions are common for count data in biology – seed or egg number, for example.\n\nrpois(10, lambda=10)\n\n [1]  8 13 11 10  7 17 18  9 11  8\n\n\n\n\n\nThe binomial distribution is useful for binary outcomes – variables with only two possibilities, which can coded as 0 or 1 (or true/false). The size parameter is the number of events (e.g. number of coin flips), and the prob parameter is the probability of getting a 1 each time.\nBinomial distributions are commonly used in population genetics (e.g. sampling alleles with different frequencies).\n\nrbinom(10, size=1, prob=0.5) \n\n [1] 0 1 0 0 1 0 0 0 1 0\n\nrbinom(10, size=10, prob=0.5)\n\n [1] 4 7 3 6 4 4 3 5 3 5\n\n\n\n\n\nHere are a few other random distributions you might be familiar with:\n\n\n\nDistribution\nR function\n\n\n\n\nChi-Squared\nchisq()\n\n\nt\nt()\n\n\nF\nF()\n\n\nExponential\nexp\n\n\nLog-Normal\nLognormal\n\n\nLogistic\nLogistic\n\n\n\n\n\n\n\nIn addition to drawing random numbers from defined distributions, it is often helpful to sample from a defined input vector.\nFor example, maybe we want to generate a data frame with alternating rows for Treatment and Control. We can use the rep() function to repeat values.\n\nrep(c(\"Treatment\",\"Control\"),3)\n\n[1] \"Treatment\" \"Control\"   \"Treatment\" \"Control\"   \"Treatment\" \"Control\"  \n\n\nOr maybe we want to repeat a function, such as sampling from a normal distribution and calculating the mean of the sample. We could try rep() again.\n\nrep(mean(rnorm(1000)),3)\n\n[1] -0.04046735 -0.04046735 -0.04046735\n\n\nNote that your numbers will probably be different, due to random sampling. But there is a problem.\n\nQuestion: What is wrong with this output?\n\nAnswer: We are not repeating the nested function mean(rnorm()). Instead, we are just running it once and repeating the output.\nTo repeat the function, we use replicate() instead of rep().\n\nreplicate(3,mean(rnorm(1000)))\n\n[1]  0.030529408  0.004617699 -0.003606109\n\n\nInstead of repeating and replicating, we may want a random sample from our input vector. There are two ways to draw a random sample:\n\nWith Replacement – Randomly sample along the vector and allow for the same element to be sampled more than one. To remember this, imagine each element is a marble in a bag. When your select a specific marble, you replace it in the bag so that it can be sampled again.\nWithout Replacement – Randomly reshuffle the elements of a vector. Imagine that marbles do not get replaced in the bag, so that each element can be sampled only once.\n\nSample with replacement\n\nsample(c(1:10),10,replace=T)\n\n [1]  5 10  9  3  6  6  8 10  6  9\n\n\nSample without replacement\n\nsample(c(1:10),10,replace=F)\n\n [1]  5  6  7  8 10  4  3  1  2  9\n\n\n\n\n\nFun fact: random numbers generated by a computer are not truly random. Instead, the numbers involve a calculation that require a starting number called a seed. The seed might be the current Year/Month/Day/Hour/Minute/Second/Millisecond, which means the ‘random’ number could be determined by somebody who knows the equation and the precise time it was executed.\nIn practice, computer-generated random numbers are much more ‘random’ than numbers ‘randomly’ chosen by a human mind.\nWe can also take advantage of a computer’s pseudo-random number generation by defining the seed number. This can help with testing and debugging our code, and for writing code for research that is 100% reproducible. With the same seed, anyone can generate the exact same “random” numbers. We do this with the set.seed() function.\nCompare these outputs:\n\nrunif(5)\n\n[1] 0.4395633 0.3930605 0.4975544 0.9320028 0.5169364\n\nrunif(5)\n\n[1] 0.39510560 0.03523317 0.94210203 0.92165593 0.47039949\n\nset.seed(3)\nrunif(5)\n\n[1] 0.1680415 0.8075164 0.3849424 0.3277343 0.6021007\n\nset.seed(3)\nrunif(5)\n\n[1] 0.1680415 0.8075164 0.3849424 0.3277343 0.6021007\n\nset.seed(172834782)\nrunif(5)\n\n[1] 0.13729290 0.18587365 0.01860484 0.88440060 0.21414154\n\nset.seed(172834782)\nrunif(5)\n\n[1] 0.13729290 0.18587365 0.01860484 0.88440060 0.21414154\n\nrunif(5)\n\n[1] 0.19787402 0.84870074 0.27303904 0.12225215 0.08365613\n\n\nSee how the same ‘random’ numbers are generated with the same seed?\n\n\n\nReturning now to the concatenation function, we saw how to use use c() to concatenate single objects.\n\nc(1,2,5)\n\n[1] 1 2 5\n\n\nWe can also nest functions, for example we can use c() inside of another concatenate function.\n\nc(c(1,2,5),c(48,49,50))\n\n[1]  1  2  5 48 49 50\n\n\nIf we need to concatenate a range of whole numbers, we can simplify with the colon :\n\nc(1:10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nc(100:90)\n\n [1] 100  99  98  97  96  95  94  93  92  91  90\n\nc(-1:1)\n\n[1] -1  0  1\n\n\n\nQuestion: How could you use this to generate a set of numbers from -1.0 to 1.0 in increments of 0.1? You already have all the coding knowledge you need to do this! You just have to try combining two of the things you have learned so far.\n\nHint: Think about how many elements should be in the vector, and what kind of math operation you could use.\n\n\n\nAlternatively, you can also use seq() to generate more complicated sequences\n\nseq(-1, 1, by = 0.1)\n\n [1] -1.0 -0.9 -0.8 -0.7 -0.6 -0.5 -0.4 -0.3 -0.2 -0.1  0.0  0.1  0.2  0.3  0.4\n[16]  0.5  0.6  0.7  0.8  0.9  1.0\n\nseq(-1, 1, length=7)\n\n[1] -1.0000000 -0.6666667 -0.3333333  0.0000000  0.3333333  0.6666667  1.0000000\n\n\n\n\n\nAs noted above, the output of c() with two or more elements is a vector object that is conceptually similar to a set of rows or columns in a spreadsheet.\nUse cbind() to bind columns and rbind() to bind rows, as shown in Figures 1.3 & 1.4. The result is a two-dimensional matrix, which is conceptually similar to a spreadsheet of n rows by c columns.\n\n\n\ncbind() function combines columns\n\n\n\ncbind(1:10,10:1)\n\n      [,1] [,2]\n [1,]    1   10\n [2,]    2    9\n [3,]    3    8\n [4,]    4    7\n [5,]    5    6\n [6,]    6    5\n [7,]    7    4\n [8,]    8    3\n [9,]    9    2\n[10,]   10    1\n\n\n\n\n\nrbind() function combines rows\n\n\n\nrbind(1:10,10:1)\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n[1,]    1    2    3    4    5    6    7    8    9    10\n[2,]   10    9    8    7    6    5    4    3    2     1\n\n\n\nWhat are n (number of rows) and c (number of columns) for each of the above examples?\n\n\n\nOkay, let’s take a quick breather from writing code. You have been typing along, right? If not, go back and type out the code. It really is so important if you want to learn this!\nWe are about to delve deeper into the realm of object-oriented programming, but first we need to cover a few basic concepts.\n\n\n\n\nProgramming languages like R use different data types.\nIt’s very important to understand data types in order to properly encode and analyze data in R. Here is an overview of the main data types:\n\n\n\n\n\n\n\n\nType\nExample\nDescription\n\n\n\n\nstring\n\"String\"\nStrings are the most common and versatile data type. They can be defined with single ('') or double (\"\") quotation marks. The downside of strings is that you typically can’t do mathematical functions with them.\n\n\nnumeric (float)\n12.421\nNumeric variables are numbers and come in a few flavours. Floats are rational numbers.\n\n\nnumeric (integer)\n12\nIntegers are numeric objects that store whole numbers, and may be positive or negative (no decimal).\n\n\ncomplex\n0+12.43i\nComplex numbers include real and imaginary numbers.\n\n\nlogical\nT or TRUE\nLogical (aka Boolean) variables are either TRUE or FALSE, which can be shortened to T and F (Note the use of capital letters only). NOTE: TRUE and T are a special logical data type and are interchangeable in R, but \"TRUE\" and 'TRUE' and \"T\" with quotation marks are strings and are treated as separate entities.\n\n\nfactors\nany\nFactors are a special type of data that may include strings and/or numbers but have a limited number of classes. Factors are often used to code groups in statistical models.\n\n\n\nNote that computers cannot store irrational (i.e. infinite, non-repeating) numbers, instead they have encoded as fractions or equations and rounded to some (tiny) decimal place.\nWhy does it matter? It’s very common to have errors in statistical analyses caused by the wrong kind of data. Here is a very common example of a big coding error in Biology:\nImagine you have an experiment set up with three experimental groups coded as 1, 2 and 3.\n\nQuestion: What data type should these be?\n\nAnswer: These should be coded and analyzed as factors NOT numeric variables. Running statistical anlayses in R on numeric objects that should be factors will give completely different (and wrong!) statistical results.\nMore generally, you should keep these data types in mind. Consider memorizing them, or even just printing or writing them out and pasting them on your wall. When you get to a point where you are collecting your own data or working with other data sources, you will need to think carefully about which data type each observation should be coded as. This is called data coding and it is one of the most important steps in any data analysis pipeline.\n\n\n\nR supports Object-Oriented Programming (OOP), which is a programming style that defines and manipulates objects\nAs we have seen, an object in R can be a lot of things, but to understand some of the key objects, let’s start by thinking about a spreadsheet (example Microsoft Excel).\nA spreadsheet has individual cells or elements (boxes) organized into rows (e.g., numbers) and columns (e.g., letters), and may have multiple sheets (tabs). Any of these can be coded objects in R. Objects can also be more complicated types of text files. In biology, we might have DNA (or RNA or protein) sequence data, or matrices of species community data, or time series, or the output of a statistical test. All of these can be coded as objects in R.\nVariables are objects that can change value. In R, we can assign variables using &lt;- or =. Almost everything you need to know about R to be a prolific data scientist in biology involves manipulating object variables with functions!\n\n\nThe most basic object is a single value. For example, a string:\n\nX&lt;-\"string\"\n\n\nQuestion: Why no output?\n\nAnswer: When we wrote: X&lt;-\"string\" R created the object called X. The value of \"string\" is stored in the R object called X, so no output is produced.\nThere are a few options to see the contents of X:\n\nprint(X)\n\n[1] \"string\"\n\n\nprint() Is most generic and versatile for providing feedback while running complex scripts (e.g. during loops, Bash scripts, etc)\n\npaste(X)\n\n[1] \"string\"\n\n\npaste() Converts objects to a string, we’ll come back to this.\n\nX\n\n[1] \"string\"\n\n\nGenerally print() or paste() are preferred over calling the object directly.\nOR, we can put the whole thing in brackets, which saves a line of code:\n\n(X&lt;-\"string\")\n\n[1] \"string\"\n\n\nWhich one should you use? It’s ok to use the bracket methods for simple scripts and reports, but use print() for more complicated analysis pipelines, especially those that run through a scheduler on remote computers.\n\n\n\nA vector is a one-dimensional array of cells. This could be part of a row or column in our spreadsheet example.\nEach cell within the vector has an ‘address’ – a number corresponding to the cell ranging from \\(1\\) to \\(N\\), where \\(N\\) is the number of cells.\nThe number of cells in a vector is called the length of the vector.\nAll items in a vector must be of the same data type. If you mix data types, then the whole vector will be formatted to the most inclusive type. For example, if you include a string with any other format, then the whole vector will be treated as a string:\n\nXvec&lt;-c(1.1829378, X, 1:10, \"E\", \"Computational Biology\", 100:90)\nXvec\n\n [1] \"1.1829378\"             \"string\"                \"1\"                    \n [4] \"2\"                     \"3\"                     \"4\"                    \n [7] \"5\"                     \"6\"                     \"7\"                    \n[10] \"8\"                     \"9\"                     \"10\"                   \n[13] \"E\"                     \"Computational Biology\" \"100\"                  \n[16] \"99\"                    \"98\"                    \"97\"                   \n[19] \"96\"                    \"95\"                    \"94\"                   \n[22] \"93\"                    \"92\"                    \"91\"                   \n[25] \"90\"                   \n\n\nA string is more inclusive because whole numbers can be stored as strings, but there is universally accepted way to store a character string as a whole number.\nSimilarly, a vector containing integer and rational numbers is a vector of only rational numbers:\n\nc(1,2,3,1.23)\n\n[1] 1.00 2.00 3.00 1.23\n\n\n\nProtip: A common problem when importing data to R occurs when a column of numeric data includes at least one text value (e.g. “missing” or “&lt; 1”). R will treat the entire column as text rather than numeric values. Watch for this when working with real data!\n\nIf you want to mix data types without converting them, you can use a list object, which is described later. But first we will need to get comfortable working with the more basic data types.\n\n\nEach cell within a vector has a specific address. Just as text message with the correct email address can find its way to your computer, you can find an element in a vector using its address. Remember that in R, addresses start with the number \\(1\\) and increase up to the total number of elements.\nR uses square brackets [] to subset a vector based on the element addresses.\n\nXvec[1]\n\n[1] \"1.1829378\"\n\nXvec[13]\n\n[1] \"E\"\n\nXvec[1:3]\n\n[1] \"1.1829378\" \"string\"    \"1\"        \n\n\n\n\n\n\nA matrix is a 2-D array of cells, equivalent to one sheet in a spreadsheet program. The matrix() function can convert a vector to a matrix.\n\nXmat&lt;-matrix(Xvec,nrow=5)\nXmat\n\n     [,1]        [,2] [,3]                    [,4] [,5]\n[1,] \"1.1829378\" \"4\"  \"9\"                     \"99\" \"94\"\n[2,] \"string\"    \"5\"  \"10\"                    \"98\" \"93\"\n[3,] \"1\"         \"6\"  \"E\"                     \"97\" \"92\"\n[4,] \"2\"         \"7\"  \"Computational Biology\" \"96\" \"91\"\n[5,] \"3\"         \"8\"  \"100\"                   \"95\" \"90\"\n\n\nBe sure to understand what happened here. Compare this Xmat object to the Xvec object, above. See how we have re-arranged the elements of a one-dimensional vector into a two-dimensional matrix? Note: these two objects need the same number of elements – \\(1\\times25\\) for Xvec and \\(5\\times5\\) for Xmat.\n\n\nDid you notice the square brackets along the top and left side? Do you see how the rows have numbers before the comma and columns have numbers after the comma?\nThese show the address of each element in the matrix. We can subset with square brackets, just like we did with vectors. Since there are two dimensions, we need to specify two numbers using the syntax [row,column].\nFor example, if we want to select the element from the 3rd column of the 1st row:\n\nXmat[1,3]\n\n[1] \"9\"\n\n\nOr leave it blank if you want the whole row or column:\n\nXmat[1,]\n\n[1] \"1.1829378\" \"4\"         \"9\"         \"99\"        \"94\"       \n\nXmat[,3]\n\n[1] \"9\"                     \"10\"                    \"E\"                    \n[4] \"Computational Biology\" \"100\"                  \n\n\n\nProtip: Always remember [row,col]: “rows before and columns after the comma. Say it with me”Rows before columns”. Say it again, and again, until it is hard-coded in your brain.\n\n\n\n\n\nArray is a general term to describe any object with \\(N\\) dimensions. We’ve already seen a few different examples:\n\n\n\nDimension\nObject Name\n\n\n\n\n0\nCell\n\n\n1\nVector\n\n\n2\nMatrix\n\n\n3+\nArray\n\n\n\nIn R you can build arrays by adding as many dimensions as you need using the array() function.\n\nXarray&lt;-array(0, dim=c(3,3,2)) # 3 dimensions\nXarray\n\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    0    0    0\n[2,]    0    0    0\n[3,]    0    0    0\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]    0    0    0\n[2,]    0    0    0\n[3,]    0    0    0\n\n\nNotice how 3rd dimension is sliced to print out in 2D. Another way to conceptualize this array is to think of two matrices with the same dimension (rows-by-columns). The element of each matrix can be addressed by its [row,col] but we need a third dimension do distinguish between the two matrices. You can see this in the output above each matrix: ,,1 vs ,,2. Together, this array has three dimensions: [row,col,matrix].\n\nQuestion: If we add a third matrix with the same number of rows and columns, how many dimensions would you need to pull out a specific cell element in R? What if there were 10 or 100 instead of three?\n\nAnswer: Three! In each case, we whould still have a 3-dimensional array, and we can access any element as above: [row,col,matrix]. All we are changing is the matrix number from 2 to 3 to 10 to 100!\nHigher-order arrays are also possible, but a bit tricky to read on a 2-dimensional screen, and very hard to conceptualize.\nHere’s an example of a six-dimensional array.\n\nXarray&lt;-array(rnorm(64), dim=c(2,2,2,2,2,2))\n\nOnce you get the hang of it, it’s easy to subset. Just think of each dimension, separated by commas.\n\nXarray[1:2,1:2,1,1,1,1]\n\n          [,1]       [,2]\n[1,] -1.718987  0.3487603\n[2,]  1.779268 -0.3523615\n\nXarray[1:2,1,1,1:2,1,1]\n\n          [,1]      [,2]\n[1,] -1.718987 0.8664164\n[2,]  1.779268 1.2394975\n\n\n\nQuestion: Why are these numbers not the same?\n\nAnswer: Look at the array[] function and compare to the 6-D array to understand how this works. Each function captures a different 2-dimensional subspace of the 6-dimensional array\nIf these higher-dimension arrays are too abstract, don’t worry. You can get a better understanding with practice. They are important for neural networks, machine learning, and multivariate data (e.g. quantitative genetics, community ecology). Luckily, most of what you need to know you can extrapolate from your intuition about 2-dimensional and 3-dimensional space. Just make sure you understand the similarities and differences among cells/elements, vectors and matrices before you move on.\n\n\n\n\nMatrices and higher-order arrays generally all have the same data type and sub-dimension. For example, if you want to combine two separate 2D matrices into a single 3-D array, then the individual matrices have to have the same number of rows and columns. They should also have the same data type, or else everything will be converted to the most inclusive type, as noted earlier in the Vectors section.\nOften we may want to link different types of information together while still maintaining their different data types. Think of a record in a database where you may have information about an organism’s taxonomic classification (factors) height (numeric), weight (numeric), general notes and observations (string), number of scales (integer), and maybe a photograph (numeric matrix) and a DNA sequence (string vector). This wouldn’t fit neatly into an array format. Instead, we can use a list object.\nLists are useful for mixing data types, and can combine different dimensions of cells, vectors, and higher-order arrays.\nEach element in a list needs a name:\n\nMyList&lt;-list(name=\"SWC\",potpourri=Xvec,numbers=1:10)\nMyList\n\n$name\n[1] \"SWC\"\n\n$potpourri\n [1] \"1.1829378\"             \"string\"                \"1\"                    \n [4] \"2\"                     \"3\"                     \"4\"                    \n [7] \"5\"                     \"6\"                     \"7\"                    \n[10] \"8\"                     \"9\"                     \"10\"                   \n[13] \"E\"                     \"Computational Biology\" \"100\"                  \n[16] \"99\"                    \"98\"                    \"97\"                   \n[19] \"96\"                    \"95\"                    \"94\"                   \n[22] \"93\"                    \"92\"                    \"91\"                   \n[25] \"90\"                   \n\n$numbers\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\nImportant: Many of the statistical functions and other tools in R use list objects to store output. Taking some time now to think about how lists work will save time later when you need to interpret output of R functions.\n\n\n\nThere are a few different ways to subset a list object. We can subset by name using the $ character\n\nMyList$numbers # Use $ to subset by name\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nOr we can slice using square brackets.\n\nMyList[3] # A 'slice' of MyList\n\n$numbers\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nThis is similar to the way we used [] in vectors and matrices BUT note the inclusion of the name $numbers at the top of the output.\nWith lists, we have another option, to extract using double square brackets.\n\nMyList[[3]] # An 'extract' of MyList\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\nWhat’s the difference between [] and [[]]?\n\nThis is a bit tricky, but if you invest some time now to understand, you will save yourself a lot of headaches troubleshooting error messages in your code. Do your future-self a favour and take some time to understand this…\nFirst, Look carefully at the output above; notice how the [] includes $numbers but the [[]] includes only the values? This is important if you want to use the slice:\n\n2*MyList[3]\n\nError in 2 * MyList[3]: non-numeric argument to binary operator\n\n2*MyList[[3]]\n\n [1]  2  4  6  8 10 12 14 16 18 20\n\n\n\nNote the error generated in the first case\n\nThe second case is just a pure vector of numbers, that’s why we can multiply each value by two. The first case is still connected to a list object, with the $numbers indicating that we are looking at the numbers element of the list. This is part of a larger object, so R returns an error when we try to multiply a number.\nIn other words, the $numbers heading is part of the sliced object created with [] but NOT the extracted object created with [[]].\n\n\n\n\nAs noted earlier, the print function is the go-to function for printing output to the user. The paste function is useful for combining things together.\nPaste is a versatile function for manipulating output:\n\npaste(\"Hello World!\") # Basic string\n\n[1] \"Hello World!\"\n\npaste(\"Hello\",\"World!\") # Concatenate two strings\n\n[1] \"Hello World!\"\n\n\nSometimes we need to convert numbers to strings. paste is an easy way to do this:\n\npaste(1:10) # Paste numbers as strings\n\n [1] \"1\"  \"2\"  \"3\"  \"4\"  \"5\"  \"6\"  \"7\"  \"8\"  \"9\"  \"10\"\n\npaste(1:10)[4]\n\n[1] \"4\"\n\n\nNote how each number above is a separate cell in a vector of strings.\nUse as.numeric to convert strings back to numbers.\n\nas.numeric(paste(1:10)) # Convert back to numbers\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nWe can also use the collapse parameter to condense a multi-cell vector into a single cell. Reading the help for new functions reveals a lot of great treasures like this!\n\npaste(1:10,collapse=\".\")\n\n[1] \"1.2.3.4.5.6.7.8.9.10\"\n\n\n\nQuestion: What happens if we paste objects of different length?\n\n\npaste(c(\"Hello\",\"World\"),1:10,sep=\"-\") \n\n [1] \"Hello-1\"  \"World-2\"  \"Hello-3\"  \"World-4\"  \"Hello-5\"  \"World-6\" \n [7] \"Hello-7\"  \"World-8\"  \"Hello-9\"  \"World-10\"\n\n\nAnswer: The shorter vector gets recycled until every element of the longer vector is pasted.\nIt is not uncommon to nest a paste function within a print function to communicate output in more complex R scripts.\n\nprint(paste(\"Hello\",1:10,sep=\"-\"))\n\n [1] \"Hello-1\"  \"Hello-2\"  \"Hello-3\"  \"Hello-4\"  \"Hello-5\"  \"Hello-6\" \n [7] \"Hello-7\"  \"Hello-8\"  \"Hello-9\"  \"Hello-10\"\n\n\nThis would be useful if you were running a long program with many steps, maybe on a remote high-performance computer where you submit your jobs to a scheduler and you want your program to keep notes about its progress, or generate other notes or feedback. The output of paste is not shown on the screen if used inside of a loop, whereas the output of print is. More about loops below.\n\n\n\nSo far we’ve done everything within the R environment. If we quit R, then everything we have made will be removed from memory and we’ll have to start all over.\nFor larger projects and reproducible analysis, it is useful to save and load data from external files.\n\n\nThe working directory is the place on your computer where R looks to load or save your files. You can find your current working directory with the getwd() function.\n\ngetwd()\n\nNote: The output is specific to your computer, so it isn’t shown here.\n\n\n\nThe directory shown in getwd() is called an absolute path. A path is just computer jargon for the way you get to your working directory, like walking down a path in your computer, turning into the correct directory or folder each time until you reach your destination. The absolute term means that it is a location that is unchanging. The problem, for reproducible research, is that the location is specific to your user profile on your computer.\nYou can set an absolute path with setwd(). Here’s one example:\n\nsetwd(\"C:/Users/ColauttiLab/Documents\")\n\n\nDid you type out the above line? You should if you have been following the instructions! If you haven’t, go back to where you stopped and type everything out to reinforce it in your brain. Remember, going through and typing everything out is one of the most effective ways to learn to code.\n\nIf you have been typing along, you should have an error message, unless you are working in Windows and for some reason have a ColauttiLab username. Now try changing to a different directory on your computer.\nIf you are a mac user, your directory is probably similar, but without the C::\n\nsetwd(\"/Users/ColauttiLab/Documents\")\n\nAgain, you will get an error unless you replace the above with a directory that exists on your own computer.\nThis is why you should never use absolute path names in your code. You should always aim for reproducible code, and absolute paths are not reproducible on other computers.\nDon’t worry, there is a better way…\n\n\n\nIn the absolute path example above, we first go to the root directory, which is the most basic level (or the C: drive in the case of Windows). From the root directory we first click on the Users folder, then the ColauttiLab folder, and finally the Documents folder.\nIn R, we can just provide a path name as text rather than clicking through all the different folders each time. But as we have seen, the problem with absolute path names is that they are often unique to each user.\nInstead of an absolute path, we should use a relative path in our code. The relative path in R is denoted with a period, usually followed by a forward slash.\nFor example, if we have a folder called Data inside our Documents folder, and our current working directory is one of the two examples above, we can use a relative path name to set the Data folder as the working directory. Before you type this out, you should make a folder called Data inside of your current working directory, or else you will get an error.\n\nsetwd(\"./Data\")\n\nThe single dot (.) means inside of my current directory and the /Data means move into the Data folder. This is the coding equivalent of double-clicking the data Folder in your Windows File Explorer or Mac OS Finder.\nNow try running this code:\n\ngetwd()\nsetwd(\"..\")\ngetwd()\n\nCompare the working directories. The double dot (..) means go to the parent directory (i.e. directory containing the current working directory).\nThe neat thing about relative directories is that it makes it easy to share complex R code between Windows, MacOS and Linux/Unix. In fact, the syntax used by R is the same as Unix, GNU, and Linux.\nTo make relative paths reproducible, we just have to make sure that the user has all of the relevant files and directors that we are using in our main working directory. R Studio makes it easy to organize your files and code in a sharable working directory by creating an R Project folder.\n\n\n\n\nWorking with relative paths can get a little bit confusing, especially if you start using setwd(). A good way to avoid confusion is to make an R project in R Studio\nFile--&gt;New Project--&gt;New Directory--&gt;New Project\nThen make a name for your project and choose where you want to save it on your computer.\nNow quit R studio and look inside the new directory that you just created on your computer. You will see a file with the name of your project followed by .Rproj\nIf you can’t see this file, make sure you can view hidden files. Search for ‘show hidden files’ in your operating system help if you don’t know how to do this.\nThis file is an R project file, and you can double-click on it to open the project. Now here’s the cool part: Start R Studio by double-clicking the .Rproj project file instead of opening the R Studio App directly. This should open R Studio, but you will see that the project folder will be your default relative path, which you can check with getwd().\nThere are several good reasons to use R Projects, which become more obvious as you progress as a coder and start working on collaborative projects. For now, think of your project folder as your self-contained programming pipeline. In principle, you want to be able to send the project folder to somebody else to run on another computer without making any changes to the code. You can do this if you learn how to use relative path names.\n\n\n\nDownload this data file from the Colautti Lab resources website: https://colauttilab.github.io/RCrashCourse/FallopiaData.csv\nSave this as a .csv file in a directory called Data inside of your new project folder:\nNow open the file with a text editor and take a look at it.\nThe .csv suffix stands for ‘Comma Separated Values’. This is really just a regular old text file with different columns separated by commas and different rows separated by each line of text (i.e. hit ‘Enter’ to add a new row). You can see this if you try opening the file in a simple text editor (e.g. Notepad for Windows or textEdit for MacOS).\n\nPro-tip: You can easily create a .csv by choosing the Save As or Export in most spreadsheet programs (e.g. MS Excel), and choosing CSV as the output format.\n\nTo import this data into R, we can use the read.csv() function and save it as an object.\n\nMyData&lt;-read.csv(\"Data/FallopiaData.csv\",header=T)\n\nOften we have column names as the first row, so we include the parameter header=T to convert the first row to column names.\nData without column names would have data on the first row, so we would want header=F or else our first row of data would be treated as column names.\nImportant: In R, objects created by read.csv and other read.? functions are special objects called data.frame objects.\n\n\nA data.frame is a special type of object in R that is similar to a 2D matrix, but with additional indexing information for rows and columns of data. This format is partly why base R is so useful for writing a quick, reproducible data analysis.\nThere are a number of useful functions for inspecting a data.frame object.\nThe indices used for column names can accessed with the names() function\n\nnames(MyData)\n\n [1] \"PotNum\"       \"Scenario\"     \"Nutrients\"    \"Taxon\"        \"Symphytum\"   \n [6] \"Silene\"       \"Urtica\"       \"Geranium\"     \"Geum\"         \"All_Natives\" \n[11] \"Fallopia\"     \"Total\"        \"Pct_Fallopia\"\n\n\nThere are also a number of functions to quickly inspect the data.frame:\n\nShow the first six rows of data\n\n\nhead(MyData) \n\n  PotNum Scenario Nutrients Taxon Symphytum Silene Urtica Geranium Geum\n1      1      low       low japon      9.81  36.36  16.08     4.68 0.12\n2      2      low       low japon      8.64  29.65   5.59     5.75 0.55\n3      3      low       low japon      2.65  36.03  17.09     5.13 0.09\n4      5      low       low japon      1.44  21.43  12.39     5.37 0.31\n5      6      low       low japon      9.15  23.90   5.19     0.00 0.17\n6      7      low       low japon      6.31  24.40   7.00     9.05 0.97\n  All_Natives Fallopia Total Pct_Fallopia\n1       67.05     0.01 67.06         0.01\n2       50.18     0.04 50.22         0.08\n3       60.99     0.09 61.08         0.15\n4       40.94     0.77 41.71         1.85\n5       38.41     3.40 41.81         8.13\n6       47.73     0.54 48.27         1.12\n\n\n\nShow the last six rows\n\n\ntail(MyData) \n\n    PotNum     Scenario Nutrients Taxon Symphytum Silene Urtica Geranium Geum\n118    143 fluctuations      high bohem      5.06  12.81  23.82     3.64 0.16\n119    144 fluctuations      high bohem     19.93  21.07   6.08     2.80 0.43\n120    145 fluctuations      high bohem      4.89  32.93   6.30     9.64 0.00\n121    147 fluctuations      high bohem      7.84  31.16  13.61     6.58 0.03\n122    148 fluctuations      high bohem      4.15  38.70  23.59     5.11 1.36\n123    149 fluctuations      high bohem      1.72  10.41  23.48     8.51 0.43\n    All_Natives Fallopia Total Pct_Fallopia\n118       45.49    21.31 66.80        31.90\n119       50.31     0.00 50.31         0.00\n120       53.76     2.36 56.12         4.21\n121       59.22     3.74 62.96         5.94\n122       72.91     5.89 78.80         7.47\n123       44.55    19.70 64.25        30.66\n\n\n\nCheck the dimension – the number of rows and columns\n\n\ndim(MyData) \n\n[1] 123  13\n\n\n\nCheck the number of rows only\n\n\nnrow(MyData) \n\n[1] 123\n\n\n\nCheck the number of columns only\n\n\nncol(MyData) \n\n[1] 13\n\n\n\nInterrogate the structure of the data\n\n\nstr(MyData) \n\n'data.frame':   123 obs. of  13 variables:\n $ PotNum      : int  1 2 3 5 6 7 8 9 10 11 ...\n $ Scenario    : chr  \"low\" \"low\" \"low\" \"low\" ...\n $ Nutrients   : chr  \"low\" \"low\" \"low\" \"low\" ...\n $ Taxon       : chr  \"japon\" \"japon\" \"japon\" \"japon\" ...\n $ Symphytum   : num  9.81 8.64 2.65 1.44 9.15 ...\n $ Silene      : num  36.4 29.6 36 21.4 23.9 ...\n $ Urtica      : num  16.08 5.59 17.09 12.39 5.19 ...\n $ Geranium    : num  4.68 5.75 5.13 5.37 0 9.05 3.51 9.64 7.3 6.36 ...\n $ Geum        : num  0.12 0.55 0.09 0.31 0.17 0.97 0.4 0.01 0.47 0.33 ...\n $ All_Natives : num  67 50.2 61 40.9 38.4 ...\n $ Fallopia    : num  0.01 0.04 0.09 0.77 3.4 0.54 2.05 0.26 0 0 ...\n $ Total       : num  67.1 50.2 61.1 41.7 41.8 ...\n $ Pct_Fallopia: num  0.01 0.08 0.15 1.85 8.13 1.12 3.7 0.61 0 0 ...\n\n\nWe can use this to see column headers, types of data contained in each column, and the first few values in each column.\n\nProtip: str() is also useful for inspecting other objects, like the output of functions used for statistics or plotting\n\nPay careful attention to integer int vs numeric num vs factor columns in the str() output. These are the data types assigned to each column. As noted earlier, a common source of error students make when starting to analyze data is using the wrong data type.\nHere’s an example of data types gone rogue: In an analysis of variance (ANOVA), you want a factor as a predictor and a num or int as a response. But in linear regression you want int or num as a predictor instead of factor. If you code your factor (e.g. treatment) as a number (e.g. 1-4) then R will treat it as an integer when you import the data. When you run a linear model with the lm function, you will be running a regression rather than ANOVA! As a result, you will estimate a slope rather than the difference between group means.\n\n\nAlways check your data types (e.g. using str) when you first import the data.\n\n\n\n\nThe data.frame object can be subset, just like a matrix object.\n\nMyData[1,] # Returns first row of data.frame\n\n  PotNum Scenario Nutrients Taxon Symphytum Silene Urtica Geranium Geum\n1      1      low       low japon      9.81  36.36  16.08     4.68 0.12\n  All_Natives Fallopia Total Pct_Fallopia\n1       67.05     0.01 67.06         0.01\n\nMyData[1,1] # Returns first value of data.frame\n\n[1] 1\n\n\nIn addition to numbers, you can subset a column by its header.\n\nMyData[1:4,\"PotNum\"] # Returns values in \"PotNum\" column\n\n[1] 1 2 3 5\n\nMyData$PotNum[1:4] # A shortcut to subset the column\n\n[1] 1 2 3 5\n\n\nNote how we also include 1:4 to show only the first 4 elements, which reduces the output to a more manageable level. If you aren’t sure why, try running the above without 1:4 to see the difference.\nWe can also subset the data based on particular row values. For example, we can find only the records in the extreme treatment scenario.\n\nsubset(MyData,Scenario==\"low\" & Total &gt; 60) # Subset\n\n  PotNum Scenario Nutrients Taxon Symphytum Silene Urtica Geranium Geum\n1      1      low       low japon      9.81  36.36  16.08     4.68 0.12\n3      3      low       low japon      2.65  36.03  17.09     5.13 0.09\n  All_Natives Fallopia Total Pct_Fallopia\n1       67.05     0.01 67.06         0.01\n3       60.99     0.09 61.08         0.15\n\n\n\n\n\nIt’s easy to add new columns to a data frame. For example, to add a new column that is the sum of two others:\n\nMyData$NewTotal&lt;-MyData$Symphytum + MyData$Silene + MyData$Urtica\nnames(MyData)\n\n [1] \"PotNum\"       \"Scenario\"     \"Nutrients\"    \"Taxon\"        \"Symphytum\"   \n [6] \"Silene\"       \"Urtica\"       \"Geranium\"     \"Geum\"         \"All_Natives\" \n[11] \"Fallopia\"     \"Total\"        \"Pct_Fallopia\" \"NewTotal\"    \n\n\nNotice the new column added to the end. Let’s look at the first 10 values:\n\nprint(MyData$NewTotal[1:10])      \n\n [1] 62.25 43.88 55.77 35.26 38.24 37.71 49.46 32.77 45.76 39.20\n\n\n\n\n\n\nHere are a few more useful functions for inspecting your data:\n\n\nFind all the unique values within a vector using unique().\n\nunique(MyData$Nutrients)\n\n[1] \"low\"  \"high\"\n\n\n\n\n\nLook at each value in a vector and return a TRUE if it is duplicated and FALSE if it is unique.\n\nduplicated(MyData$Nutrients)\n\n  [1] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [13]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [25]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [37]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [49]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [61]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [73]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [85]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [97]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[109]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[121]  TRUE  TRUE  TRUE\n\n\n\n\n\nQuickly calculate means of one column of data (NewTotal) for each value of another column with groups (Nutrients).\n\naggregate(MyData$NewTotal,list(MyData$Nutrients), mean) \n\n  Group.1        x\n1    high 46.51173\n2     low 42.76800\n\n\nThe tilde (~) provides an alternative way to write this function. In R the tilde usually means by and it is often used in statistical models.\n\naggregate(NewTotal ~ Nutrients, data=MyData, mean)\n\n  Nutrients NewTotal\n1      high 46.51173\n2       low 42.76800\n\n\n\nHint: If you got an error, make sure you entered a tilde, not a minus sign.\n\nFor the code above, we can say “aggregate NewTotal by Nutrients grouping”.\nThe nice thing about doing it this way is that we preserve the column name. Compare the column names here: Nutrients and NewTotal, vs above: Group.1 and x.\nWe can also use the colon (:) or asterisk (*) to calculate means across different combinations of two or more grouping columns.\n\naggregate(NewTotal ~ Nutrients:Taxon:Scenario, data=MyData, mean)\n\n   Nutrients Taxon     Scenario NewTotal\n1       high bohem      extreme 45.24833\n2       high japon      extreme 45.31500\n3       high bohem fluctuations 43.89545\n4       high japon fluctuations 44.77692\n5       high bohem      gradual 45.36923\n6       high japon      gradual 50.43417\n7       high bohem         high 52.04273\n8       high japon         high 45.69417\n9        low bohem          low 41.75231\n10       low japon          low 43.86833\n\n\nNote that mean in the aggregate function is just the mean() function in R, applied inside the aggregate function. Instead of mean, we can use other functions, like the standard deviation sd:\n\naggregate(NewTotal ~ Nutrients, data = MyData, sd) \n\n  Nutrients  NewTotal\n1      high 11.175885\n2       low  8.227402\n\n\n\n\n\ntapply() works similarly, but using a list() function\nFor example, we can calculate means of each Nutrients group:\n\ntapply(MyData$NewTotal, list(MyData$Nutrients), mean)\n\n    high      low \n46.51173 42.76800 \n\n\nCompare this output with aggregate above. Here, the groups are the column names.\n\n\n\nThe sapply() and lapply() functions are similar in principle to tapply(), but are used to apply a function repeatedly and output the result as a vector (sapply) or list object (lapply).\nHere’s an example, where we can summarize the class of each column in our data.frame\n\nlapply(MyData, class)[1:3]\n\n$PotNum\n[1] \"integer\"\n\n$Scenario\n[1] \"character\"\n\n$Nutrients\n[1] \"character\"\n\nsapply(MyData, class)[1:3]\n\n     PotNum    Scenario   Nutrients \n  \"integer\" \"character\" \"character\" \n\n\nCompare the above with:\n\nclass(MyData)\n\n[1] \"data.frame\"\n\nclass(MyData$Taxon)\n\n[1] \"character\"\n\n\n\n\n\n\nMost of the methods above for managing and summarizing data are the classic or base R functions. More recently, the tidyverse group of functions has gained popularity and these functions have a lot of advantages over the classic tools, particularly for complex data management.\nFor example, it is easy to string together multiple steps into a single ‘pipe’ of data reorganization. The Data Science Chapter in this book introduces the dplyr package as an introduction to the tidyverse.\n\n\n\nJust as we can load FROM external files, we can save TO external files. We just change read to write. For a CSV file:\n\n## Calulate means\nNutrientMeans&lt;-tapply(MyData$NewTotal,list(MyData$Nutrients),mean)\n## Save means as .csv file\nwrite.csv(NutrientMeans,\"MyData_Nutrient_Means.csv\", \n          row.names=F)\n\n\nNote: the default for write.csv() adds a column of row names (i.e. numbers) to the output file. To avoid this, use row.names=F\n\nYou should see a file called MyData_Nutrients_Means.csv in your working directory.\n\n\nLarger projects may generate a lot of different output files, which you may want to organize in an Output folder inside of your project folder. Saving to this folder is easy for relative path names, just add ./Data/ before the file name in your write.csv() function. Just make sure the folder exists before you try to save to it!\n\nwrite.csv(NutrientMeans,\n          \"./Data/MyData_Nutrient_Means.csv\",\n          row.names=F)\n\n\n\n\n\nAs noted earlier, functions in R use brackets () and generally have input and output objects as well as arguments (i.e. parameters) that affect the behaviour of the functions.\nAll of the functions in this tutorial are automatically loaded when you start R (or R Studio), but there are many more functions available. For example, our lab developed the baRcodeR package for creating unique identifier codes with printable barcodes and data sheets to help with sample management and data collection. You may find this helpful for labelling and tracking samples in your own work: https://doi.org/10.1111/2041-210X.13405\nA package in R is a set of functions grouped together. For example, the stats package is automatically loaded when you run R and contains many useful functions. You can see what package a function belongs to at the beginning of the help file:\n\n?cor\n\nThe package is shown in curly brackets at the top of the help file. In this case, we see cor {stats} telling us that the cor function is part of the stats package. You can see which packages are loaded if you click on the Packages tab in R Studio (by default it is in the bottom-right window). The loaded packages are shown with check marks.\n\n\nThere are many more packages available that are not yet installed on your computer. You will need to install a new package before you can use it. You only have to do this once, but it is a good idea to update the package periodically, especially when you update to a new version of R. This ensures that you are using the most recent version of the package.\nPackages are installed with install.packages(), with the package indicated with single or double quotation marks. When you run this code the first time, you may be prompted to choose a repository, in which case choose one that is geographically close to you.\n\ninstall.packages('baRcodeR')\n\nNote that installing a package just downloads it from an online repository (remote computer) and saves to your personal computer.\n\n\n\nTo use a package that you already have installed, you can access it two ways.\n\nYou can load the package using the library() function, giving you access to all of the functions contained within it:\n\n\nlibrary(baRcodeR)\nmake_labels()\n\nThis will generate a pop-up menu for creating barcode labels with unique identifiers.\nThis method is more common, especially when you will use multiple functions from the same package, or use the same functions multiple times.\n\nYou can use double colons (::) to call a function without loading the whole package\n\n\nbaRcodeR::make_labels()\n\nThis line of code translates to “Run the make_labels function from the baRcodeR library”. This method is convenient if you just want to use one function from a large library.\n\nDon’t forget to install a package that you want to use by either method.\n\nAnother reason to go with the second method is that some packages have functions with the same names. Let’s say you load two packages pkgA and pkgB that have different functions but both are called cor. When you run the cor function, R will assume you want the one from whichever package was most recently loaded using the library() function.\nTo avoid confusion, you can use the second method to specify which function to run:\n\npkgA::cor()\npkgB::cor()\n\n\nDid you run the above code? You should still be typing along, in which case you will see the error created when you try to use a package that is not installed on your computer.\n\n\n\nThe terms library and package are often used interchangeably. Technically, the package is the collection of functions whereas the library is the specific folder where the R packages are stored. A library may contain more than one package.\nFor the most part, you just need to know that a package and a library are a collection of functions.\n\n\n\n\n\nNow that you’ve learned how to code, let’s take a few minutes to think about best practices. It’s important to make your code readable and interpretable by collaborators, peer reviewers, and yourself 6 months from now. There are lots of opinions on this but here are a few basic suggestions:\n\nAdd documentation to explain what you are doing\nAdd spacing between parameters to improve readability\nAdd spacing on either side of &lt;- when making objects\nBreak long functions into multiple lines; add the line break after a comma to ensure that the function will run properly:\n\n\nFunction(line=1,\n         line=2,\n         line=3)\n\n\nInclude a list of variables and an overview at the top of your code, or in a separate file for larger projects.\nFollow these additional suggestions for names:\n\n\nTry to keep your names short and concise but meaningful\nUse underscore _ or capital letters in your object names to improve readability\nAlways start object names with a letter, never a number or symbol\nAvoid symbols completely\n\n\n\n\nBad\nGood\n\n\n\n\nsum(X,na.rm=T)\nsum(X, na.rm=T)\n\n\nX\nMass\n\n\nDays.To.First.Flower\nFlwr_Days or FDays\n\n\n10d.Height\nHt10d\n\n\nLength*Width\nLxW\n\n\n\nBreak up longer code across multiple lines:\nBad:\n\nMyObject&lt;-cor(c(1,2,NA,5,9,8,1,2,5),c(2,2,6,3,6,8,3,NA),...)\n\nGood:\n\nMyObject &lt;- cor(c(1,2,NA,5,9,8,1,2,5),\n                c(2,2,6,3,6,8,3,NA),\n                method=\"spearman\", na.rm=T)\n\nTo take your code to the next level, look into the Tidyverse Style Guide: https://style.tidyverse.org/index.html"
  },
  {
    "objectID": "fundamentals.html#introduction",
    "href": "fundamentals.html#introduction",
    "title": "R Fundamentals",
    "section": "",
    "text": "This chapter provides a rapid breakdown of the core functionality of R. There is a lot to cover in a very short time. You may be tempted to skip over some of these sections, but this chapter forms the foundation of future chapters. If you don’t have a solid foundation, you will have trouble building your coding skills. Remember that you can only learn coding through repetition. Take the extra time and make the effort to type out each code and run it in your console.\nI can’t stress this enough: It is important that you physically participate and code along with the examples. Type everything out. The physical act of typing into R and troubleshooting any errors you get is a crucial part of the learning process.\nIt’s very likely you will sometimes get a different result, such as a warning or error message. Don’t get frustrated! Think of it as an opportunity to work on you debugging skills. Check to make sure you don’t have any typos, like the letter l and the number 1, or \\ vs /, or missing spaces or other changes that may be hard to spot visually. If you are getting a warning, read it carefully."
  },
  {
    "objectID": "fundamentals.html#r-basics",
    "href": "fundamentals.html#r-basics",
    "title": "R Fundamentals",
    "section": "",
    "text": "Make comments inside your code with the hash mark #. When you type this character, it tells the R program to ignore everything that comes after it.\nDocumentation is an important part of coding. It takes a bit of extra time to write, but it will save you a lot of time. Careful documentation will be essential when coding collaboratively, even if your collaborator is you when you wrote code six months back.\nIt’s ok to play around with code to get it working, but once you have a piece you are happy with, be sure to go back and add documentation.\nLater, we will see how to use R markdown to provide more attractive documents for reproducible analysis. But for dedicated programs, you can get creative with characters to help make long documentation more readable:\n\n# Use hastags to make comments - not read by the R console\n# Use other characters and blank lines to improve readability:\n# ------------------------- \n# My first R script \n# Today's Date\n# -------------------------\n# Add a summary description of what the script does\n# This script will...\n# And annotate individual parts of the script\n\n\n\nYou can do basic mathematical equations in R. Many of us choose to become biologists because we aren’t comfortable with mathematical equations, only to find out later how important math is for biology! As we’ll see later, coding can help to demystify mathematical equations. Let’s start with some basics:\n\nYes, type these out and look at the output!\n\n\n10 + 2 # add\n10 - 2 # subtract\n10 * 2 # multiply\n10 / 2 # divide\n10 ^ 2 # exponent\n10 %% 2 # modulo\n\n\nQuestion: Did you type this out? If not, you missed something important. Go back to the beginning of the book and read more carefully.\n\nThe modulo %% is one you may not be familiar with, but it comes in really handy in a lot of coding contexts. The modulo is just the remainder of a division. So 10 %% 2 returns a zero because 2 divides into 10 five times, but 10 %% 3 returns a 1 because three divides into 10 three times with 1 remainder.\nThis can be useful to determine whether a number (x) is even (i.e. if x %% 2 returns zero).\n\nTip: To get more practice, use R instead of your calculator app whenever you need to calculate something. It seems silly to go through the trouble to open R Studio to calculate a few numbers, but it will get you comfortable using R and R Studio, which will pay off in the long run..\n\n\n\n\nObjects and functions are the bread and butter of the R programming language. An object can take many forms, but is generally assigned from an input or to an output. This could include a letter or a number, or a set of letters or numbers, or a set of letters and numbers, or more structured types of objects that link together more complex forms of information.\nObjects are manipulated with functions. Each function has a unique name followed by a set of parentheses, which are used to define input and parameters that are used by the function, including inputs and outputs.\nIn fact, there is a function called function(). Yes, there is a function in R called function, and you can use it to write your own custom functions, but we’ll save that for later.\nFor now, just remember that functions have brackets. Brackets are used to define input and parameters that the function uses to produce output.\n\nWarning: Do not put a space between the function name and the opening bracket ( or you will generate an error.\n\n\n\n\nThe concatenate function c() is a very simple yet important and common function in R. Use it to group items together.\n\nc(1,2,3,5)\n\n[1] 1 2 3 5\n\n\nIn this function, the numbers 1, 2, 3, and 5 are the input parameters. Each number is itself an object in R.\nThe output is a type of object called a vector that contains four elements. The c() function takes four separate objects (elements) and combines them into a new object (vector). If this seems weird, take a few minutes to think it through because this difference will be important later.\nThink of a vector as part of a row or column in a spreadsheet, and an element as one of the cells, as shown in Figure 1.1. We can also have more complex objects that are equivalent to entire spreadsheets, or a combination of multiple spreadsheets and other kinds of structured data.\n\n\n\nVectors contain elements\n\n\n\n\n\nHere are some functions for common mathematical calculations. Type these out and then try changing some of the numbers in brackets to get a feel for them:\n\nabs(-10) # absolute value\nsqrt(10-1) # square root (with subtraction)\nlog(10) # natural log\nlog10(10) # log base 10\nexp(1) # power of e\nsin(pi/2) # sine function\nasin(1) # inverse sine\ncos(pi) # cosine\nacos(-1) # inverse cosine\ntan(0) # tangent\natan(0) # inverse tangent\n\n\nNote that pi is a special object containing the digits of pi. Try typing pi in the R Console and pressing Enter.\n\n\n\n\nWe can use R for rounding and truncating numbers.\n\nround(pi, digits=3) # standard rounding to 3 digits\n\n[1] 3.142\n\nfloor(pi) # round down to closest whole number\n\n[1] 3\n\nceiling(pi) # round up to closest whole number\n\n[1] 4\n\nsignif(pi, digits=2) # round to keep 2 significant digits\n\n[1] 3.1\n\n\n\nPro-tip: round() with digits=3 is a great function to avoid clutter when generating output for your reports, manuscripts, theses, and other scientific documents.\n\nLater, we’ll look at how to generate reports that incorporate code (e.g. statistical analyses) that you can quickly update with new data. Rounding the output of your R code with round() makes for much cleaner, and more readable reports. More than three digits may be necessary in a few cases, but in most cases it just adds unnecessary clutter.\n\n\n\nQuestion: What’s the proper way to round a number that ends with 5 (e.g. 1.5, 2.5, 3.5, 4.5)?\nA Twitter by evolutionary entomologist Dr. Stephen B. Heard reveals some confusion about this rule, as shown in Figure 1.2.\n\n\n\nA Twitter poll from Stephen Heard showing confusion about rounding rules, which we can investigate in R\n\n\nAnswer: One convention is to round the nearest even number. But, this is not the only convention you’ll see.\nR has the answer:\n\nround(c(1.5,2.5,3.5,4.5))\n\n[1] 2 2 4 4\n\n\nRounding numbers can produce some unexpected results. For example:\n\nround(2.675,2)\n\n[1] 2.67\n\n\nWhy not 2.68? The reason is the way that programming languages store numbers with decimals. These are called float numbers and they way they are encoded in memory can cause very slight deviations in the numbers. In this case, 2.675 is stored as 2.67499999999999982236431605997495353221893310546875, which is close enough for most calculations. However, it’s just slightly smaller than 2.675, which causes it to round down to 2.7 instead of up to 2.8 – these small differences usually don’t matter much, but in more advanced calculations they can be important. For example, models that use probabilities often add log-probabilities rather than multiply raw probabilities to avoid multiplying very small numbers that can be problematic for computers. The key is to carefully review the output of your programs and double-check your calculations.\nNotice how we nested a function inside of another function. We wrote a concatenate c() function to generate a vector of four elements. We then put that function inside of the round() function. The round function applied the rounding algorithm separately to each of the four elements created by c(), generating a vector output containing four elements – one rounded number for each of the four input numbers. This may be a bit tricky to understand, but we’ll work through more examples in this book.\n\n\n\nAn operator is used to compare objects. We’ll use these a lot when we start writing our own custom programs and functions. It also comes in handy for sub-setting your data.\n\n1 &gt; 2 # greater than\n\n[1] FALSE\n\n1 &lt; 2 # less than\n\n[1] TRUE\n\n1 &lt;= 2 # less than or equal to\n\n[1] TRUE\n\n1 == 1 # equal to\n\n[1] TRUE\n\n1 == 2 | 1 == 1 # | means 'OR'\n\n[1] TRUE\n\n1 == 2 & 1 == 1 # & means 'AND' \n\n[1] FALSE\n\n1 == 1 & 1 == 1\n\n[1] TRUE\n\n\nWe can also use! as a negation/inverse operator\n\n1 != 1 # not equal to\n\n[1] FALSE\n\n\n\n\n\nInstead of the vertical bar character |, you can use %in% with c() to check a large number of values.\n\n1 %in% c(1,2,3,4,5,6,7,8,9,10)\n\n[1] TRUE\n\n\n\n\n\nBefore we move on to the next section, take a second to look back at all the coding skills you’ve already learned: documenting code, basic math, working with objects and functions, combining objects, some advanced math functions, and comparing objects. Well done!\nSeriously, you already know enough write your own R program! Try it!\n\nMake a new file: File--&gt;New File--&gt;R Script\nWrite some code – try to use as many concepts above as you can.\nDon’t forget your documentation!\nSave the file\nRun the file and look at the output\nDebug any errors and warning messages.\nShow off your program to your friends and family\n\nYou are a coder now! Let’s take your skills to the next level."
  },
  {
    "objectID": "fundamentals.html#use-for-help",
    "href": "fundamentals.html#use-for-help",
    "title": "R Fundamentals",
    "section": "",
    "text": "Whenever you are learning a new function, you should use ? and carefully read about all the parameters and outputs. The explanations can be a bit technical, which is intimidating at first. But after enough practice you will start to understand more and more of the descriptions. Let’s break it down:\n\n?round\n\n\nNote: In R Studio, the help will open in a separate ‘Help’ tab (lower, right panel in the default view)\n\n\n\nThe description gives a general overview of the function. In this case, round() is one of a set of related functions, which are all described together in the same help file\n\n\n\nThis shows the general form of the function that is run in the R Console.\n\n\n\nThis explains the ‘arguments’ of the function, which are the input values and parameters. In the case of round the arguments include a numeric vector x as input and digits as a parameter.\n\n\n\nThis help doesn’t have a Value subheading, but more complex functions do. For example, try ?lm to see the help for linear models. Values are objects created by the function as output. For example, the model coefficients and residuals are separate objects of a linear model created by the lm() function.\n\n\n\nThis explains the function(s) in greater detail, and is worth reading the first few times you use a function.\n\n\n\nThis section gives examples as reproducible code, which you can copy-paste right into your terminal.\nTo conclude, always read the help carefully when you first use a function. It’s normal to keep referring to the help every time you use a function that you aren’t too familiar with. It’s also normal that you might not understand everything in the help file. Just do your best and be persistent and over time it will start to make more sense to you. You will find these get easier as you read about more functions and try to apply whatever you can understand."
  },
  {
    "objectID": "fundamentals.html#random-numbers",
    "href": "fundamentals.html#random-numbers",
    "title": "R Fundamentals",
    "section": "",
    "text": "The ability to quickly and efficiently generate random numbers has a lot of useful applications for biologists. What are some examples?\n\nGenerating random numbers as part of an experimental design.\nSimulating ‘noise’ or stochastic processes in a biological model.\nDeveloping a null model for statistical significance testing.\nExploring ‘parameter space’ in a Maximum Likelihood model or a Markov Chain Monte Carlo simulation.\n\nIt is very easy to generate some random numbers in R, from a variety of different sampling distributions.\nThese are covered in more detail in the Distributions Chapter of the book R STATS Crash Course for Biologists , which is part of a different book (R Stats Crash Course for Biologists). For now, we’ll just focus on generating random numbers.\n\n\nPerhaps the simplest random number is a whole number (i.e. no decimal) drawn from a uniform distribution, meaning that each number has an equal probability of being selected.\n\nrunif(n=10, min=0, max=1)\n\n [1] 0.1117100 0.6569272 0.8211328 0.2509286 0.3734155 0.6324161 0.3697382\n [8] 0.9881294 0.5996457 0.2274557\n\n\n\nNote that your randomly chosen numbers will be different from the ones randomly chosen here.\n\nThe runif() function here uses 3 parameters:\n\nn – the number of random values to generate\nmin – the minimum value that can be chosen\nmax – the maximum value that can be chosen.\n\nWe’ll talk more about parameters later.\n\n\n\nOne of the most common random distributions in biology is the Gaussian distribution with parameters for mean and sd (standard deviation). Rational numbers (i.e. with decimal) closer to the mean are more likely to be chosen, with sd defining probability of sampling a value far above or below the mean value.\n\nrnorm(10, mean=0, sd=1)\n\n [1]  0.03593779  1.28266635  1.08966908  0.74950488  1.21493109 -0.04426062\n [7]  1.32068115 -0.20525943 -0.01895848  1.02895996\n\n\n\nSide note: Look what we did here. We wrote 10 instead of n=10 and the function still works! In fact, we can get away with:\n\n\nrnorm(10,0,1)\n\n [1] -0.56314215  0.07072185 -0.44125894  0.15320699 -0.64631668 -1.36350796\n [7]  1.86823425  1.60473928  0.13220764 -0.87088755\n\n\nYou can figure out the order by reading the help (?) for the function. When you are starting out, it’s a good idea to type the extra characters to specify the parameter names to avoid bugs in your code. It also makes the code more readable to others.\n\n\n\nA poisson distribution includes only whole numbers with a parameter lambda, which is analogous to the mean in the normal distribution.\nPoisson distributions are common for count data in biology – seed or egg number, for example.\n\nrpois(10, lambda=10)\n\n [1]  8 13 11 10  7 17 18  9 11  8\n\n\n\n\n\nThe binomial distribution is useful for binary outcomes – variables with only two possibilities, which can coded as 0 or 1 (or true/false). The size parameter is the number of events (e.g. number of coin flips), and the prob parameter is the probability of getting a 1 each time.\nBinomial distributions are commonly used in population genetics (e.g. sampling alleles with different frequencies).\n\nrbinom(10, size=1, prob=0.5) \n\n [1] 0 1 0 0 1 0 0 0 1 0\n\nrbinom(10, size=10, prob=0.5)\n\n [1] 4 7 3 6 4 4 3 5 3 5\n\n\n\n\n\nHere are a few other random distributions you might be familiar with:\n\n\n\nDistribution\nR function\n\n\n\n\nChi-Squared\nchisq()\n\n\nt\nt()\n\n\nF\nF()\n\n\nExponential\nexp\n\n\nLog-Normal\nLognormal\n\n\nLogistic\nLogistic"
  },
  {
    "objectID": "fundamentals.html#repeat-replicate-sample",
    "href": "fundamentals.html#repeat-replicate-sample",
    "title": "R Fundamentals",
    "section": "",
    "text": "In addition to drawing random numbers from defined distributions, it is often helpful to sample from a defined input vector.\nFor example, maybe we want to generate a data frame with alternating rows for Treatment and Control. We can use the rep() function to repeat values.\n\nrep(c(\"Treatment\",\"Control\"),3)\n\n[1] \"Treatment\" \"Control\"   \"Treatment\" \"Control\"   \"Treatment\" \"Control\"  \n\n\nOr maybe we want to repeat a function, such as sampling from a normal distribution and calculating the mean of the sample. We could try rep() again.\n\nrep(mean(rnorm(1000)),3)\n\n[1] -0.04046735 -0.04046735 -0.04046735\n\n\nNote that your numbers will probably be different, due to random sampling. But there is a problem.\n\nQuestion: What is wrong with this output?\n\nAnswer: We are not repeating the nested function mean(rnorm()). Instead, we are just running it once and repeating the output.\nTo repeat the function, we use replicate() instead of rep().\n\nreplicate(3,mean(rnorm(1000)))\n\n[1]  0.030529408  0.004617699 -0.003606109\n\n\nInstead of repeating and replicating, we may want a random sample from our input vector. There are two ways to draw a random sample:\n\nWith Replacement – Randomly sample along the vector and allow for the same element to be sampled more than one. To remember this, imagine each element is a marble in a bag. When your select a specific marble, you replace it in the bag so that it can be sampled again.\nWithout Replacement – Randomly reshuffle the elements of a vector. Imagine that marbles do not get replaced in the bag, so that each element can be sampled only once.\n\nSample with replacement\n\nsample(c(1:10),10,replace=T)\n\n [1]  5 10  9  3  6  6  8 10  6  9\n\n\nSample without replacement\n\nsample(c(1:10),10,replace=F)\n\n [1]  5  6  7  8 10  4  3  1  2  9"
  },
  {
    "objectID": "fundamentals.html#set.seed",
    "href": "fundamentals.html#set.seed",
    "title": "R Fundamentals",
    "section": "",
    "text": "Fun fact: random numbers generated by a computer are not truly random. Instead, the numbers involve a calculation that require a starting number called a seed. The seed might be the current Year/Month/Day/Hour/Minute/Second/Millisecond, which means the ‘random’ number could be determined by somebody who knows the equation and the precise time it was executed.\nIn practice, computer-generated random numbers are much more ‘random’ than numbers ‘randomly’ chosen by a human mind.\nWe can also take advantage of a computer’s pseudo-random number generation by defining the seed number. This can help with testing and debugging our code, and for writing code for research that is 100% reproducible. With the same seed, anyone can generate the exact same “random” numbers. We do this with the set.seed() function.\nCompare these outputs:\n\nrunif(5)\n\n[1] 0.4395633 0.3930605 0.4975544 0.9320028 0.5169364\n\nrunif(5)\n\n[1] 0.39510560 0.03523317 0.94210203 0.92165593 0.47039949\n\nset.seed(3)\nrunif(5)\n\n[1] 0.1680415 0.8075164 0.3849424 0.3277343 0.6021007\n\nset.seed(3)\nrunif(5)\n\n[1] 0.1680415 0.8075164 0.3849424 0.3277343 0.6021007\n\nset.seed(172834782)\nrunif(5)\n\n[1] 0.13729290 0.18587365 0.01860484 0.88440060 0.21414154\n\nset.seed(172834782)\nrunif(5)\n\n[1] 0.13729290 0.18587365 0.01860484 0.88440060 0.21414154\n\nrunif(5)\n\n[1] 0.19787402 0.84870074 0.27303904 0.12225215 0.08365613\n\n\nSee how the same ‘random’ numbers are generated with the same seed?"
  },
  {
    "objectID": "fundamentals.html#combining-objects",
    "href": "fundamentals.html#combining-objects",
    "title": "R Fundamentals",
    "section": "",
    "text": "Returning now to the concatenation function, we saw how to use use c() to concatenate single objects.\n\nc(1,2,5)\n\n[1] 1 2 5\n\n\nWe can also nest functions, for example we can use c() inside of another concatenate function.\n\nc(c(1,2,5),c(48,49,50))\n\n[1]  1  2  5 48 49 50\n\n\nIf we need to concatenate a range of whole numbers, we can simplify with the colon :\n\nc(1:10)\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\nc(100:90)\n\n [1] 100  99  98  97  96  95  94  93  92  91  90\n\nc(-1:1)\n\n[1] -1  0  1\n\n\n\nQuestion: How could you use this to generate a set of numbers from -1.0 to 1.0 in increments of 0.1? You already have all the coding knowledge you need to do this! You just have to try combining two of the things you have learned so far.\n\nHint: Think about how many elements should be in the vector, and what kind of math operation you could use."
  },
  {
    "objectID": "fundamentals.html#sequence",
    "href": "fundamentals.html#sequence",
    "title": "R Fundamentals",
    "section": "",
    "text": "Alternatively, you can also use seq() to generate more complicated sequences\n\nseq(-1, 1, by = 0.1)\n\n [1] -1.0 -0.9 -0.8 -0.7 -0.6 -0.5 -0.4 -0.3 -0.2 -0.1  0.0  0.1  0.2  0.3  0.4\n[16]  0.5  0.6  0.7  0.8  0.9  1.0\n\nseq(-1, 1, length=7)\n\n[1] -1.0000000 -0.6666667 -0.3333333  0.0000000  0.3333333  0.6666667  1.0000000"
  },
  {
    "objectID": "fundamentals.html#rows-and-columns",
    "href": "fundamentals.html#rows-and-columns",
    "title": "R Fundamentals",
    "section": "",
    "text": "As noted above, the output of c() with two or more elements is a vector object that is conceptually similar to a set of rows or columns in a spreadsheet.\nUse cbind() to bind columns and rbind() to bind rows, as shown in Figures 1.3 & 1.4. The result is a two-dimensional matrix, which is conceptually similar to a spreadsheet of n rows by c columns.\n\n\n\ncbind() function combines columns\n\n\n\ncbind(1:10,10:1)\n\n      [,1] [,2]\n [1,]    1   10\n [2,]    2    9\n [3,]    3    8\n [4,]    4    7\n [5,]    5    6\n [6,]    6    5\n [7,]    7    4\n [8,]    8    3\n [9,]    9    2\n[10,]   10    1\n\n\n\n\n\nrbind() function combines rows\n\n\n\nrbind(1:10,10:1)\n\n     [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n[1,]    1    2    3    4    5    6    7    8    9    10\n[2,]   10    9    8    7    6    5    4    3    2     1\n\n\n\nWhat are n (number of rows) and c (number of columns) for each of the above examples?\n\n\n\nOkay, let’s take a quick breather from writing code. You have been typing along, right? If not, go back and type out the code. It really is so important if you want to learn this!\nWe are about to delve deeper into the realm of object-oriented programming, but first we need to cover a few basic concepts."
  },
  {
    "objectID": "fundamentals.html#data-types",
    "href": "fundamentals.html#data-types",
    "title": "R Fundamentals",
    "section": "",
    "text": "Programming languages like R use different data types.\nIt’s very important to understand data types in order to properly encode and analyze data in R. Here is an overview of the main data types:\n\n\n\n\n\n\n\n\nType\nExample\nDescription\n\n\n\n\nstring\n\"String\"\nStrings are the most common and versatile data type. They can be defined with single ('') or double (\"\") quotation marks. The downside of strings is that you typically can’t do mathematical functions with them.\n\n\nnumeric (float)\n12.421\nNumeric variables are numbers and come in a few flavours. Floats are rational numbers.\n\n\nnumeric (integer)\n12\nIntegers are numeric objects that store whole numbers, and may be positive or negative (no decimal).\n\n\ncomplex\n0+12.43i\nComplex numbers include real and imaginary numbers.\n\n\nlogical\nT or TRUE\nLogical (aka Boolean) variables are either TRUE or FALSE, which can be shortened to T and F (Note the use of capital letters only). NOTE: TRUE and T are a special logical data type and are interchangeable in R, but \"TRUE\" and 'TRUE' and \"T\" with quotation marks are strings and are treated as separate entities.\n\n\nfactors\nany\nFactors are a special type of data that may include strings and/or numbers but have a limited number of classes. Factors are often used to code groups in statistical models.\n\n\n\nNote that computers cannot store irrational (i.e. infinite, non-repeating) numbers, instead they have encoded as fractions or equations and rounded to some (tiny) decimal place.\nWhy does it matter? It’s very common to have errors in statistical analyses caused by the wrong kind of data. Here is a very common example of a big coding error in Biology:\nImagine you have an experiment set up with three experimental groups coded as 1, 2 and 3.\n\nQuestion: What data type should these be?\n\nAnswer: These should be coded and analyzed as factors NOT numeric variables. Running statistical anlayses in R on numeric objects that should be factors will give completely different (and wrong!) statistical results.\nMore generally, you should keep these data types in mind. Consider memorizing them, or even just printing or writing them out and pasting them on your wall. When you get to a point where you are collecting your own data or working with other data sources, you will need to think carefully about which data type each observation should be coded as. This is called data coding and it is one of the most important steps in any data analysis pipeline."
  },
  {
    "objectID": "fundamentals.html#objects-variables",
    "href": "fundamentals.html#objects-variables",
    "title": "R Fundamentals",
    "section": "",
    "text": "R supports Object-Oriented Programming (OOP), which is a programming style that defines and manipulates objects\nAs we have seen, an object in R can be a lot of things, but to understand some of the key objects, let’s start by thinking about a spreadsheet (example Microsoft Excel).\nA spreadsheet has individual cells or elements (boxes) organized into rows (e.g., numbers) and columns (e.g., letters), and may have multiple sheets (tabs). Any of these can be coded objects in R. Objects can also be more complicated types of text files. In biology, we might have DNA (or RNA or protein) sequence data, or matrices of species community data, or time series, or the output of a statistical test. All of these can be coded as objects in R.\nVariables are objects that can change value. In R, we can assign variables using &lt;- or =. Almost everything you need to know about R to be a prolific data scientist in biology involves manipulating object variables with functions!\n\n\nThe most basic object is a single value. For example, a string:\n\nX&lt;-\"string\"\n\n\nQuestion: Why no output?\n\nAnswer: When we wrote: X&lt;-\"string\" R created the object called X. The value of \"string\" is stored in the R object called X, so no output is produced.\nThere are a few options to see the contents of X:\n\nprint(X)\n\n[1] \"string\"\n\n\nprint() Is most generic and versatile for providing feedback while running complex scripts (e.g. during loops, Bash scripts, etc)\n\npaste(X)\n\n[1] \"string\"\n\n\npaste() Converts objects to a string, we’ll come back to this.\n\nX\n\n[1] \"string\"\n\n\nGenerally print() or paste() are preferred over calling the object directly.\nOR, we can put the whole thing in brackets, which saves a line of code:\n\n(X&lt;-\"string\")\n\n[1] \"string\"\n\n\nWhich one should you use? It’s ok to use the bracket methods for simple scripts and reports, but use print() for more complicated analysis pipelines, especially those that run through a scheduler on remote computers.\n\n\n\nA vector is a one-dimensional array of cells. This could be part of a row or column in our spreadsheet example.\nEach cell within the vector has an ‘address’ – a number corresponding to the cell ranging from \\(1\\) to \\(N\\), where \\(N\\) is the number of cells.\nThe number of cells in a vector is called the length of the vector.\nAll items in a vector must be of the same data type. If you mix data types, then the whole vector will be formatted to the most inclusive type. For example, if you include a string with any other format, then the whole vector will be treated as a string:\n\nXvec&lt;-c(1.1829378, X, 1:10, \"E\", \"Computational Biology\", 100:90)\nXvec\n\n [1] \"1.1829378\"             \"string\"                \"1\"                    \n [4] \"2\"                     \"3\"                     \"4\"                    \n [7] \"5\"                     \"6\"                     \"7\"                    \n[10] \"8\"                     \"9\"                     \"10\"                   \n[13] \"E\"                     \"Computational Biology\" \"100\"                  \n[16] \"99\"                    \"98\"                    \"97\"                   \n[19] \"96\"                    \"95\"                    \"94\"                   \n[22] \"93\"                    \"92\"                    \"91\"                   \n[25] \"90\"                   \n\n\nA string is more inclusive because whole numbers can be stored as strings, but there is universally accepted way to store a character string as a whole number.\nSimilarly, a vector containing integer and rational numbers is a vector of only rational numbers:\n\nc(1,2,3,1.23)\n\n[1] 1.00 2.00 3.00 1.23\n\n\n\nProtip: A common problem when importing data to R occurs when a column of numeric data includes at least one text value (e.g. “missing” or “&lt; 1”). R will treat the entire column as text rather than numeric values. Watch for this when working with real data!\n\nIf you want to mix data types without converting them, you can use a list object, which is described later. But first we will need to get comfortable working with the more basic data types.\n\n\nEach cell within a vector has a specific address. Just as text message with the correct email address can find its way to your computer, you can find an element in a vector using its address. Remember that in R, addresses start with the number \\(1\\) and increase up to the total number of elements.\nR uses square brackets [] to subset a vector based on the element addresses.\n\nXvec[1]\n\n[1] \"1.1829378\"\n\nXvec[13]\n\n[1] \"E\"\n\nXvec[1:3]\n\n[1] \"1.1829378\" \"string\"    \"1\"        \n\n\n\n\n\n\nA matrix is a 2-D array of cells, equivalent to one sheet in a spreadsheet program. The matrix() function can convert a vector to a matrix.\n\nXmat&lt;-matrix(Xvec,nrow=5)\nXmat\n\n     [,1]        [,2] [,3]                    [,4] [,5]\n[1,] \"1.1829378\" \"4\"  \"9\"                     \"99\" \"94\"\n[2,] \"string\"    \"5\"  \"10\"                    \"98\" \"93\"\n[3,] \"1\"         \"6\"  \"E\"                     \"97\" \"92\"\n[4,] \"2\"         \"7\"  \"Computational Biology\" \"96\" \"91\"\n[5,] \"3\"         \"8\"  \"100\"                   \"95\" \"90\"\n\n\nBe sure to understand what happened here. Compare this Xmat object to the Xvec object, above. See how we have re-arranged the elements of a one-dimensional vector into a two-dimensional matrix? Note: these two objects need the same number of elements – \\(1\\times25\\) for Xvec and \\(5\\times5\\) for Xmat.\n\n\nDid you notice the square brackets along the top and left side? Do you see how the rows have numbers before the comma and columns have numbers after the comma?\nThese show the address of each element in the matrix. We can subset with square brackets, just like we did with vectors. Since there are two dimensions, we need to specify two numbers using the syntax [row,column].\nFor example, if we want to select the element from the 3rd column of the 1st row:\n\nXmat[1,3]\n\n[1] \"9\"\n\n\nOr leave it blank if you want the whole row or column:\n\nXmat[1,]\n\n[1] \"1.1829378\" \"4\"         \"9\"         \"99\"        \"94\"       \n\nXmat[,3]\n\n[1] \"9\"                     \"10\"                    \"E\"                    \n[4] \"Computational Biology\" \"100\"                  \n\n\n\nProtip: Always remember [row,col]: “rows before and columns after the comma. Say it with me”Rows before columns”. Say it again, and again, until it is hard-coded in your brain.\n\n\n\n\n\nArray is a general term to describe any object with \\(N\\) dimensions. We’ve already seen a few different examples:\n\n\n\nDimension\nObject Name\n\n\n\n\n0\nCell\n\n\n1\nVector\n\n\n2\nMatrix\n\n\n3+\nArray\n\n\n\nIn R you can build arrays by adding as many dimensions as you need using the array() function.\n\nXarray&lt;-array(0, dim=c(3,3,2)) # 3 dimensions\nXarray\n\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    0    0    0\n[2,]    0    0    0\n[3,]    0    0    0\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]    0    0    0\n[2,]    0    0    0\n[3,]    0    0    0\n\n\nNotice how 3rd dimension is sliced to print out in 2D. Another way to conceptualize this array is to think of two matrices with the same dimension (rows-by-columns). The element of each matrix can be addressed by its [row,col] but we need a third dimension do distinguish between the two matrices. You can see this in the output above each matrix: ,,1 vs ,,2. Together, this array has three dimensions: [row,col,matrix].\n\nQuestion: If we add a third matrix with the same number of rows and columns, how many dimensions would you need to pull out a specific cell element in R? What if there were 10 or 100 instead of three?\n\nAnswer: Three! In each case, we whould still have a 3-dimensional array, and we can access any element as above: [row,col,matrix]. All we are changing is the matrix number from 2 to 3 to 10 to 100!\nHigher-order arrays are also possible, but a bit tricky to read on a 2-dimensional screen, and very hard to conceptualize.\nHere’s an example of a six-dimensional array.\n\nXarray&lt;-array(rnorm(64), dim=c(2,2,2,2,2,2))\n\nOnce you get the hang of it, it’s easy to subset. Just think of each dimension, separated by commas.\n\nXarray[1:2,1:2,1,1,1,1]\n\n          [,1]       [,2]\n[1,] -1.718987  0.3487603\n[2,]  1.779268 -0.3523615\n\nXarray[1:2,1,1,1:2,1,1]\n\n          [,1]      [,2]\n[1,] -1.718987 0.8664164\n[2,]  1.779268 1.2394975\n\n\n\nQuestion: Why are these numbers not the same?\n\nAnswer: Look at the array[] function and compare to the 6-D array to understand how this works. Each function captures a different 2-dimensional subspace of the 6-dimensional array\nIf these higher-dimension arrays are too abstract, don’t worry. You can get a better understanding with practice. They are important for neural networks, machine learning, and multivariate data (e.g. quantitative genetics, community ecology). Luckily, most of what you need to know you can extrapolate from your intuition about 2-dimensional and 3-dimensional space. Just make sure you understand the similarities and differences among cells/elements, vectors and matrices before you move on."
  },
  {
    "objectID": "fundamentals.html#lists",
    "href": "fundamentals.html#lists",
    "title": "R Fundamentals",
    "section": "",
    "text": "Matrices and higher-order arrays generally all have the same data type and sub-dimension. For example, if you want to combine two separate 2D matrices into a single 3-D array, then the individual matrices have to have the same number of rows and columns. They should also have the same data type, or else everything will be converted to the most inclusive type, as noted earlier in the Vectors section.\nOften we may want to link different types of information together while still maintaining their different data types. Think of a record in a database where you may have information about an organism’s taxonomic classification (factors) height (numeric), weight (numeric), general notes and observations (string), number of scales (integer), and maybe a photograph (numeric matrix) and a DNA sequence (string vector). This wouldn’t fit neatly into an array format. Instead, we can use a list object.\nLists are useful for mixing data types, and can combine different dimensions of cells, vectors, and higher-order arrays.\nEach element in a list needs a name:\n\nMyList&lt;-list(name=\"SWC\",potpourri=Xvec,numbers=1:10)\nMyList\n\n$name\n[1] \"SWC\"\n\n$potpourri\n [1] \"1.1829378\"             \"string\"                \"1\"                    \n [4] \"2\"                     \"3\"                     \"4\"                    \n [7] \"5\"                     \"6\"                     \"7\"                    \n[10] \"8\"                     \"9\"                     \"10\"                   \n[13] \"E\"                     \"Computational Biology\" \"100\"                  \n[16] \"99\"                    \"98\"                    \"97\"                   \n[19] \"96\"                    \"95\"                    \"94\"                   \n[22] \"93\"                    \"92\"                    \"91\"                   \n[25] \"90\"                   \n\n$numbers\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\nImportant: Many of the statistical functions and other tools in R use list objects to store output. Taking some time now to think about how lists work will save time later when you need to interpret output of R functions.\n\n\n\nThere are a few different ways to subset a list object. We can subset by name using the $ character\n\nMyList$numbers # Use $ to subset by name\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nOr we can slice using square brackets.\n\nMyList[3] # A 'slice' of MyList\n\n$numbers\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nThis is similar to the way we used [] in vectors and matrices BUT note the inclusion of the name $numbers at the top of the output.\nWith lists, we have another option, to extract using double square brackets.\n\nMyList[[3]] # An 'extract' of MyList\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\n\nWhat’s the difference between [] and [[]]?\n\nThis is a bit tricky, but if you invest some time now to understand, you will save yourself a lot of headaches troubleshooting error messages in your code. Do your future-self a favour and take some time to understand this…\nFirst, Look carefully at the output above; notice how the [] includes $numbers but the [[]] includes only the values? This is important if you want to use the slice:\n\n2*MyList[3]\n\nError in 2 * MyList[3]: non-numeric argument to binary operator\n\n2*MyList[[3]]\n\n [1]  2  4  6  8 10 12 14 16 18 20\n\n\n\nNote the error generated in the first case\n\nThe second case is just a pure vector of numbers, that’s why we can multiply each value by two. The first case is still connected to a list object, with the $numbers indicating that we are looking at the numbers element of the list. This is part of a larger object, so R returns an error when we try to multiply a number.\nIn other words, the $numbers heading is part of the sliced object created with [] but NOT the extracted object created with [[]]."
  },
  {
    "objectID": "fundamentals.html#print-and-paste",
    "href": "fundamentals.html#print-and-paste",
    "title": "R Fundamentals",
    "section": "",
    "text": "As noted earlier, the print function is the go-to function for printing output to the user. The paste function is useful for combining things together.\nPaste is a versatile function for manipulating output:\n\npaste(\"Hello World!\") # Basic string\n\n[1] \"Hello World!\"\n\npaste(\"Hello\",\"World!\") # Concatenate two strings\n\n[1] \"Hello World!\"\n\n\nSometimes we need to convert numbers to strings. paste is an easy way to do this:\n\npaste(1:10) # Paste numbers as strings\n\n [1] \"1\"  \"2\"  \"3\"  \"4\"  \"5\"  \"6\"  \"7\"  \"8\"  \"9\"  \"10\"\n\npaste(1:10)[4]\n\n[1] \"4\"\n\n\nNote how each number above is a separate cell in a vector of strings.\nUse as.numeric to convert strings back to numbers.\n\nas.numeric(paste(1:10)) # Convert back to numbers\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nWe can also use the collapse parameter to condense a multi-cell vector into a single cell. Reading the help for new functions reveals a lot of great treasures like this!\n\npaste(1:10,collapse=\".\")\n\n[1] \"1.2.3.4.5.6.7.8.9.10\"\n\n\n\nQuestion: What happens if we paste objects of different length?\n\n\npaste(c(\"Hello\",\"World\"),1:10,sep=\"-\") \n\n [1] \"Hello-1\"  \"World-2\"  \"Hello-3\"  \"World-4\"  \"Hello-5\"  \"World-6\" \n [7] \"Hello-7\"  \"World-8\"  \"Hello-9\"  \"World-10\"\n\n\nAnswer: The shorter vector gets recycled until every element of the longer vector is pasted.\nIt is not uncommon to nest a paste function within a print function to communicate output in more complex R scripts.\n\nprint(paste(\"Hello\",1:10,sep=\"-\"))\n\n [1] \"Hello-1\"  \"Hello-2\"  \"Hello-3\"  \"Hello-4\"  \"Hello-5\"  \"Hello-6\" \n [7] \"Hello-7\"  \"Hello-8\"  \"Hello-9\"  \"Hello-10\"\n\n\nThis would be useful if you were running a long program with many steps, maybe on a remote high-performance computer where you submit your jobs to a scheduler and you want your program to keep notes about its progress, or generate other notes or feedback. The output of paste is not shown on the screen if used inside of a loop, whereas the output of print is. More about loops below."
  },
  {
    "objectID": "fundamentals.html#external-files",
    "href": "fundamentals.html#external-files",
    "title": "R Fundamentals",
    "section": "",
    "text": "So far we’ve done everything within the R environment. If we quit R, then everything we have made will be removed from memory and we’ll have to start all over.\nFor larger projects and reproducible analysis, it is useful to save and load data from external files.\n\n\nThe working directory is the place on your computer where R looks to load or save your files. You can find your current working directory with the getwd() function.\n\ngetwd()\n\nNote: The output is specific to your computer, so it isn’t shown here.\n\n\n\nThe directory shown in getwd() is called an absolute path. A path is just computer jargon for the way you get to your working directory, like walking down a path in your computer, turning into the correct directory or folder each time until you reach your destination. The absolute term means that it is a location that is unchanging. The problem, for reproducible research, is that the location is specific to your user profile on your computer.\nYou can set an absolute path with setwd(). Here’s one example:\n\nsetwd(\"C:/Users/ColauttiLab/Documents\")\n\n\nDid you type out the above line? You should if you have been following the instructions! If you haven’t, go back to where you stopped and type everything out to reinforce it in your brain. Remember, going through and typing everything out is one of the most effective ways to learn to code.\n\nIf you have been typing along, you should have an error message, unless you are working in Windows and for some reason have a ColauttiLab username. Now try changing to a different directory on your computer.\nIf you are a mac user, your directory is probably similar, but without the C::\n\nsetwd(\"/Users/ColauttiLab/Documents\")\n\nAgain, you will get an error unless you replace the above with a directory that exists on your own computer.\nThis is why you should never use absolute path names in your code. You should always aim for reproducible code, and absolute paths are not reproducible on other computers.\nDon’t worry, there is a better way…\n\n\n\nIn the absolute path example above, we first go to the root directory, which is the most basic level (or the C: drive in the case of Windows). From the root directory we first click on the Users folder, then the ColauttiLab folder, and finally the Documents folder.\nIn R, we can just provide a path name as text rather than clicking through all the different folders each time. But as we have seen, the problem with absolute path names is that they are often unique to each user.\nInstead of an absolute path, we should use a relative path in our code. The relative path in R is denoted with a period, usually followed by a forward slash.\nFor example, if we have a folder called Data inside our Documents folder, and our current working directory is one of the two examples above, we can use a relative path name to set the Data folder as the working directory. Before you type this out, you should make a folder called Data inside of your current working directory, or else you will get an error.\n\nsetwd(\"./Data\")\n\nThe single dot (.) means inside of my current directory and the /Data means move into the Data folder. This is the coding equivalent of double-clicking the data Folder in your Windows File Explorer or Mac OS Finder.\nNow try running this code:\n\ngetwd()\nsetwd(\"..\")\ngetwd()\n\nCompare the working directories. The double dot (..) means go to the parent directory (i.e. directory containing the current working directory).\nThe neat thing about relative directories is that it makes it easy to share complex R code between Windows, MacOS and Linux/Unix. In fact, the syntax used by R is the same as Unix, GNU, and Linux.\nTo make relative paths reproducible, we just have to make sure that the user has all of the relevant files and directors that we are using in our main working directory. R Studio makes it easy to organize your files and code in a sharable working directory by creating an R Project folder."
  },
  {
    "objectID": "fundamentals.html#r-projects",
    "href": "fundamentals.html#r-projects",
    "title": "R Fundamentals",
    "section": "",
    "text": "Working with relative paths can get a little bit confusing, especially if you start using setwd(). A good way to avoid confusion is to make an R project in R Studio\nFile--&gt;New Project--&gt;New Directory--&gt;New Project\nThen make a name for your project and choose where you want to save it on your computer.\nNow quit R studio and look inside the new directory that you just created on your computer. You will see a file with the name of your project followed by .Rproj\nIf you can’t see this file, make sure you can view hidden files. Search for ‘show hidden files’ in your operating system help if you don’t know how to do this.\nThis file is an R project file, and you can double-click on it to open the project. Now here’s the cool part: Start R Studio by double-clicking the .Rproj project file instead of opening the R Studio App directly. This should open R Studio, but you will see that the project folder will be your default relative path, which you can check with getwd().\nThere are several good reasons to use R Projects, which become more obvious as you progress as a coder and start working on collaborative projects. For now, think of your project folder as your self-contained programming pipeline. In principle, you want to be able to send the project folder to somebody else to run on another computer without making any changes to the code. You can do this if you learn how to use relative path names."
  },
  {
    "objectID": "fundamentals.html#import-data",
    "href": "fundamentals.html#import-data",
    "title": "R Fundamentals",
    "section": "",
    "text": "Download this data file from the Colautti Lab resources website: https://colauttilab.github.io/RCrashCourse/FallopiaData.csv\nSave this as a .csv file in a directory called Data inside of your new project folder:\nNow open the file with a text editor and take a look at it.\nThe .csv suffix stands for ‘Comma Separated Values’. This is really just a regular old text file with different columns separated by commas and different rows separated by each line of text (i.e. hit ‘Enter’ to add a new row). You can see this if you try opening the file in a simple text editor (e.g. Notepad for Windows or textEdit for MacOS).\n\nPro-tip: You can easily create a .csv by choosing the Save As or Export in most spreadsheet programs (e.g. MS Excel), and choosing CSV as the output format.\n\nTo import this data into R, we can use the read.csv() function and save it as an object.\n\nMyData&lt;-read.csv(\"Data/FallopiaData.csv\",header=T)\n\nOften we have column names as the first row, so we include the parameter header=T to convert the first row to column names.\nData without column names would have data on the first row, so we would want header=F or else our first row of data would be treated as column names.\nImportant: In R, objects created by read.csv and other read.? functions are special objects called data.frame objects.\n\n\nA data.frame is a special type of object in R that is similar to a 2D matrix, but with additional indexing information for rows and columns of data. This format is partly why base R is so useful for writing a quick, reproducible data analysis.\nThere are a number of useful functions for inspecting a data.frame object.\nThe indices used for column names can accessed with the names() function\n\nnames(MyData)\n\n [1] \"PotNum\"       \"Scenario\"     \"Nutrients\"    \"Taxon\"        \"Symphytum\"   \n [6] \"Silene\"       \"Urtica\"       \"Geranium\"     \"Geum\"         \"All_Natives\" \n[11] \"Fallopia\"     \"Total\"        \"Pct_Fallopia\"\n\n\nThere are also a number of functions to quickly inspect the data.frame:\n\nShow the first six rows of data\n\n\nhead(MyData) \n\n  PotNum Scenario Nutrients Taxon Symphytum Silene Urtica Geranium Geum\n1      1      low       low japon      9.81  36.36  16.08     4.68 0.12\n2      2      low       low japon      8.64  29.65   5.59     5.75 0.55\n3      3      low       low japon      2.65  36.03  17.09     5.13 0.09\n4      5      low       low japon      1.44  21.43  12.39     5.37 0.31\n5      6      low       low japon      9.15  23.90   5.19     0.00 0.17\n6      7      low       low japon      6.31  24.40   7.00     9.05 0.97\n  All_Natives Fallopia Total Pct_Fallopia\n1       67.05     0.01 67.06         0.01\n2       50.18     0.04 50.22         0.08\n3       60.99     0.09 61.08         0.15\n4       40.94     0.77 41.71         1.85\n5       38.41     3.40 41.81         8.13\n6       47.73     0.54 48.27         1.12\n\n\n\nShow the last six rows\n\n\ntail(MyData) \n\n    PotNum     Scenario Nutrients Taxon Symphytum Silene Urtica Geranium Geum\n118    143 fluctuations      high bohem      5.06  12.81  23.82     3.64 0.16\n119    144 fluctuations      high bohem     19.93  21.07   6.08     2.80 0.43\n120    145 fluctuations      high bohem      4.89  32.93   6.30     9.64 0.00\n121    147 fluctuations      high bohem      7.84  31.16  13.61     6.58 0.03\n122    148 fluctuations      high bohem      4.15  38.70  23.59     5.11 1.36\n123    149 fluctuations      high bohem      1.72  10.41  23.48     8.51 0.43\n    All_Natives Fallopia Total Pct_Fallopia\n118       45.49    21.31 66.80        31.90\n119       50.31     0.00 50.31         0.00\n120       53.76     2.36 56.12         4.21\n121       59.22     3.74 62.96         5.94\n122       72.91     5.89 78.80         7.47\n123       44.55    19.70 64.25        30.66\n\n\n\nCheck the dimension – the number of rows and columns\n\n\ndim(MyData) \n\n[1] 123  13\n\n\n\nCheck the number of rows only\n\n\nnrow(MyData) \n\n[1] 123\n\n\n\nCheck the number of columns only\n\n\nncol(MyData) \n\n[1] 13\n\n\n\nInterrogate the structure of the data\n\n\nstr(MyData) \n\n'data.frame':   123 obs. of  13 variables:\n $ PotNum      : int  1 2 3 5 6 7 8 9 10 11 ...\n $ Scenario    : chr  \"low\" \"low\" \"low\" \"low\" ...\n $ Nutrients   : chr  \"low\" \"low\" \"low\" \"low\" ...\n $ Taxon       : chr  \"japon\" \"japon\" \"japon\" \"japon\" ...\n $ Symphytum   : num  9.81 8.64 2.65 1.44 9.15 ...\n $ Silene      : num  36.4 29.6 36 21.4 23.9 ...\n $ Urtica      : num  16.08 5.59 17.09 12.39 5.19 ...\n $ Geranium    : num  4.68 5.75 5.13 5.37 0 9.05 3.51 9.64 7.3 6.36 ...\n $ Geum        : num  0.12 0.55 0.09 0.31 0.17 0.97 0.4 0.01 0.47 0.33 ...\n $ All_Natives : num  67 50.2 61 40.9 38.4 ...\n $ Fallopia    : num  0.01 0.04 0.09 0.77 3.4 0.54 2.05 0.26 0 0 ...\n $ Total       : num  67.1 50.2 61.1 41.7 41.8 ...\n $ Pct_Fallopia: num  0.01 0.08 0.15 1.85 8.13 1.12 3.7 0.61 0 0 ...\n\n\nWe can use this to see column headers, types of data contained in each column, and the first few values in each column.\n\nProtip: str() is also useful for inspecting other objects, like the output of functions used for statistics or plotting\n\nPay careful attention to integer int vs numeric num vs factor columns in the str() output. These are the data types assigned to each column. As noted earlier, a common source of error students make when starting to analyze data is using the wrong data type.\nHere’s an example of data types gone rogue: In an analysis of variance (ANOVA), you want a factor as a predictor and a num or int as a response. But in linear regression you want int or num as a predictor instead of factor. If you code your factor (e.g. treatment) as a number (e.g. 1-4) then R will treat it as an integer when you import the data. When you run a linear model with the lm function, you will be running a regression rather than ANOVA! As a result, you will estimate a slope rather than the difference between group means.\n\n\nAlways check your data types (e.g. using str) when you first import the data.\n\n\n\n\nThe data.frame object can be subset, just like a matrix object.\n\nMyData[1,] # Returns first row of data.frame\n\n  PotNum Scenario Nutrients Taxon Symphytum Silene Urtica Geranium Geum\n1      1      low       low japon      9.81  36.36  16.08     4.68 0.12\n  All_Natives Fallopia Total Pct_Fallopia\n1       67.05     0.01 67.06         0.01\n\nMyData[1,1] # Returns first value of data.frame\n\n[1] 1\n\n\nIn addition to numbers, you can subset a column by its header.\n\nMyData[1:4,\"PotNum\"] # Returns values in \"PotNum\" column\n\n[1] 1 2 3 5\n\nMyData$PotNum[1:4] # A shortcut to subset the column\n\n[1] 1 2 3 5\n\n\nNote how we also include 1:4 to show only the first 4 elements, which reduces the output to a more manageable level. If you aren’t sure why, try running the above without 1:4 to see the difference.\nWe can also subset the data based on particular row values. For example, we can find only the records in the extreme treatment scenario.\n\nsubset(MyData,Scenario==\"low\" & Total &gt; 60) # Subset\n\n  PotNum Scenario Nutrients Taxon Symphytum Silene Urtica Geranium Geum\n1      1      low       low japon      9.81  36.36  16.08     4.68 0.12\n3      3      low       low japon      2.65  36.03  17.09     5.13 0.09\n  All_Natives Fallopia Total Pct_Fallopia\n1       67.05     0.01 67.06         0.01\n3       60.99     0.09 61.08         0.15\n\n\n\n\n\nIt’s easy to add new columns to a data frame. For example, to add a new column that is the sum of two others:\n\nMyData$NewTotal&lt;-MyData$Symphytum + MyData$Silene + MyData$Urtica\nnames(MyData)\n\n [1] \"PotNum\"       \"Scenario\"     \"Nutrients\"    \"Taxon\"        \"Symphytum\"   \n [6] \"Silene\"       \"Urtica\"       \"Geranium\"     \"Geum\"         \"All_Natives\" \n[11] \"Fallopia\"     \"Total\"        \"Pct_Fallopia\" \"NewTotal\"    \n\n\nNotice the new column added to the end. Let’s look at the first 10 values:\n\nprint(MyData$NewTotal[1:10])      \n\n [1] 62.25 43.88 55.77 35.26 38.24 37.71 49.46 32.77 45.76 39.20"
  },
  {
    "objectID": "fundamentals.html#other-functions",
    "href": "fundamentals.html#other-functions",
    "title": "R Fundamentals",
    "section": "",
    "text": "Here are a few more useful functions for inspecting your data:\n\n\nFind all the unique values within a vector using unique().\n\nunique(MyData$Nutrients)\n\n[1] \"low\"  \"high\"\n\n\n\n\n\nLook at each value in a vector and return a TRUE if it is duplicated and FALSE if it is unique.\n\nduplicated(MyData$Nutrients)\n\n  [1] FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [13]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [25]  TRUE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [37]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [49]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [61]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [73]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [85]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n [97]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[109]  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n[121]  TRUE  TRUE  TRUE\n\n\n\n\n\nQuickly calculate means of one column of data (NewTotal) for each value of another column with groups (Nutrients).\n\naggregate(MyData$NewTotal,list(MyData$Nutrients), mean) \n\n  Group.1        x\n1    high 46.51173\n2     low 42.76800\n\n\nThe tilde (~) provides an alternative way to write this function. In R the tilde usually means by and it is often used in statistical models.\n\naggregate(NewTotal ~ Nutrients, data=MyData, mean)\n\n  Nutrients NewTotal\n1      high 46.51173\n2       low 42.76800\n\n\n\nHint: If you got an error, make sure you entered a tilde, not a minus sign.\n\nFor the code above, we can say “aggregate NewTotal by Nutrients grouping”.\nThe nice thing about doing it this way is that we preserve the column name. Compare the column names here: Nutrients and NewTotal, vs above: Group.1 and x.\nWe can also use the colon (:) or asterisk (*) to calculate means across different combinations of two or more grouping columns.\n\naggregate(NewTotal ~ Nutrients:Taxon:Scenario, data=MyData, mean)\n\n   Nutrients Taxon     Scenario NewTotal\n1       high bohem      extreme 45.24833\n2       high japon      extreme 45.31500\n3       high bohem fluctuations 43.89545\n4       high japon fluctuations 44.77692\n5       high bohem      gradual 45.36923\n6       high japon      gradual 50.43417\n7       high bohem         high 52.04273\n8       high japon         high 45.69417\n9        low bohem          low 41.75231\n10       low japon          low 43.86833\n\n\nNote that mean in the aggregate function is just the mean() function in R, applied inside the aggregate function. Instead of mean, we can use other functions, like the standard deviation sd:\n\naggregate(NewTotal ~ Nutrients, data = MyData, sd) \n\n  Nutrients  NewTotal\n1      high 11.175885\n2       low  8.227402\n\n\n\n\n\ntapply() works similarly, but using a list() function\nFor example, we can calculate means of each Nutrients group:\n\ntapply(MyData$NewTotal, list(MyData$Nutrients), mean)\n\n    high      low \n46.51173 42.76800 \n\n\nCompare this output with aggregate above. Here, the groups are the column names.\n\n\n\nThe sapply() and lapply() functions are similar in principle to tapply(), but are used to apply a function repeatedly and output the result as a vector (sapply) or list object (lapply).\nHere’s an example, where we can summarize the class of each column in our data.frame\n\nlapply(MyData, class)[1:3]\n\n$PotNum\n[1] \"integer\"\n\n$Scenario\n[1] \"character\"\n\n$Nutrients\n[1] \"character\"\n\nsapply(MyData, class)[1:3]\n\n     PotNum    Scenario   Nutrients \n  \"integer\" \"character\" \"character\" \n\n\nCompare the above with:\n\nclass(MyData)\n\n[1] \"data.frame\"\n\nclass(MyData$Taxon)\n\n[1] \"character\""
  },
  {
    "objectID": "fundamentals.html#tidyverse",
    "href": "fundamentals.html#tidyverse",
    "title": "R Fundamentals",
    "section": "",
    "text": "Most of the methods above for managing and summarizing data are the classic or base R functions. More recently, the tidyverse group of functions has gained popularity and these functions have a lot of advantages over the classic tools, particularly for complex data management.\nFor example, it is easy to string together multiple steps into a single ‘pipe’ of data reorganization. The Data Science Chapter in this book introduces the dplyr package as an introduction to the tidyverse."
  },
  {
    "objectID": "fundamentals.html#save",
    "href": "fundamentals.html#save",
    "title": "R Fundamentals",
    "section": "",
    "text": "Just as we can load FROM external files, we can save TO external files. We just change read to write. For a CSV file:\n\n## Calulate means\nNutrientMeans&lt;-tapply(MyData$NewTotal,list(MyData$Nutrients),mean)\n## Save means as .csv file\nwrite.csv(NutrientMeans,\"MyData_Nutrient_Means.csv\", \n          row.names=F)\n\n\nNote: the default for write.csv() adds a column of row names (i.e. numbers) to the output file. To avoid this, use row.names=F\n\nYou should see a file called MyData_Nutrients_Means.csv in your working directory.\n\n\nLarger projects may generate a lot of different output files, which you may want to organize in an Output folder inside of your project folder. Saving to this folder is easy for relative path names, just add ./Data/ before the file name in your write.csv() function. Just make sure the folder exists before you try to save to it!\n\nwrite.csv(NutrientMeans,\n          \"./Data/MyData_Nutrient_Means.csv\",\n          row.names=F)"
  },
  {
    "objectID": "fundamentals.html#packages",
    "href": "fundamentals.html#packages",
    "title": "R Fundamentals",
    "section": "",
    "text": "As noted earlier, functions in R use brackets () and generally have input and output objects as well as arguments (i.e. parameters) that affect the behaviour of the functions.\nAll of the functions in this tutorial are automatically loaded when you start R (or R Studio), but there are many more functions available. For example, our lab developed the baRcodeR package for creating unique identifier codes with printable barcodes and data sheets to help with sample management and data collection. You may find this helpful for labelling and tracking samples in your own work: https://doi.org/10.1111/2041-210X.13405\nA package in R is a set of functions grouped together. For example, the stats package is automatically loaded when you run R and contains many useful functions. You can see what package a function belongs to at the beginning of the help file:\n\n?cor\n\nThe package is shown in curly brackets at the top of the help file. In this case, we see cor {stats} telling us that the cor function is part of the stats package. You can see which packages are loaded if you click on the Packages tab in R Studio (by default it is in the bottom-right window). The loaded packages are shown with check marks.\n\n\nThere are many more packages available that are not yet installed on your computer. You will need to install a new package before you can use it. You only have to do this once, but it is a good idea to update the package periodically, especially when you update to a new version of R. This ensures that you are using the most recent version of the package.\nPackages are installed with install.packages(), with the package indicated with single or double quotation marks. When you run this code the first time, you may be prompted to choose a repository, in which case choose one that is geographically close to you.\n\ninstall.packages('baRcodeR')\n\nNote that installing a package just downloads it from an online repository (remote computer) and saves to your personal computer.\n\n\n\nTo use a package that you already have installed, you can access it two ways.\n\nYou can load the package using the library() function, giving you access to all of the functions contained within it:\n\n\nlibrary(baRcodeR)\nmake_labels()\n\nThis will generate a pop-up menu for creating barcode labels with unique identifiers.\nThis method is more common, especially when you will use multiple functions from the same package, or use the same functions multiple times.\n\nYou can use double colons (::) to call a function without loading the whole package\n\n\nbaRcodeR::make_labels()\n\nThis line of code translates to “Run the make_labels function from the baRcodeR library”. This method is convenient if you just want to use one function from a large library.\n\nDon’t forget to install a package that you want to use by either method.\n\nAnother reason to go with the second method is that some packages have functions with the same names. Let’s say you load two packages pkgA and pkgB that have different functions but both are called cor. When you run the cor function, R will assume you want the one from whichever package was most recently loaded using the library() function.\nTo avoid confusion, you can use the second method to specify which function to run:\n\npkgA::cor()\npkgB::cor()\n\n\nDid you run the above code? You should still be typing along, in which case you will see the error created when you try to use a package that is not installed on your computer.\n\n\n\nThe terms library and package are often used interchangeably. Technically, the package is the collection of functions whereas the library is the specific folder where the R packages are stored. A library may contain more than one package.\nFor the most part, you just need to know that a package and a library are a collection of functions."
  },
  {
    "objectID": "fundamentals.html#readable-code",
    "href": "fundamentals.html#readable-code",
    "title": "R Fundamentals",
    "section": "",
    "text": "Now that you’ve learned how to code, let’s take a few minutes to think about best practices. It’s important to make your code readable and interpretable by collaborators, peer reviewers, and yourself 6 months from now. There are lots of opinions on this but here are a few basic suggestions:\n\nAdd documentation to explain what you are doing\nAdd spacing between parameters to improve readability\nAdd spacing on either side of &lt;- when making objects\nBreak long functions into multiple lines; add the line break after a comma to ensure that the function will run properly:\n\n\nFunction(line=1,\n         line=2,\n         line=3)\n\n\nInclude a list of variables and an overview at the top of your code, or in a separate file for larger projects.\nFollow these additional suggestions for names:\n\n\nTry to keep your names short and concise but meaningful\nUse underscore _ or capital letters in your object names to improve readability\nAlways start object names with a letter, never a number or symbol\nAvoid symbols completely\n\n\n\n\nBad\nGood\n\n\n\n\nsum(X,na.rm=T)\nsum(X, na.rm=T)\n\n\nX\nMass\n\n\nDays.To.First.Flower\nFlwr_Days or FDays\n\n\n10d.Height\nHt10d\n\n\nLength*Width\nLxW\n\n\n\nBreak up longer code across multiple lines:\nBad:\n\nMyObject&lt;-cor(c(1,2,NA,5,9,8,1,2,5),c(2,2,6,3,6,8,3,NA),...)\n\nGood:\n\nMyObject &lt;- cor(c(1,2,NA,5,9,8,1,2,5),\n                c(2,2,6,3,6,8,3,NA),\n                method=\"spearman\", na.rm=T)\n\nTo take your code to the next level, look into the Tidyverse Style Guide: https://style.tidyverse.org/index.html"
  },
  {
    "objectID": "datascience.html",
    "href": "datascience.html",
    "title": "Data Science",
    "section": "",
    "text": "Data Science is a relatively new field of study that merges computer science and statistics to answer questions in other domains (e.g. business, medicine, biology, psychology). Data Science as a discipline has grown in popularity in response to the rapid rate of increase in data collection and publication.\nData Science often involves ‘Big Data’, which doesn’t have a strict quantitative definition but will usually have one or more of the following characteristics:\n\nHigh Volume – large file sizes with may observations.\nWide Variety – many different types of data.\nHigh Velocity – data accumulates at a high rate.\nCompromised Veracity – data quality issues must be addressed otherwise downstream analyses will be compromised.\n\n\nQuestion: What are some examples of ‘big data’ in Biology?\n\nAnswer: There are many types of biological data that could be listed, but medical records, remote sensing data, and ‘omics data are common examples of ’big data’ in biology.\nIn biology, it can be helpful to think of Data Science as a continuous life-cycle with multiple stages:\n\n\n\nHypothesize – Make initial observations about the natural world, or insights from other data, that lead to testable hypotheses. Your core biology training is crucial here.\nCollect – This may involve taking measurements yourself, manually entering data that is not yet in electronic format, requesting data from authors of published studies, or importing data from online sources. Collecting data is a crucial step that is often done poorly.\nCorrect – Investigate the data for quality assurance, to identify and fix potential errors. Start to visualize the data to look for outliers, odd frequency distributions, or nonsensical relationships among variables.\nExplore – Try to understand the data, where they come from, and potential limitations on their use. Continue visualizing data; this may cause you to modify your hypotheses slightly.\nModel – Now that hypotheses are clearly defined, apply statistical tests of their validity.\nReport – Use visualizations along with the results of your statistical tests to summarise your findings.\nRepeat – Return to step 1.\n\nIn this chapter, we focus mainly on coding in R for steps 2, 3, and 6. Step 5 requires a firm understanding of statistics, which is the focus of the book R STATS Crash Course for Biologists. Visualizations in Steps 3 & 4 were covered in earlier chapters.. Step 1 requires a good understanding of the study system, as covered in a typical university degree in the biological sciences.\nData collection and management are crucial steps in the Data Science Life-Cycle. Read the baRcodeR paper by Wu et al (2022) (https://doi.org/10.1111/2041-210X.13405). called baRcodeR with PyTrackDat: Open-source labelling and tracking of biological samples for repeatable science. Pay particular attention to the ‘Data Standards’ section. The baRcodeR and PyTrackDat programs and their application to current projects may also be of interest.\n\n\n\n\nThe tidyverse library is a set of packages created by the same developers responsible for R Studio. There are a number of packages and functions that improve on base R. The name is based on the idea of living in a data universe that is neat and tidy.\nThe book R for Data Science by Hadley Wickham & Garrett Grolemund is an excellent resource for learning about the tidyverse. In this chapter we’ll touch on a few of the main packages and functions.\n\nProtip In general, any book by Hadley Wickham that you come across is worth reading if you want to be proficient in R.\n\nWhile we wait for the tidyverse to install, let’s consider a few general principles of data science.\n\n\n\nThe dplyr library in R has many useful features for importing and reorganizing your data for steps 2, 3 and 4 in the Data Science Life-Cycle.\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\nNote: This error message informs us that the dplyr package uses function or parameter names that are the same as other base or stats packages in R. These base/stats functions are ‘masked’ meaning that when you run one (e.g. filter) then R will run the dplyr version rather than the stats version.\n\n\nlibrary(tidyr)\n\nWe’ll work with our FallopiaData.csv data set, and remind ourselves of the structure of the data\n\n\nWe looked at data.frame objects in the first chapter as an expansion of matrices with a few additional features like column and row names. A tibble is the tidyverse version of the data.frame object and includes a few more useful features. To import a dataset to a tibble instead of a data.frame object, we use read_csv instead of read.csv.\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.3.2\n\n\nWarning: package 'readr' was built under R version 4.3.2\n\n\nWarning: package 'forcats' was built under R version 4.3.2\n\n\nWarning: package 'lubridate' was built under R version 4.3.2\n\n\nYou may also see a message about conflicts here, with the filter() and lag() functions from dplyr masking the functions from the stats package.\n\nFallo&lt;-read_csv(\n  \"https://colauttilab.github.io/RCrashCourse/FallopiaData.csv\")\nstr(Fallo)\n\n\n\nRows: 123 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): Scenario, Nutrients, Taxon\ndbl (10): PotNum, Symphytum, Silene, Urtica, Geranium, Geum, All_Natives, Fa...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nspc_tbl_ [123 × 13] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ PotNum      : num [1:123] 1 2 3 5 6 7 8 9 10 11 ...\n $ Scenario    : chr [1:123] \"low\" \"low\" \"low\" \"low\" ...\n $ Nutrients   : chr [1:123] \"low\" \"low\" \"low\" \"low\" ...\n $ Taxon       : chr [1:123] \"japon\" \"japon\" \"japon\" \"japon\" ...\n $ Symphytum   : num [1:123] 9.81 8.64 2.65 1.44 9.15 ...\n $ Silene      : num [1:123] 36.4 29.6 36 21.4 23.9 ...\n $ Urtica      : num [1:123] 16.08 5.59 17.09 12.39 5.19 ...\n $ Geranium    : num [1:123] 4.68 5.75 5.13 5.37 0 9.05 3.51 9.64 7.3 6.36 ...\n $ Geum        : num [1:123] 0.12 0.55 0.09 0.31 0.17 0.97 0.4 0.01 0.47 0.33 ...\n $ All_Natives : num [1:123] 67 50.2 61 40.9 38.4 ...\n $ Fallopia    : num [1:123] 0.01 0.04 0.09 0.77 3.4 0.54 2.05 0.26 0 0 ...\n $ Total       : num [1:123] 67.1 50.2 61.1 41.7 41.8 ...\n $ Pct_Fallopia: num [1:123] 0.01 0.08 0.15 1.85 8.13 1.12 3.7 0.61 0 0 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   PotNum = col_double(),\n  ..   Scenario = col_character(),\n  ..   Nutrients = col_character(),\n  ..   Taxon = col_character(),\n  ..   Symphytum = col_double(),\n  ..   Silene = col_double(),\n  ..   Urtica = col_double(),\n  ..   Geranium = col_double(),\n  ..   Geum = col_double(),\n  ..   All_Natives = col_double(),\n  ..   Fallopia = col_double(),\n  ..   Total = col_double(),\n  ..   Pct_Fallopia = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nThis file is an example of a 2-dimensional data set, which is common in biology. 2D datasets have the familiar row x column layout used by spreadsheet programs like Microsoft Excel or Google Sheets. There are some exceptions, but data in this format should typically follows 3 rules:\n\nEach cell contains a single value\nEach variable must have its own column\nEach observation must have its own row\n\nMaking sure your data are arranged this way will usually make it much easier to work with.\n\n\n\nThe filter() function will subset observations based on values of interest within particular columns of our dataset. For example, we may want to filter the rows (i.e. pots) that had at least 70 \\(g\\) of total biomass.\n\nPot1&lt;-filter(Fallo,Total &gt;= 70)\nhead(Pot1)\n\n# A tibble: 6 × 13\n  PotNum Scenario Nutrients Taxon Symphytum Silene Urtica Geranium  Geum\n   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1     60 high     high      bohem      7.77   51.4   5.13    10.1   0.37\n2     67 gradual  high      japon      2.92   25.2  19.1     23.1   0.06\n3     70 gradual  high      japon     10.1    47.0  18.6      0.64  0.82\n4     86 gradual  high      bohem      2.93   60.9   4.11     6.67  1.27\n5     95 extreme  high      japon      4.92   25.9  40.3      4.92  0.07\n6    103 extreme  high      japon      6.92   49.4   0       10.3   0.42\n# ℹ 4 more variables: All_Natives &lt;dbl&gt;, Fallopia &lt;dbl&gt;, Total &lt;dbl&gt;,\n#   Pct_Fallopia &lt;dbl&gt;\n\n\n\n\n\nThere are different options to change the names of columns in your data. In base R you can use the names() function with the square bracket index []:\n\nX&lt;-Fallo\nnames(X)\n\n [1] \"PotNum\"       \"Scenario\"     \"Nutrients\"    \"Taxon\"        \"Symphytum\"   \n [6] \"Silene\"       \"Urtica\"       \"Geranium\"     \"Geum\"         \"All_Natives\" \n[11] \"Fallopia\"     \"Total\"        \"Pct_Fallopia\"\n\nnames(X)[12]&lt;-\"Total_Biomass\"\nnames(X)\n\n [1] \"PotNum\"        \"Scenario\"      \"Nutrients\"     \"Taxon\"        \n [5] \"Symphytum\"     \"Silene\"        \"Urtica\"        \"Geranium\"     \n [9] \"Geum\"          \"All_Natives\"   \"Fallopia\"      \"Total_Biomass\"\n[13] \"Pct_Fallopia\" \n\n\nThere is also a simple dplyr function to do this:\n\nX&lt;-rename(Fallo, Total_Biomass = Total)\nnames(X)\n\n [1] \"PotNum\"        \"Scenario\"      \"Nutrients\"     \"Taxon\"        \n [5] \"Symphytum\"     \"Silene\"        \"Urtica\"        \"Geranium\"     \n [9] \"Geum\"          \"All_Natives\"   \"Fallopia\"      \"Total_Biomass\"\n[13] \"Pct_Fallopia\" \n\n\n\n\n\nUse the arrange() function to sort the rows of your data based on the values in one or more columns. For example, let’s re-arrange our FallopiaData.csv dataset based on Taxon (a string denoting the species of Fallopia used) and Total (a float denoting the total biomass in each pot).\n\nX&lt;-arrange(Fallo, Taxon, Total)\nhead(X)\n\n# A tibble: 6 × 13\n  PotNum Scenario Nutrients Taxon Symphytum Silene Urtica Geranium  Geum\n   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1     26 low      low       bohem     13.2    18.1   0        0     0.1 \n2     17 low      low       bohem      4.9    29.5   1.36     0     0.19\n3     80 gradual  high      bohem     11.9    17.2   8.92     0.94  0.18\n4     18 low      low       bohem      3.51   27.6   8.14     3.81  0.21\n5     28 low      low       bohem     10.6    18.8   7.19     6.73  0.17\n6     22 low      low       bohem      0.76   22.7   9.85    10.6   0.74\n# ℹ 4 more variables: All_Natives &lt;dbl&gt;, Fallopia &lt;dbl&gt;, Total &lt;dbl&gt;,\n#   Pct_Fallopia &lt;dbl&gt;\n\n\nuse the desc() function with arrange() to reverse and sort in descending order. We can sort by multiple columns in the by_group= parameter.\n\nX&lt;-arrange(Fallo, by_group=Taxon, desc(Silene))\nhead(X)\n\n# A tibble: 6 × 13\n  PotNum Scenario Nutrients Taxon Symphytum Silene Urtica Geranium  Geum\n   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1     86 gradual  high      bohem      2.93   60.9   4.11     6.67  1.27\n2     53 high     high      bohem      7.05   56.3   1.14     4.07  0   \n3     60 high     high      bohem      7.77   51.4   5.13    10.1   0.37\n4     50 high     high      bohem      8.52   44.6   1.27     9.45  0.35\n5     79 gradual  high      bohem     11.0    44.6   1.56     0.03  0.05\n6    107 extreme  high      bohem      0      43.8   8.1      8.18  0.36\n# ℹ 4 more variables: All_Natives &lt;dbl&gt;, Fallopia &lt;dbl&gt;, Total &lt;dbl&gt;,\n#   Pct_Fallopia &lt;dbl&gt;\n\n\n\n\n\nThe select() function can be used to select a subset of columns (i.e. variables) from your data.\nSuppose we only want to look at total biomass, but keep all the treatment columns:\n\nX&lt;-select(Fallo, PotNum, Scenario, Nutrients, Taxon, Total)\nhead(X)\n\n# A tibble: 6 × 5\n  PotNum Scenario Nutrients Taxon Total\n   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt; &lt;dbl&gt;\n1      1 low      low       japon  67.1\n2      2 low      low       japon  50.2\n3      3 low      low       japon  61.1\n4      5 low      low       japon  41.7\n5      6 low      low       japon  41.8\n6      7 low      low       japon  48.3\n\n\nYou can also use the colon : to select a range of columns:\n\nX&lt;-select(Fallo, PotNum:Taxon, Total)\nhead(X)\n\n# A tibble: 6 × 5\n  PotNum Scenario Nutrients Taxon Total\n   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt; &lt;dbl&gt;\n1      1 low      low       japon  67.1\n2      2 low      low       japon  50.2\n3      3 low      low       japon  61.1\n4      5 low      low       japon  41.7\n5      6 low      low       japon  41.8\n6      7 low      low       japon  48.3\n\n\nExclude columns with -\n\nX&lt;-select(Fallo, -PotNum:Taxon, -Total)\n\nWarning in x:y: numerical expression has 12 elements: only the first used\n\n\n\nOops, what generated that warning? Take a careful look at the error message and see if you can figure it out.\n\nThe problem is we are using the range of columns between PotNum and Taxon, but in one case we are excluding and the other we are including. We need to be consistent:\n\nX&lt;-select(Fallo, -PotNum:-Taxon, Total)\nhead(X)\n\n# A tibble: 6 × 9\n  Symphytum Silene Urtica Geranium  Geum All_Natives Fallopia Total Pct_Fallopia\n      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;\n1      9.81   36.4  16.1      4.68  0.12        67.0     0.01  67.1         0.01\n2      8.64   29.6   5.59     5.75  0.55        50.2     0.04  50.2         0.08\n3      2.65   36.0  17.1      5.13  0.09        61.0     0.09  61.1         0.15\n4      1.44   21.4  12.4      5.37  0.31        40.9     0.77  41.7         1.85\n5      9.15   23.9   5.19     0     0.17        38.4     3.4   41.8         8.13\n6      6.31   24.4   7        9.05  0.97        47.7     0.54  48.3         1.12\n\n\nOr a bit more clear:\n\nX&lt;-select(Fallo, -(PotNum:Taxon), Scenario)\nhead(X)\n\n# A tibble: 6 × 10\n  Symphytum Silene Urtica Geranium  Geum All_Natives Fallopia Total Pct_Fallopia\n      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;\n1      9.81   36.4  16.1      4.68  0.12        67.0     0.01  67.1         0.01\n2      8.64   29.6   5.59     5.75  0.55        50.2     0.04  50.2         0.08\n3      2.65   36.0  17.1      5.13  0.09        61.0     0.09  61.1         0.15\n4      1.44   21.4  12.4      5.37  0.31        40.9     0.77  41.7         1.85\n5      9.15   23.9   5.19     0     0.17        38.4     3.4   41.8         8.13\n6      6.31   24.4   7        9.05  0.97        47.7     0.54  48.3         1.12\n# ℹ 1 more variable: Scenario &lt;chr&gt;\n\n\n\n\n\nUse the everything() function with select() to rearrange your columns without losing any:\n\nX&lt;-select(Fallo, Taxon, Scenario, Nutrients, PotNum, \n          Pct_Fallopia, everything())\nhead(X)\n\n# A tibble: 6 × 13\n  Taxon Scenario Nutrients PotNum Pct_Fallopia Symphytum Silene Urtica Geranium\n  &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n1 japon low      low            1         0.01      9.81   36.4  16.1      4.68\n2 japon low      low            2         0.08      8.64   29.6   5.59     5.75\n3 japon low      low            3         0.15      2.65   36.0  17.1      5.13\n4 japon low      low            5         1.85      1.44   21.4  12.4      5.37\n5 japon low      low            6         8.13      9.15   23.9   5.19     0   \n6 japon low      low            7         1.12      6.31   24.4   7        9.05\n# ℹ 4 more variables: Geum &lt;dbl&gt;, All_Natives &lt;dbl&gt;, Fallopia &lt;dbl&gt;,\n#   Total &lt;dbl&gt;\n\n\n\n\n\nSuppose we want to make a new column to our data frame or tibble. For example, we calculate the sum of biomass of Urtica and Geranium only. In base R, we could use $ to select the column from the data frame.\n\nX&lt;-Fallo\nX$UrtSil&lt;-X$Urtica+X$Silene\n\nIn the dplyr package we can use the mutate() function.\n\nX&lt;-mutate(Fallo, UrtSil = Urtica + Silene)\nhead(X)\n\n# A tibble: 6 × 14\n  PotNum Scenario Nutrients Taxon Symphytum Silene Urtica Geranium  Geum\n   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1      1 low      low       japon      9.81   36.4  16.1      4.68  0.12\n2      2 low      low       japon      8.64   29.6   5.59     5.75  0.55\n3      3 low      low       japon      2.65   36.0  17.1      5.13  0.09\n4      5 low      low       japon      1.44   21.4  12.4      5.37  0.31\n5      6 low      low       japon      9.15   23.9   5.19     0     0.17\n6      7 low      low       japon      6.31   24.4   7        9.05  0.97\n# ℹ 5 more variables: All_Natives &lt;dbl&gt;, Fallopia &lt;dbl&gt;, Total &lt;dbl&gt;,\n#   Pct_Fallopia &lt;dbl&gt;, UrtSil &lt;dbl&gt;\n\n\nThis is a lot more readable, especially when you have complicated equations or you want to add many of new columns.\n\nQuestion: What if you only wanted to retain the new columns and delete everything else? Try it.\n\nWhich functions did you use?\n\n\n\nThe transmute() functions acts as a combination of mutate() + select()\n\nX&lt;-transmute(Fallo, UrtSil = Urtica + Silene)\nhead(X)\n\n# A tibble: 6 × 1\n  UrtSil\n   &lt;dbl&gt;\n1   52.4\n2   35.2\n3   53.1\n4   33.8\n5   29.1\n6   31.4\n\n\n\n\n\nThis can be useful for quickly summarizing your data, for example to find the mean or standard deviation based on a particular treatment or group.\n\nTrtGrp&lt;-group_by(Fallo,Taxon,Scenario,Nutrients)\nsummarise(TrtGrp, Mean=mean(Total), SD=sd(Total))\n\n`summarise()` has grouped output by 'Taxon', 'Scenario'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 10 × 5\n# Groups:   Taxon, Scenario [10]\n   Taxon Scenario     Nutrients  Mean    SD\n   &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 bohem extreme      high       58.3  7.34\n 2 bohem fluctuations high       58.4  9.20\n 3 bohem gradual      high       57.5  9.34\n 4 bohem high         high       60.3  8.68\n 5 bohem low          low        48.0  8.86\n 6 japon extreme      high       57.2 10.9 \n 7 japon fluctuations high       56.4 13.7 \n 8 japon gradual      high       59.7  9.57\n 9 japon high         high       56.4  8.20\n10 japon low          low        52.0  8.29\n\n\n\n\n\nIn our dataset, the Taxon column shows which of two species of Fallopia were used in the competition experiments. We might want to take the mean total biomass for each of the two Fallopia species.\nHowever, there are other factors in our experiment that may affect biomass. The Nutrients column tells us whether pots received high or low nutrients, and this also affects biomass:\n\nX&lt;-group_by(Fallo,Nutrients)\nsummarise(X, Mean=mean(Total), SD=sd(Total))\n\n# A tibble: 2 × 3\n  Nutrients  Mean    SD\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 high       58.0  9.61\n2 low        49.9  8.66\n\n\nWe can see that the Nutrients treatment had a large effect on the mean biomass of a pot. Now imagine if our sampling design is ‘unbalanced’. For example, maybe we had some plant mortality or lost some plants to a tornado. If one of the two species in the Taxon column had more high-nutrient pots left over, then it would have a higher mean. BUT, what if the nutrients promoted growth? We would expect to see a difference in the mean of each Taxon group, this expected difference is just an artifact of unbalanced replication of the high nutrient treatment. We can simulate this effect by re-shuffling the species names:\n\nRFallo&lt;-Fallo\nset.seed(256)\nRFallo$Taxon&lt;-rbinom(nrow(RFallo),size=1,prob=0.7)\n\nX&lt;-group_by(RFallo,Taxon)\nsummarise(X, Mean=mean(Total))\n\n# A tibble: 2 × 2\n  Taxon  Mean\n  &lt;int&gt; &lt;dbl&gt;\n1     0  56.1\n2     1  56.5\n\n\nIn this example, the difference is less than $ 1%$ of the mean (i.e., \\(0.4 / 56.1 = 0.0071 = 0.71%\\)), but this could be due to the imbalance offsetting biological differences between the taxa. In other scenarios it could be much larger. To fix this problem, we may want to take a weighted mean. In this case, we want to weight the mean of each taxon by the unbalanced Scenario and Treatment categories.\nStep 1: Group by all three columns and calculate group means\n\nX1&lt;-group_by(RFallo,Taxon,Scenario,Nutrients)\n(Y1&lt;-summarise(X1,Mean=mean(Total)))\n\n# A tibble: 10 × 4\n# Groups:   Taxon, Scenario [10]\n   Taxon Scenario     Nutrients  Mean\n   &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;     &lt;dbl&gt;\n 1     0 extreme      high       54.7\n 2     0 fluctuations high       58.4\n 3     0 gradual      high       56.2\n 4     0 high         high       59.8\n 5     0 low          low        50.2\n 6     1 extreme      high       59.1\n 7     1 fluctuations high       56.8\n 8     1 gradual      high       59.9\n 9     1 high         high       57.6\n10     1 low          low        49.8\n\n\nNow we have a single value for each of the subgroups. Instead of over-representing pots from Taxon=1, we have equal weightings of all three groups.\nStep 2: Take the output from Step 1 and calculate the mean of means for the column of interest.\n\nX2&lt;-group_by(Y1,Taxon)\nY2&lt;-summarise(X2, Mean=mean(Mean))\narrange(Y2,desc(Mean))\n\n# A tibble: 2 × 2\n  Taxon  Mean\n  &lt;int&gt; &lt;dbl&gt;\n1     1  56.6\n2     0  55.8\n\n\nNow there is more than a \\(2.1 %\\) difference (i.e., \\(1.2 / 55.8 = 0.0215\\)) between the taxa. This is triple the effect that we calculated with unbalanced samples!\n\n\n\nThe dplyr package includes a special command (%&gt;%) called a pipe. The pipe is a very handy tool for complex data wrangling. It allows us to string together multiple functions and then pipe our data from one function to the next. In principle, this is similar to the multi-function plotting commands we made with ggplot() in earlier chapters.\nThe pipe is useful to combine operations without creating a whole bunch of new objects. This can save on memory use and run speed because you are not making new objects for every single function.\n\nPro-tip: Use Ctl + Shift + m to add a pipe quickly\n\nFor example, we can re-write the weighted mean example using pipes.\n\nRFallo %&gt;% \n  group_by(Taxon,Scenario,Nutrients) %&gt;% \n  summarise(Mean=mean(Total)) %&gt;% \n  group_by(Taxon,Scenario) %&gt;% \n  summarise(Mean=mean(Mean)) %&gt;% \n  group_by(Taxon) %&gt;% \n  summarise(Mean=mean(Mean)) %&gt;% \n  arrange(desc(Mean))\n\n# A tibble: 2 × 2\n  Taxon  Mean\n  &lt;int&gt; &lt;dbl&gt;\n1     1  56.6\n2     0  55.8\n\n\nWe declare the input data set in the first line, and then pipe to group_by() in line 2. The output of group_by() is then *piped to summarise(), and so on. There are two important things to note here:\n\nWe do not declare the data object inside of each function. The pipe command does this for us.\nWe still have to assign the output to an object if we want to retain it. For example, we might change line 1 to wtMean&lt;-RFallo %&gt;%. We didn’t redirect the command to an object, so it simply output to your R Console.\n\nCompared to the earlier example, it is clear that the pipe can make for cleaner, more concise code with fewer objects added to the environment. also avoids potential for bugs in our program. Imagine if we mis-spelled ‘Taxon’ in our second line by accidentally pressing ‘s’ along with ‘x’. Compare the output:\n\nX&lt;-group_by(Fallo,Taxon,Scenario,Nutrients)\nX&lt;-group_by(X,Tasxon,Scenario)\n\nError in `group_by()`:\n! Must group by variables found in `.data`.\n✖ Column `Tasxon` is not found.\n\nX&lt;-group_by(X,Taxon)\nX&lt;-summarise(X, Mean=mean(Total), SD=sd(Total))\narrange(X,desc(Mean))\n\n# A tibble: 2 × 3\n  Taxon  Mean    SD\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 japon  56.4 10.4 \n2 bohem  56.3  9.54\n\n\n\nFallo %&gt;% \n  group_by(Taxon,Scenario,Nutrients) %&gt;%\n  group_by(Tasxon,Scenario) %&gt;%\n  group_by(Taxon) %&gt;%\n  summarise(Mean=mean(Total), SD=sd(Total)) %&gt;%\n  arrange(desc(Mean))\n\nError in `group_by()`:\n! Must group by variables found in `.data`.\n✖ Column `Tasxon` is not found.\n\n\nIn both cases we get an error, but in one case we still calculate the means and sd of the two species.\n\nA bug that produces no output is much less dangerous than an error that gives an output. Why?\n\n\n\n\n\nThe dplyr package has some handy joing_ tools to combine data sets. There are four main ways to join data sets. I find it helpful to think of Venn diagrams when I’m trying to remember what all of these functions do. You can get more information on these with the help command ?join after loading the dplyr library, but here is a quick overview. For each of these, imagine a Venn diagram with two datasets: \\(X\\) as a circle on the left side and \\(Y\\) as a circle on the right side. The rows we choose to combine from the two datasets depend on one or more identifying columns that we can define (e.g. sample ID, date, time).\n\n\n\nVenn diagram of datasets\n\n\n\nleft_join() - Keep all rows of X and add matching rows from Y. Any rows in Y that don’t match X are excluded.\nright_join() - The reverse of left_join()\ninner_join() - Only keep rows that are common to both X AND Y. Remove any rows that don’t match.\nfull_join() - Keep any columns that are in either X OR Y. Add NA for missing data in either of the columns for rows that don’t match.\n\nTo try these out, we can create a couple of quick data sets and compare the output. For each case, note the addition of NA for missing data.\n\nX&lt;-data.frame(ID=c(\"A\",\"C\"),Xdat=c(1,3))\nY&lt;-data.frame(ID=c(\"B\",\"C\"),Ydat=c(2,3))\nX\n\n  ID Xdat\n1  A    1\n2  C    3\n\nY\n\n  ID Ydat\n1  B    2\n2  C    3\n\nleft_join(X,Y,by=\"ID\")\n\n  ID Xdat Ydat\n1  A    1   NA\n2  C    3    3\n\nright_join(X,Y,by=\"ID\")\n\n  ID Xdat Ydat\n1  C    3    3\n2  B   NA    2\n\ninner_join(X,Y,by=\"ID\")\n\n  ID Xdat Ydat\n1  C    3    3\n\nfull_join(X,Y,by=\"ID\")\n\n  ID Xdat Ydat\n1  A    1   NA\n2  C    3    3\n3  B   NA    2\n\n\n\n\n\nMost of the data examples we’ve looked at are in the ‘wide’ format, where we have a single individual as a row, and multiple measurements as separate columns.\nHowever, there are many cases where we may want to reorganize our data into the ‘long’ format, where each row is an individual observation. Many statistical models use this format, and it’s also useful for visualizations.\nHere is one very handy example that I use all the time. If we have a bunch of columns of observations and we want to generate plots quickly (e.g. frequency distributions), we can convert the data to the long format and then use the facet_ functions from ggplot2 to create separate plots for each of the original columns containing the observations.\n\n\nThe pivot_longer() function in the tidyr library converts data from wide to long. We use the cols= parameter to specify the data columns, then names_to= specifies the column name containing the parameter, and the cols_to= specifies the column name of the values.\nThis is a bit confusing to read, but it’s easier to understand if you compare the output with the full_join function in the previous section.\n\nLongData&lt;-full_join(X,Y,by=\"ID\") %&gt;%\n  pivot_longer(cols=c(\"Xdat\",\"Ydat\"),\n               names_to=\"Measurement\",\n               values_to=\"Value\")\nLongData\n\n# A tibble: 6 × 3\n  ID    Measurement Value\n  &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;\n1 A     Xdat            1\n2 A     Ydat           NA\n3 C     Xdat            3\n4 C     Ydat            3\n5 B     Xdat           NA\n6 B     Ydat            2\n\n\nThis is the long format. Compare to the wide format:\n\nfull_join(X,Y,by=\"ID\")\n\n  ID Xdat Ydat\n1  A    1   NA\n2  C    3    3\n3  B   NA    2\n\n\nNote how there is only one column of data values, with the Measurement column indicating which measurement the value belongs to, and the ID column is repeated for each measurement.\nThis is why the long data format is sometimes called the repeated measures data format.\n\n\n\nThe pivot_wider() function does the reverse. This time, we specify the column that contains the values with values_from= and the corresponding column names with names_from=. This should recover the original data set:\n\nWideData&lt;-LongData %&gt;%\n  pivot_wider(values_from=Value,\n              names_from=Measurement)\nWideData\n\n# A tibble: 3 × 3\n  ID     Xdat  Ydat\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 A         1    NA\n2 C         3     3\n3 B        NA     2\n\n\nNote the slight difference in output.\n\nQuestion: Can you explain why this output is different than the original?\n\nAnswer: The original was a data.frame object, but the output of pivot_longer() and pivot_wider()\n\n\n\n\nSo far we have worked on a pristine data set that has already been edited for errors. More often datasets will contain missing values.\n\n\nAs we have already seen, the R language uses a special object NA to denote missing data.\n\nVec&lt;-c(1,2,3,NA,5,6,7)\nVec\n\n[1]  1  2  3 NA  5  6  7\n\n\nWhen a function is run on a vector or other object containing NA, the function will often return NA or give an error message:\n\nmean(Vec)\n\n[1] NA\n\n\nThis is by design, because it is not always clear what NA means. Should these data be removed from consideration, or is it a zero to be included in calculations and statistical models? Many functions in R include an na.rm parameter that is set to FALSE by default. Setting it to true tells the function to ignore the NA (i.e., remove it from the calculation).\n\nmean(Vec, na.rm=T)\n\n[1] 4\n\n\n\n\n\nA common mistake students make is to put 0 for missing data. This can be a big problem when analyzing the data since the calculations are very different.\n\nVec1&lt;-c(1,2,3,NA,5,6,7)\nmean(Vec1, na.rm=T)\n\n[1] 4\n\nVec2&lt;-c(1,2,3,0,5,6,7)\nmean(Vec2, na.rm=T)\n\n[1] 3.428571\n\n\nHowever, there are many cases where NA does represent a zero for purposes of calculation or statistical analysis. There is no simple rule to follow here. The best decision will depend on the specific details of the biological data and the assumptions of the particular calculation or statistic.\n\n\n\nIn large datasets you might want to check for missing values. Let’s simulate this in our FallopiaData.csv dataset.\nTo set up a test data frame, randomly select 10 rows and replace the value for ‘Total’ with NA.\n\nQuestion: Can you remember how to do this, from the R Fundamentals Chapter?\n\nAnswer: There are many ways that you could approach this. One way is to first randomly select from a vector representing the row number in our data frame. Then, replace the values of these particular observations with NA.\n\nX&lt;-sample(c(1:nrow(Fallo)),10,replace=F)\nFallo$Total[X]&lt;-NA\nFallo$Total\n\n  [1] 67.06 50.22 61.08 41.71 41.81 48.27 55.42    NA 53.53 45.89 59.02 57.66\n [13] 48.98 35.97 43.28 52.27    NA 44.61 59.13 58.97 55.36 31.46 43.46 44.65\n [25] 59.69 60.82 57.21 34.09 58.57    NA 63.18 59.88 54.09 55.27 61.31 53.56\n [37] 52.66 64.71 61.06 45.34 64.20 57.50 68.55 49.55    NA 54.06 66.60 74.82\n [49] 53.71 49.75 58.45 66.06 67.01 70.41    NA 63.43    NA 47.50 61.79 54.96\n [61] 48.99 52.01    NA    NA 42.47 46.18 62.56 54.36 69.54 75.91 56.34 64.97\n [73] 60.71 57.80 41.72 67.44 58.78 77.74 65.68 58.42 55.35 50.28 55.04 39.56\n [85] 71.07 45.23 57.20 67.70 52.46 60.86 62.19 65.53 48.19 60.89 48.13 60.37\n [97] 67.86 56.40 49.13 56.11 49.78 69.00 65.40    NA 63.08 60.93 29.54 49.12\n[109] 68.73 31.90 69.88 69.48 47.88    NA 58.13 50.51 54.83 66.80 50.31 56.12\n[121] 62.96 78.80 64.25\n\n\nUse is.na() to check for missing values:\n\nis.na(Fallo$Total)\n\n  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n [13] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [25] FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\n [49] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE FALSE\n [61] FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [85] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [97] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n[109] FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n[121] FALSE FALSE FALSE\n\n\nNote that the output is a vector of True/False. Each cell corresponds to a value of ‘Total’ with TRUE indicating missing values. This is an example of a boolean variable, which has some handy properties in R.\nFirst, we can use it as an index. For example, let’s see which pots have missing ‘Total’ values:\n\nMissing&lt;-is.na(Fallo$Total)\nFallo$PotNum[Missing]\n\n [1]   9  20  36  55  68  70  78  79 126 138\n\n\nAnother handy trick to check for missing values is to sum the vector:\n\nsum(is.na(Fallo$Total))\n\n[1] 10\n\n\nThis takes advantage of the fact that the Boolean TRUE/FALSE variable is equivalent to the binary 1/0 values. By default, R treats TRUE as 1 and FALSE as 0 if used in any mathematical operation.\n\n\n\n\nNaughty data contain the same information as a standard row x column (i.e. 2-dimensional) data frame but break tone or more of the following best practices.\n\nEach cell contains a single value\nEach variable must have its own column\nEach observation must have its own row\n\nNaughty data are very common in biology, this often occurs when either the collector(s) were not familiar with these best practices, or decisions are made to favour human readibility and usability rather than computer interpretability.\n\n\n\nExamples of Naughty Data from Wu et al. (2022), shown on the left side. The right side shown well-behaved data, with one observation per row, and column names as the first row.\n\n\nNaughty data can be very time consuming to fix, but regular expressions can make this a bit easier, as discussed in the Regular Expressions Chapter.\n\n\n\nAs biologists, we often work with dates or time. We may want to analyze the date a sample was collected, or a measurement was taken. But dates are often encoded in formats that don’t fit neatly into the usual data types that we’ve worked with so far. The lubridate package provides a convenient framework for switching between human-readable dates and mathematical relationships among them – for example, the number of days, minutes, or seconds between two time points.\n\nlibrary(lubridate)\n\nIt can be convenient to automatically include the current date or time, especially when you are producing reports in R.\nWe can get the date as a date object in the year-month-day format,\n\ntoday()\n\n[1] \"2024-01-29\"\n\n\nAnd we can get the date along with the time as a datetime object, in the year-month-day hour:minute:second timezone format.\n\nnow()\n\n[1] \"2024-01-29 20:45:14 EST\"\n\n\nThis is an important distinction, because the datetime object extends on the date object to include hours, minutes, seconds, and time zone.\n\n\nWe can use a datetime object to track the run time of our code. The run time is how long it takes to run a particular program. We first create an object to store the computer clock before the program runs, then we subtract that object from the computer clock after the program finishes:\n\nBefore&lt;-now()\n\nfor(i in 1:1000){\n  rpois(10,lambda=10)^rnorm(10)\n}\n\nnow()-Before\n\nTime difference of 0.0129571 secs\n\n\nNote that your run time may be different, depending on the specifications of your computer and other processes that are running on it.\nThis technique can be useful for estimating the run time of a large program. Specifically, we run a fraction of the data and/or loop iterations and multiply to get an estimate the run time for the full data set.\n\nQuestion: If you add a print() function to print out the result in each iteration of the for loop, how much does this slow down the run time?\n\nAnswer: Try it to find out!\nFor a small program like this, it’s not a problem to ad a few fractions of a second. But this can scale up exponentially in ‘big data’ applications. For example, imagine you are sequencing a human genome at 100× coverage, which might typically involve one a billion sequences, each 300 bases long. You want to write a program to assemble all of the sequences into a full genome. How much time would it add to your program run time if you add a 1 millisecond step for each sequence, or each base pair in the experiment?\nFor each sequence, it would be \\(10^9sequences \\times 0.001 s = 10^6 s\\), or about 11.5 days! For each base pair, multiply by 300 – it would take more than 10 years!\nThese are problems of scale that don’t matter so much when you are starting out, but developing an efficiency mindset early, it can pay off when we move on to bigger data projects.\nNext, we’ll explore the date and datetime objects in more detail.\n\n\n\nHuman-readable dates come in many different forms, which we can encode as strings. Here are some examples that we might see for encoding the end-date of the most recent 5,126-year-long Mesoamerican calendar used by the ancient Maya civilization:\n\nDate2&lt;-\"2012-12-21\"\nDate3&lt;-\"21.12.2012\"\nDate4&lt;-\"Dec 21, 2012\"\nDate5&lt;-\"21 December, 2012\"\n\nThe lubridate package has a number of related functions that correspond to the order of entry – d for day, m for month, and y for year:\n\nymd(Date2)\n\n[1] \"2012-12-21\"\n\ndmy(Date3)\n\n[1] \"2012-12-21\"\n\nmdy(Date4)\n\n[1] \"2012-12-21\"\n\ndmy(Date5)\n\n[1] \"2012-12-21\"\n\n\nNotice the flexibility here! Some dates have dashes, dots, spaces, and commas! Yet, all are easily converted to a common object type. On the surface, these objects look like simple strings, but compare the structure of the date object with its original input string:\n\nstr(Date2)\n\n chr \"2012-12-21\"\n\nstr(ymd(Date2))\n\n Date[1:1], format: \"2012-12-21\"\n\n\nNotice how the first is a simple chr character object, whereas the second is a Date object. The date object can be treated as a numeric variable, that outputs as a readable date. For example, what if we want to know what the date 90 days before or after?\n\nc(ymd(Date2)-90,ymd(Date2)+90)\n\n[1] \"2012-09-22\" \"2013-03-21\"\n\n\nWhen using date objects, R will even account for the different number of days in each month, and adjust for leap years!\nAs with any function in R, we can apply ymd() to a column in a data frame or any other vector of strings:\n\nDateVec&lt;-c(\"2012-12-1\",\"2012-12-20\",\n           \"2012-12-23\", \"2012-11-05\")\nymd(DateVec)\n\n[1] \"2012-12-01\" \"2012-12-20\" \"2012-12-23\" \"2012-11-05\"\n\n\nThe elements must have the have the same order as the function, but surprisingly they don’t have to have the same format:\n\nMixVec&lt;-c(\"2012-12-11\",\"12-20-2012\",\n          \"2012, December 21\", \"2013, Nov-11\")\nymd(MixVec)\n\nWarning: 1 failed to parse.\n\n\n[1] \"2012-12-11\" NA           \"2012-12-21\" \"2013-11-11\"\n\n\nNote the warning and the resulting output. The first, third, and fourth elements are converted, even though they have different formats. The second element is replaced with NA because the order is not year, month, day, as required by ymd().\nBecause these objects are numeric, we can also use them for plotting:\n\nggplot() + \n  geom_histogram(aes(ymd(DateVec)),\n                 binwidth=1)\n\n\n\n\n\n\n\n\n\nQuestion: What do you notice about the x-axis? How would it differ if we used strings instead?\n\nAnswer: Try to run the above with DateVec as a string instead of date.\nYou should get an error, because you can’t bin values of a character variable to generate a frequency histogram. This shows us that R is able to treat the date object as a continuous variable, spacing the bins based on the number of days separating them.\n\n\n\nThe datetime object adds a time element to the date. Just as there are different functions to translate different date formats, there are different datetime functions. Each datetime function starts with one of the three date functions we used earlier, but then adds time elements after an underscore _ to define nine different functions. Here are a few examples:\n\nmdy_h(\"Dec 21, 2012 -- 10am\")\n\n[1] \"2012-12-21 10:00:00 UTC\"\n\nymd_hm(\"2012-12-21, 08:30\")\n\n[1] \"2012-12-21 08:30:00 UTC\"\n\ndmy_hms(\"21 December, 2012; 11:59:59\")\n\n[1] \"2012-12-21 11:59:59 UTC\"\n\n\n\n\n\nWe can extract components from date and datetime objects. For example, we can extract only the year:\n\nyear(Date2)\n\n[1] 2012\n\n\nor the month:\n\nmonth(Date2)\n\n[1] 12\n\n\nWe have several options for days.\nFirst, we can get the day of the year, also known as the Julian day:\n\nyday(Date2)\n\n[1] 356\n\n\nOr the day of the month\n\nmday(Date2)\n\n[1] 21\n\n\nor the day of the week (rather than month) using wday()\n\nwday(Date2)\n\n[1] 6\n\n\nWe can use the label=T parameter rather than number, to get the name of the specific month or day, rather than the number:\n\nmonth(Date2, label=T)\n\n[1] Dec\n12 Levels: Jan &lt; Feb &lt; Mar &lt; Apr &lt; May &lt; Jun &lt; Jul &lt; Aug &lt; Sep &lt; ... &lt; Dec\n\nwday(Date2, label=T)\n\n[1] Fri\nLevels: Sun &lt; Mon &lt; Tue &lt; Wed &lt; Thu &lt; Fri &lt; Sat\n\n\n\n\n\nWe’ve seen above how dates are more like numeric objects rather than strings, but we can also treat dates as categorical data.\nOne example that is becoming more and more common in biology is the analysis of data from data loggers, which automatically save observations over time. Think of climate stations that measure temperature and precipitation as a common example. Another example might be location information of a study organism using image analysis or PIT tags (i.e. Passive Integrated Transponders).\nIn many cases, the timescale of collection is much shorter than what we need for our analysis. We end up with too much data! Luckily, dplyr with lubridate offer a great combination for summarizing these data.\n\n\nHere’s an example that takes advantage that takes advantage of our ability to treat components of datetime objects as categorical variables as well as continuous variables.\nImagine we have observations taken every minute, and we just want to calculate the average for each hour . To see how to do this, we will generate a toy data set in R using the tibble function, and then assigning column names: DayTime for the day-time object and Obs for the observation (i.e. measurement):\nFirst, we create the imaginary dataset, using the replicate function and a random number generator:\n\nTestData&lt;-tibble(\n  DayTime=now()+minutes(rep(c(0:359),100)),\n  Obs=rnorm(36000))\n\nWe can calculate the hourly average by piping our TestData tibble (above) through a group_by and then a summarise() function.\n\nTestData %&gt;%\n  group_by(yday(DayTime),hour(DayTime)) %&gt;%\n  summarise(Mean=mean(Obs))\n\n`summarise()` has grouped output by 'yday(DayTime)'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 7 × 3\n# Groups:   yday(DayTime) [2]\n  `yday(DayTime)` `hour(DayTime)`      Mean\n            &lt;dbl&gt;           &lt;int&gt;     &lt;dbl&gt;\n1              29              20  0.0161  \n2              29              21 -0.0170  \n3              29              22  0.0111  \n4              29              23 -0.0108  \n5              30               0 -0.0159  \n6              30               1 -0.0283  \n7              30               2 -0.000328\n\n\n\nQuestion Why do we include yday() in the group_by function?\n\nAnswer: Remove yday(DayTime) and compare the output."
  },
  {
    "objectID": "datascience.html#introduction",
    "href": "datascience.html#introduction",
    "title": "Data Science",
    "section": "",
    "text": "Data Science is a relatively new field of study that merges computer science and statistics to answer questions in other domains (e.g. business, medicine, biology, psychology). Data Science as a discipline has grown in popularity in response to the rapid rate of increase in data collection and publication.\nData Science often involves ‘Big Data’, which doesn’t have a strict quantitative definition but will usually have one or more of the following characteristics:\n\nHigh Volume – large file sizes with may observations.\nWide Variety – many different types of data.\nHigh Velocity – data accumulates at a high rate.\nCompromised Veracity – data quality issues must be addressed otherwise downstream analyses will be compromised.\n\n\nQuestion: What are some examples of ‘big data’ in Biology?\n\nAnswer: There are many types of biological data that could be listed, but medical records, remote sensing data, and ‘omics data are common examples of ’big data’ in biology.\nIn biology, it can be helpful to think of Data Science as a continuous life-cycle with multiple stages:\n\n\n\nHypothesize – Make initial observations about the natural world, or insights from other data, that lead to testable hypotheses. Your core biology training is crucial here.\nCollect – This may involve taking measurements yourself, manually entering data that is not yet in electronic format, requesting data from authors of published studies, or importing data from online sources. Collecting data is a crucial step that is often done poorly.\nCorrect – Investigate the data for quality assurance, to identify and fix potential errors. Start to visualize the data to look for outliers, odd frequency distributions, or nonsensical relationships among variables.\nExplore – Try to understand the data, where they come from, and potential limitations on their use. Continue visualizing data; this may cause you to modify your hypotheses slightly.\nModel – Now that hypotheses are clearly defined, apply statistical tests of their validity.\nReport – Use visualizations along with the results of your statistical tests to summarise your findings.\nRepeat – Return to step 1.\n\nIn this chapter, we focus mainly on coding in R for steps 2, 3, and 6. Step 5 requires a firm understanding of statistics, which is the focus of the book R STATS Crash Course for Biologists. Visualizations in Steps 3 & 4 were covered in earlier chapters.. Step 1 requires a good understanding of the study system, as covered in a typical university degree in the biological sciences.\nData collection and management are crucial steps in the Data Science Life-Cycle. Read the baRcodeR paper by Wu et al (2022) (https://doi.org/10.1111/2041-210X.13405). called baRcodeR with PyTrackDat: Open-source labelling and tracking of biological samples for repeatable science. Pay particular attention to the ‘Data Standards’ section. The baRcodeR and PyTrackDat programs and their application to current projects may also be of interest."
  },
  {
    "objectID": "datascience.html#setup",
    "href": "datascience.html#setup",
    "title": "Data Science",
    "section": "",
    "text": "The tidyverse library is a set of packages created by the same developers responsible for R Studio. There are a number of packages and functions that improve on base R. The name is based on the idea of living in a data universe that is neat and tidy.\nThe book R for Data Science by Hadley Wickham & Garrett Grolemund is an excellent resource for learning about the tidyverse. In this chapter we’ll touch on a few of the main packages and functions.\n\nProtip In general, any book by Hadley Wickham that you come across is worth reading if you want to be proficient in R.\n\nWhile we wait for the tidyverse to install, let’s consider a few general principles of data science."
  },
  {
    "objectID": "datascience.html#d-data-wrangling",
    "href": "datascience.html#d-data-wrangling",
    "title": "Data Science",
    "section": "",
    "text": "The dplyr library in R has many useful features for importing and reorganizing your data for steps 2, 3 and 4 in the Data Science Life-Cycle.\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\nNote: This error message informs us that the dplyr package uses function or parameter names that are the same as other base or stats packages in R. These base/stats functions are ‘masked’ meaning that when you run one (e.g. filter) then R will run the dplyr version rather than the stats version.\n\n\nlibrary(tidyr)\n\nWe’ll work with our FallopiaData.csv data set, and remind ourselves of the structure of the data\n\n\nWe looked at data.frame objects in the first chapter as an expansion of matrices with a few additional features like column and row names. A tibble is the tidyverse version of the data.frame object and includes a few more useful features. To import a dataset to a tibble instead of a data.frame object, we use read_csv instead of read.csv.\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.3.2\n\n\nWarning: package 'readr' was built under R version 4.3.2\n\n\nWarning: package 'forcats' was built under R version 4.3.2\n\n\nWarning: package 'lubridate' was built under R version 4.3.2\n\n\nYou may also see a message about conflicts here, with the filter() and lag() functions from dplyr masking the functions from the stats package.\n\nFallo&lt;-read_csv(\n  \"https://colauttilab.github.io/RCrashCourse/FallopiaData.csv\")\nstr(Fallo)\n\n\n\nRows: 123 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): Scenario, Nutrients, Taxon\ndbl (10): PotNum, Symphytum, Silene, Urtica, Geranium, Geum, All_Natives, Fa...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nspc_tbl_ [123 × 13] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ PotNum      : num [1:123] 1 2 3 5 6 7 8 9 10 11 ...\n $ Scenario    : chr [1:123] \"low\" \"low\" \"low\" \"low\" ...\n $ Nutrients   : chr [1:123] \"low\" \"low\" \"low\" \"low\" ...\n $ Taxon       : chr [1:123] \"japon\" \"japon\" \"japon\" \"japon\" ...\n $ Symphytum   : num [1:123] 9.81 8.64 2.65 1.44 9.15 ...\n $ Silene      : num [1:123] 36.4 29.6 36 21.4 23.9 ...\n $ Urtica      : num [1:123] 16.08 5.59 17.09 12.39 5.19 ...\n $ Geranium    : num [1:123] 4.68 5.75 5.13 5.37 0 9.05 3.51 9.64 7.3 6.36 ...\n $ Geum        : num [1:123] 0.12 0.55 0.09 0.31 0.17 0.97 0.4 0.01 0.47 0.33 ...\n $ All_Natives : num [1:123] 67 50.2 61 40.9 38.4 ...\n $ Fallopia    : num [1:123] 0.01 0.04 0.09 0.77 3.4 0.54 2.05 0.26 0 0 ...\n $ Total       : num [1:123] 67.1 50.2 61.1 41.7 41.8 ...\n $ Pct_Fallopia: num [1:123] 0.01 0.08 0.15 1.85 8.13 1.12 3.7 0.61 0 0 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   PotNum = col_double(),\n  ..   Scenario = col_character(),\n  ..   Nutrients = col_character(),\n  ..   Taxon = col_character(),\n  ..   Symphytum = col_double(),\n  ..   Silene = col_double(),\n  ..   Urtica = col_double(),\n  ..   Geranium = col_double(),\n  ..   Geum = col_double(),\n  ..   All_Natives = col_double(),\n  ..   Fallopia = col_double(),\n  ..   Total = col_double(),\n  ..   Pct_Fallopia = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nThis file is an example of a 2-dimensional data set, which is common in biology. 2D datasets have the familiar row x column layout used by spreadsheet programs like Microsoft Excel or Google Sheets. There are some exceptions, but data in this format should typically follows 3 rules:\n\nEach cell contains a single value\nEach variable must have its own column\nEach observation must have its own row\n\nMaking sure your data are arranged this way will usually make it much easier to work with.\n\n\n\nThe filter() function will subset observations based on values of interest within particular columns of our dataset. For example, we may want to filter the rows (i.e. pots) that had at least 70 \\(g\\) of total biomass.\n\nPot1&lt;-filter(Fallo,Total &gt;= 70)\nhead(Pot1)\n\n# A tibble: 6 × 13\n  PotNum Scenario Nutrients Taxon Symphytum Silene Urtica Geranium  Geum\n   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1     60 high     high      bohem      7.77   51.4   5.13    10.1   0.37\n2     67 gradual  high      japon      2.92   25.2  19.1     23.1   0.06\n3     70 gradual  high      japon     10.1    47.0  18.6      0.64  0.82\n4     86 gradual  high      bohem      2.93   60.9   4.11     6.67  1.27\n5     95 extreme  high      japon      4.92   25.9  40.3      4.92  0.07\n6    103 extreme  high      japon      6.92   49.4   0       10.3   0.42\n# ℹ 4 more variables: All_Natives &lt;dbl&gt;, Fallopia &lt;dbl&gt;, Total &lt;dbl&gt;,\n#   Pct_Fallopia &lt;dbl&gt;\n\n\n\n\n\nThere are different options to change the names of columns in your data. In base R you can use the names() function with the square bracket index []:\n\nX&lt;-Fallo\nnames(X)\n\n [1] \"PotNum\"       \"Scenario\"     \"Nutrients\"    \"Taxon\"        \"Symphytum\"   \n [6] \"Silene\"       \"Urtica\"       \"Geranium\"     \"Geum\"         \"All_Natives\" \n[11] \"Fallopia\"     \"Total\"        \"Pct_Fallopia\"\n\nnames(X)[12]&lt;-\"Total_Biomass\"\nnames(X)\n\n [1] \"PotNum\"        \"Scenario\"      \"Nutrients\"     \"Taxon\"        \n [5] \"Symphytum\"     \"Silene\"        \"Urtica\"        \"Geranium\"     \n [9] \"Geum\"          \"All_Natives\"   \"Fallopia\"      \"Total_Biomass\"\n[13] \"Pct_Fallopia\" \n\n\nThere is also a simple dplyr function to do this:\n\nX&lt;-rename(Fallo, Total_Biomass = Total)\nnames(X)\n\n [1] \"PotNum\"        \"Scenario\"      \"Nutrients\"     \"Taxon\"        \n [5] \"Symphytum\"     \"Silene\"        \"Urtica\"        \"Geranium\"     \n [9] \"Geum\"          \"All_Natives\"   \"Fallopia\"      \"Total_Biomass\"\n[13] \"Pct_Fallopia\" \n\n\n\n\n\nUse the arrange() function to sort the rows of your data based on the values in one or more columns. For example, let’s re-arrange our FallopiaData.csv dataset based on Taxon (a string denoting the species of Fallopia used) and Total (a float denoting the total biomass in each pot).\n\nX&lt;-arrange(Fallo, Taxon, Total)\nhead(X)\n\n# A tibble: 6 × 13\n  PotNum Scenario Nutrients Taxon Symphytum Silene Urtica Geranium  Geum\n   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1     26 low      low       bohem     13.2    18.1   0        0     0.1 \n2     17 low      low       bohem      4.9    29.5   1.36     0     0.19\n3     80 gradual  high      bohem     11.9    17.2   8.92     0.94  0.18\n4     18 low      low       bohem      3.51   27.6   8.14     3.81  0.21\n5     28 low      low       bohem     10.6    18.8   7.19     6.73  0.17\n6     22 low      low       bohem      0.76   22.7   9.85    10.6   0.74\n# ℹ 4 more variables: All_Natives &lt;dbl&gt;, Fallopia &lt;dbl&gt;, Total &lt;dbl&gt;,\n#   Pct_Fallopia &lt;dbl&gt;\n\n\nuse the desc() function with arrange() to reverse and sort in descending order. We can sort by multiple columns in the by_group= parameter.\n\nX&lt;-arrange(Fallo, by_group=Taxon, desc(Silene))\nhead(X)\n\n# A tibble: 6 × 13\n  PotNum Scenario Nutrients Taxon Symphytum Silene Urtica Geranium  Geum\n   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1     86 gradual  high      bohem      2.93   60.9   4.11     6.67  1.27\n2     53 high     high      bohem      7.05   56.3   1.14     4.07  0   \n3     60 high     high      bohem      7.77   51.4   5.13    10.1   0.37\n4     50 high     high      bohem      8.52   44.6   1.27     9.45  0.35\n5     79 gradual  high      bohem     11.0    44.6   1.56     0.03  0.05\n6    107 extreme  high      bohem      0      43.8   8.1      8.18  0.36\n# ℹ 4 more variables: All_Natives &lt;dbl&gt;, Fallopia &lt;dbl&gt;, Total &lt;dbl&gt;,\n#   Pct_Fallopia &lt;dbl&gt;\n\n\n\n\n\nThe select() function can be used to select a subset of columns (i.e. variables) from your data.\nSuppose we only want to look at total biomass, but keep all the treatment columns:\n\nX&lt;-select(Fallo, PotNum, Scenario, Nutrients, Taxon, Total)\nhead(X)\n\n# A tibble: 6 × 5\n  PotNum Scenario Nutrients Taxon Total\n   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt; &lt;dbl&gt;\n1      1 low      low       japon  67.1\n2      2 low      low       japon  50.2\n3      3 low      low       japon  61.1\n4      5 low      low       japon  41.7\n5      6 low      low       japon  41.8\n6      7 low      low       japon  48.3\n\n\nYou can also use the colon : to select a range of columns:\n\nX&lt;-select(Fallo, PotNum:Taxon, Total)\nhead(X)\n\n# A tibble: 6 × 5\n  PotNum Scenario Nutrients Taxon Total\n   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt; &lt;dbl&gt;\n1      1 low      low       japon  67.1\n2      2 low      low       japon  50.2\n3      3 low      low       japon  61.1\n4      5 low      low       japon  41.7\n5      6 low      low       japon  41.8\n6      7 low      low       japon  48.3\n\n\nExclude columns with -\n\nX&lt;-select(Fallo, -PotNum:Taxon, -Total)\n\nWarning in x:y: numerical expression has 12 elements: only the first used\n\n\n\nOops, what generated that warning? Take a careful look at the error message and see if you can figure it out.\n\nThe problem is we are using the range of columns between PotNum and Taxon, but in one case we are excluding and the other we are including. We need to be consistent:\n\nX&lt;-select(Fallo, -PotNum:-Taxon, Total)\nhead(X)\n\n# A tibble: 6 × 9\n  Symphytum Silene Urtica Geranium  Geum All_Natives Fallopia Total Pct_Fallopia\n      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;\n1      9.81   36.4  16.1      4.68  0.12        67.0     0.01  67.1         0.01\n2      8.64   29.6   5.59     5.75  0.55        50.2     0.04  50.2         0.08\n3      2.65   36.0  17.1      5.13  0.09        61.0     0.09  61.1         0.15\n4      1.44   21.4  12.4      5.37  0.31        40.9     0.77  41.7         1.85\n5      9.15   23.9   5.19     0     0.17        38.4     3.4   41.8         8.13\n6      6.31   24.4   7        9.05  0.97        47.7     0.54  48.3         1.12\n\n\nOr a bit more clear:\n\nX&lt;-select(Fallo, -(PotNum:Taxon), Scenario)\nhead(X)\n\n# A tibble: 6 × 10\n  Symphytum Silene Urtica Geranium  Geum All_Natives Fallopia Total Pct_Fallopia\n      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;       &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;        &lt;dbl&gt;\n1      9.81   36.4  16.1      4.68  0.12        67.0     0.01  67.1         0.01\n2      8.64   29.6   5.59     5.75  0.55        50.2     0.04  50.2         0.08\n3      2.65   36.0  17.1      5.13  0.09        61.0     0.09  61.1         0.15\n4      1.44   21.4  12.4      5.37  0.31        40.9     0.77  41.7         1.85\n5      9.15   23.9   5.19     0     0.17        38.4     3.4   41.8         8.13\n6      6.31   24.4   7        9.05  0.97        47.7     0.54  48.3         1.12\n# ℹ 1 more variable: Scenario &lt;chr&gt;\n\n\n\n\n\nUse the everything() function with select() to rearrange your columns without losing any:\n\nX&lt;-select(Fallo, Taxon, Scenario, Nutrients, PotNum, \n          Pct_Fallopia, everything())\nhead(X)\n\n# A tibble: 6 × 13\n  Taxon Scenario Nutrients PotNum Pct_Fallopia Symphytum Silene Urtica Geranium\n  &lt;chr&gt; &lt;chr&gt;    &lt;chr&gt;      &lt;dbl&gt;        &lt;dbl&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;\n1 japon low      low            1         0.01      9.81   36.4  16.1      4.68\n2 japon low      low            2         0.08      8.64   29.6   5.59     5.75\n3 japon low      low            3         0.15      2.65   36.0  17.1      5.13\n4 japon low      low            5         1.85      1.44   21.4  12.4      5.37\n5 japon low      low            6         8.13      9.15   23.9   5.19     0   \n6 japon low      low            7         1.12      6.31   24.4   7        9.05\n# ℹ 4 more variables: Geum &lt;dbl&gt;, All_Natives &lt;dbl&gt;, Fallopia &lt;dbl&gt;,\n#   Total &lt;dbl&gt;\n\n\n\n\n\nSuppose we want to make a new column to our data frame or tibble. For example, we calculate the sum of biomass of Urtica and Geranium only. In base R, we could use $ to select the column from the data frame.\n\nX&lt;-Fallo\nX$UrtSil&lt;-X$Urtica+X$Silene\n\nIn the dplyr package we can use the mutate() function.\n\nX&lt;-mutate(Fallo, UrtSil = Urtica + Silene)\nhead(X)\n\n# A tibble: 6 × 14\n  PotNum Scenario Nutrients Taxon Symphytum Silene Urtica Geranium  Geum\n   &lt;dbl&gt; &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1      1 low      low       japon      9.81   36.4  16.1      4.68  0.12\n2      2 low      low       japon      8.64   29.6   5.59     5.75  0.55\n3      3 low      low       japon      2.65   36.0  17.1      5.13  0.09\n4      5 low      low       japon      1.44   21.4  12.4      5.37  0.31\n5      6 low      low       japon      9.15   23.9   5.19     0     0.17\n6      7 low      low       japon      6.31   24.4   7        9.05  0.97\n# ℹ 5 more variables: All_Natives &lt;dbl&gt;, Fallopia &lt;dbl&gt;, Total &lt;dbl&gt;,\n#   Pct_Fallopia &lt;dbl&gt;, UrtSil &lt;dbl&gt;\n\n\nThis is a lot more readable, especially when you have complicated equations or you want to add many of new columns.\n\nQuestion: What if you only wanted to retain the new columns and delete everything else? Try it.\n\nWhich functions did you use?\n\n\n\nThe transmute() functions acts as a combination of mutate() + select()\n\nX&lt;-transmute(Fallo, UrtSil = Urtica + Silene)\nhead(X)\n\n# A tibble: 6 × 1\n  UrtSil\n   &lt;dbl&gt;\n1   52.4\n2   35.2\n3   53.1\n4   33.8\n5   29.1\n6   31.4\n\n\n\n\n\nThis can be useful for quickly summarizing your data, for example to find the mean or standard deviation based on a particular treatment or group.\n\nTrtGrp&lt;-group_by(Fallo,Taxon,Scenario,Nutrients)\nsummarise(TrtGrp, Mean=mean(Total), SD=sd(Total))\n\n`summarise()` has grouped output by 'Taxon', 'Scenario'. You can override using\nthe `.groups` argument.\n\n\n# A tibble: 10 × 5\n# Groups:   Taxon, Scenario [10]\n   Taxon Scenario     Nutrients  Mean    SD\n   &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 bohem extreme      high       58.3  7.34\n 2 bohem fluctuations high       58.4  9.20\n 3 bohem gradual      high       57.5  9.34\n 4 bohem high         high       60.3  8.68\n 5 bohem low          low        48.0  8.86\n 6 japon extreme      high       57.2 10.9 \n 7 japon fluctuations high       56.4 13.7 \n 8 japon gradual      high       59.7  9.57\n 9 japon high         high       56.4  8.20\n10 japon low          low        52.0  8.29\n\n\n\n\n\nIn our dataset, the Taxon column shows which of two species of Fallopia were used in the competition experiments. We might want to take the mean total biomass for each of the two Fallopia species.\nHowever, there are other factors in our experiment that may affect biomass. The Nutrients column tells us whether pots received high or low nutrients, and this also affects biomass:\n\nX&lt;-group_by(Fallo,Nutrients)\nsummarise(X, Mean=mean(Total), SD=sd(Total))\n\n# A tibble: 2 × 3\n  Nutrients  Mean    SD\n  &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1 high       58.0  9.61\n2 low        49.9  8.66\n\n\nWe can see that the Nutrients treatment had a large effect on the mean biomass of a pot. Now imagine if our sampling design is ‘unbalanced’. For example, maybe we had some plant mortality or lost some plants to a tornado. If one of the two species in the Taxon column had more high-nutrient pots left over, then it would have a higher mean. BUT, what if the nutrients promoted growth? We would expect to see a difference in the mean of each Taxon group, this expected difference is just an artifact of unbalanced replication of the high nutrient treatment. We can simulate this effect by re-shuffling the species names:\n\nRFallo&lt;-Fallo\nset.seed(256)\nRFallo$Taxon&lt;-rbinom(nrow(RFallo),size=1,prob=0.7)\n\nX&lt;-group_by(RFallo,Taxon)\nsummarise(X, Mean=mean(Total))\n\n# A tibble: 2 × 2\n  Taxon  Mean\n  &lt;int&gt; &lt;dbl&gt;\n1     0  56.1\n2     1  56.5\n\n\nIn this example, the difference is less than $ 1%$ of the mean (i.e., \\(0.4 / 56.1 = 0.0071 = 0.71%\\)), but this could be due to the imbalance offsetting biological differences between the taxa. In other scenarios it could be much larger. To fix this problem, we may want to take a weighted mean. In this case, we want to weight the mean of each taxon by the unbalanced Scenario and Treatment categories.\nStep 1: Group by all three columns and calculate group means\n\nX1&lt;-group_by(RFallo,Taxon,Scenario,Nutrients)\n(Y1&lt;-summarise(X1,Mean=mean(Total)))\n\n# A tibble: 10 × 4\n# Groups:   Taxon, Scenario [10]\n   Taxon Scenario     Nutrients  Mean\n   &lt;int&gt; &lt;chr&gt;        &lt;chr&gt;     &lt;dbl&gt;\n 1     0 extreme      high       54.7\n 2     0 fluctuations high       58.4\n 3     0 gradual      high       56.2\n 4     0 high         high       59.8\n 5     0 low          low        50.2\n 6     1 extreme      high       59.1\n 7     1 fluctuations high       56.8\n 8     1 gradual      high       59.9\n 9     1 high         high       57.6\n10     1 low          low        49.8\n\n\nNow we have a single value for each of the subgroups. Instead of over-representing pots from Taxon=1, we have equal weightings of all three groups.\nStep 2: Take the output from Step 1 and calculate the mean of means for the column of interest.\n\nX2&lt;-group_by(Y1,Taxon)\nY2&lt;-summarise(X2, Mean=mean(Mean))\narrange(Y2,desc(Mean))\n\n# A tibble: 2 × 2\n  Taxon  Mean\n  &lt;int&gt; &lt;dbl&gt;\n1     1  56.6\n2     0  55.8\n\n\nNow there is more than a \\(2.1 %\\) difference (i.e., \\(1.2 / 55.8 = 0.0215\\)) between the taxa. This is triple the effect that we calculated with unbalanced samples!\n\n\n\nThe dplyr package includes a special command (%&gt;%) called a pipe. The pipe is a very handy tool for complex data wrangling. It allows us to string together multiple functions and then pipe our data from one function to the next. In principle, this is similar to the multi-function plotting commands we made with ggplot() in earlier chapters.\nThe pipe is useful to combine operations without creating a whole bunch of new objects. This can save on memory use and run speed because you are not making new objects for every single function.\n\nPro-tip: Use Ctl + Shift + m to add a pipe quickly\n\nFor example, we can re-write the weighted mean example using pipes.\n\nRFallo %&gt;% \n  group_by(Taxon,Scenario,Nutrients) %&gt;% \n  summarise(Mean=mean(Total)) %&gt;% \n  group_by(Taxon,Scenario) %&gt;% \n  summarise(Mean=mean(Mean)) %&gt;% \n  group_by(Taxon) %&gt;% \n  summarise(Mean=mean(Mean)) %&gt;% \n  arrange(desc(Mean))\n\n# A tibble: 2 × 2\n  Taxon  Mean\n  &lt;int&gt; &lt;dbl&gt;\n1     1  56.6\n2     0  55.8\n\n\nWe declare the input data set in the first line, and then pipe to group_by() in line 2. The output of group_by() is then *piped to summarise(), and so on. There are two important things to note here:\n\nWe do not declare the data object inside of each function. The pipe command does this for us.\nWe still have to assign the output to an object if we want to retain it. For example, we might change line 1 to wtMean&lt;-RFallo %&gt;%. We didn’t redirect the command to an object, so it simply output to your R Console.\n\nCompared to the earlier example, it is clear that the pipe can make for cleaner, more concise code with fewer objects added to the environment. also avoids potential for bugs in our program. Imagine if we mis-spelled ‘Taxon’ in our second line by accidentally pressing ‘s’ along with ‘x’. Compare the output:\n\nX&lt;-group_by(Fallo,Taxon,Scenario,Nutrients)\nX&lt;-group_by(X,Tasxon,Scenario)\n\nError in `group_by()`:\n! Must group by variables found in `.data`.\n✖ Column `Tasxon` is not found.\n\nX&lt;-group_by(X,Taxon)\nX&lt;-summarise(X, Mean=mean(Total), SD=sd(Total))\narrange(X,desc(Mean))\n\n# A tibble: 2 × 3\n  Taxon  Mean    SD\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 japon  56.4 10.4 \n2 bohem  56.3  9.54\n\n\n\nFallo %&gt;% \n  group_by(Taxon,Scenario,Nutrients) %&gt;%\n  group_by(Tasxon,Scenario) %&gt;%\n  group_by(Taxon) %&gt;%\n  summarise(Mean=mean(Total), SD=sd(Total)) %&gt;%\n  arrange(desc(Mean))\n\nError in `group_by()`:\n! Must group by variables found in `.data`.\n✖ Column `Tasxon` is not found.\n\n\nIn both cases we get an error, but in one case we still calculate the means and sd of the two species.\n\nA bug that produces no output is much less dangerous than an error that gives an output. Why?"
  },
  {
    "objectID": "datascience.html#join-datasets",
    "href": "datascience.html#join-datasets",
    "title": "Data Science",
    "section": "",
    "text": "The dplyr package has some handy joing_ tools to combine data sets. There are four main ways to join data sets. I find it helpful to think of Venn diagrams when I’m trying to remember what all of these functions do. You can get more information on these with the help command ?join after loading the dplyr library, but here is a quick overview. For each of these, imagine a Venn diagram with two datasets: \\(X\\) as a circle on the left side and \\(Y\\) as a circle on the right side. The rows we choose to combine from the two datasets depend on one or more identifying columns that we can define (e.g. sample ID, date, time).\n\n\n\nVenn diagram of datasets\n\n\n\nleft_join() - Keep all rows of X and add matching rows from Y. Any rows in Y that don’t match X are excluded.\nright_join() - The reverse of left_join()\ninner_join() - Only keep rows that are common to both X AND Y. Remove any rows that don’t match.\nfull_join() - Keep any columns that are in either X OR Y. Add NA for missing data in either of the columns for rows that don’t match.\n\nTo try these out, we can create a couple of quick data sets and compare the output. For each case, note the addition of NA for missing data.\n\nX&lt;-data.frame(ID=c(\"A\",\"C\"),Xdat=c(1,3))\nY&lt;-data.frame(ID=c(\"B\",\"C\"),Ydat=c(2,3))\nX\n\n  ID Xdat\n1  A    1\n2  C    3\n\nY\n\n  ID Ydat\n1  B    2\n2  C    3\n\nleft_join(X,Y,by=\"ID\")\n\n  ID Xdat Ydat\n1  A    1   NA\n2  C    3    3\n\nright_join(X,Y,by=\"ID\")\n\n  ID Xdat Ydat\n1  C    3    3\n2  B   NA    2\n\ninner_join(X,Y,by=\"ID\")\n\n  ID Xdat Ydat\n1  C    3    3\n\nfull_join(X,Y,by=\"ID\")\n\n  ID Xdat Ydat\n1  A    1   NA\n2  C    3    3\n3  B   NA    2"
  },
  {
    "objectID": "datascience.html#wide-vs-long-data",
    "href": "datascience.html#wide-vs-long-data",
    "title": "Data Science",
    "section": "",
    "text": "Most of the data examples we’ve looked at are in the ‘wide’ format, where we have a single individual as a row, and multiple measurements as separate columns.\nHowever, there are many cases where we may want to reorganize our data into the ‘long’ format, where each row is an individual observation. Many statistical models use this format, and it’s also useful for visualizations.\nHere is one very handy example that I use all the time. If we have a bunch of columns of observations and we want to generate plots quickly (e.g. frequency distributions), we can convert the data to the long format and then use the facet_ functions from ggplot2 to create separate plots for each of the original columns containing the observations.\n\n\nThe pivot_longer() function in the tidyr library converts data from wide to long. We use the cols= parameter to specify the data columns, then names_to= specifies the column name containing the parameter, and the cols_to= specifies the column name of the values.\nThis is a bit confusing to read, but it’s easier to understand if you compare the output with the full_join function in the previous section.\n\nLongData&lt;-full_join(X,Y,by=\"ID\") %&gt;%\n  pivot_longer(cols=c(\"Xdat\",\"Ydat\"),\n               names_to=\"Measurement\",\n               values_to=\"Value\")\nLongData\n\n# A tibble: 6 × 3\n  ID    Measurement Value\n  &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;\n1 A     Xdat            1\n2 A     Ydat           NA\n3 C     Xdat            3\n4 C     Ydat            3\n5 B     Xdat           NA\n6 B     Ydat            2\n\n\nThis is the long format. Compare to the wide format:\n\nfull_join(X,Y,by=\"ID\")\n\n  ID Xdat Ydat\n1  A    1   NA\n2  C    3    3\n3  B   NA    2\n\n\nNote how there is only one column of data values, with the Measurement column indicating which measurement the value belongs to, and the ID column is repeated for each measurement.\nThis is why the long data format is sometimes called the repeated measures data format.\n\n\n\nThe pivot_wider() function does the reverse. This time, we specify the column that contains the values with values_from= and the corresponding column names with names_from=. This should recover the original data set:\n\nWideData&lt;-LongData %&gt;%\n  pivot_wider(values_from=Value,\n              names_from=Measurement)\nWideData\n\n# A tibble: 3 × 3\n  ID     Xdat  Ydat\n  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 A         1    NA\n2 C         3     3\n3 B        NA     2\n\n\nNote the slight difference in output.\n\nQuestion: Can you explain why this output is different than the original?\n\nAnswer: The original was a data.frame object, but the output of pivot_longer() and pivot_wider()"
  },
  {
    "objectID": "datascience.html#missing-data",
    "href": "datascience.html#missing-data",
    "title": "Data Science",
    "section": "",
    "text": "So far we have worked on a pristine data set that has already been edited for errors. More often datasets will contain missing values.\n\n\nAs we have already seen, the R language uses a special object NA to denote missing data.\n\nVec&lt;-c(1,2,3,NA,5,6,7)\nVec\n\n[1]  1  2  3 NA  5  6  7\n\n\nWhen a function is run on a vector or other object containing NA, the function will often return NA or give an error message:\n\nmean(Vec)\n\n[1] NA\n\n\nThis is by design, because it is not always clear what NA means. Should these data be removed from consideration, or is it a zero to be included in calculations and statistical models? Many functions in R include an na.rm parameter that is set to FALSE by default. Setting it to true tells the function to ignore the NA (i.e., remove it from the calculation).\n\nmean(Vec, na.rm=T)\n\n[1] 4\n\n\n\n\n\nA common mistake students make is to put 0 for missing data. This can be a big problem when analyzing the data since the calculations are very different.\n\nVec1&lt;-c(1,2,3,NA,5,6,7)\nmean(Vec1, na.rm=T)\n\n[1] 4\n\nVec2&lt;-c(1,2,3,0,5,6,7)\nmean(Vec2, na.rm=T)\n\n[1] 3.428571\n\n\nHowever, there are many cases where NA does represent a zero for purposes of calculation or statistical analysis. There is no simple rule to follow here. The best decision will depend on the specific details of the biological data and the assumptions of the particular calculation or statistic.\n\n\n\nIn large datasets you might want to check for missing values. Let’s simulate this in our FallopiaData.csv dataset.\nTo set up a test data frame, randomly select 10 rows and replace the value for ‘Total’ with NA.\n\nQuestion: Can you remember how to do this, from the R Fundamentals Chapter?\n\nAnswer: There are many ways that you could approach this. One way is to first randomly select from a vector representing the row number in our data frame. Then, replace the values of these particular observations with NA.\n\nX&lt;-sample(c(1:nrow(Fallo)),10,replace=F)\nFallo$Total[X]&lt;-NA\nFallo$Total\n\n  [1] 67.06 50.22 61.08 41.71 41.81 48.27 55.42    NA 53.53 45.89 59.02 57.66\n [13] 48.98 35.97 43.28 52.27    NA 44.61 59.13 58.97 55.36 31.46 43.46 44.65\n [25] 59.69 60.82 57.21 34.09 58.57    NA 63.18 59.88 54.09 55.27 61.31 53.56\n [37] 52.66 64.71 61.06 45.34 64.20 57.50 68.55 49.55    NA 54.06 66.60 74.82\n [49] 53.71 49.75 58.45 66.06 67.01 70.41    NA 63.43    NA 47.50 61.79 54.96\n [61] 48.99 52.01    NA    NA 42.47 46.18 62.56 54.36 69.54 75.91 56.34 64.97\n [73] 60.71 57.80 41.72 67.44 58.78 77.74 65.68 58.42 55.35 50.28 55.04 39.56\n [85] 71.07 45.23 57.20 67.70 52.46 60.86 62.19 65.53 48.19 60.89 48.13 60.37\n [97] 67.86 56.40 49.13 56.11 49.78 69.00 65.40    NA 63.08 60.93 29.54 49.12\n[109] 68.73 31.90 69.88 69.48 47.88    NA 58.13 50.51 54.83 66.80 50.31 56.12\n[121] 62.96 78.80 64.25\n\n\nUse is.na() to check for missing values:\n\nis.na(Fallo$Total)\n\n  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n [13] FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [25] FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n [37] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE\n [49] FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE  TRUE FALSE FALSE FALSE\n [61] FALSE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [73] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [85] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE\n [97] FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE\n[109] FALSE FALSE FALSE FALSE FALSE  TRUE FALSE FALSE FALSE FALSE FALSE FALSE\n[121] FALSE FALSE FALSE\n\n\nNote that the output is a vector of True/False. Each cell corresponds to a value of ‘Total’ with TRUE indicating missing values. This is an example of a boolean variable, which has some handy properties in R.\nFirst, we can use it as an index. For example, let’s see which pots have missing ‘Total’ values:\n\nMissing&lt;-is.na(Fallo$Total)\nFallo$PotNum[Missing]\n\n [1]   9  20  36  55  68  70  78  79 126 138\n\n\nAnother handy trick to check for missing values is to sum the vector:\n\nsum(is.na(Fallo$Total))\n\n[1] 10\n\n\nThis takes advantage of the fact that the Boolean TRUE/FALSE variable is equivalent to the binary 1/0 values. By default, R treats TRUE as 1 and FALSE as 0 if used in any mathematical operation."
  },
  {
    "objectID": "datascience.html#naughty-data",
    "href": "datascience.html#naughty-data",
    "title": "Data Science",
    "section": "",
    "text": "Naughty data contain the same information as a standard row x column (i.e. 2-dimensional) data frame but break tone or more of the following best practices.\n\nEach cell contains a single value\nEach variable must have its own column\nEach observation must have its own row\n\nNaughty data are very common in biology, this often occurs when either the collector(s) were not familiar with these best practices, or decisions are made to favour human readibility and usability rather than computer interpretability.\n\n\n\nExamples of Naughty Data from Wu et al. (2022), shown on the left side. The right side shown well-behaved data, with one observation per row, and column names as the first row.\n\n\nNaughty data can be very time consuming to fix, but regular expressions can make this a bit easier, as discussed in the Regular Expressions Chapter."
  },
  {
    "objectID": "datascience.html#dates",
    "href": "datascience.html#dates",
    "title": "Data Science",
    "section": "",
    "text": "As biologists, we often work with dates or time. We may want to analyze the date a sample was collected, or a measurement was taken. But dates are often encoded in formats that don’t fit neatly into the usual data types that we’ve worked with so far. The lubridate package provides a convenient framework for switching between human-readable dates and mathematical relationships among them – for example, the number of days, minutes, or seconds between two time points.\n\nlibrary(lubridate)\n\nIt can be convenient to automatically include the current date or time, especially when you are producing reports in R.\nWe can get the date as a date object in the year-month-day format,\n\ntoday()\n\n[1] \"2024-01-29\"\n\n\nAnd we can get the date along with the time as a datetime object, in the year-month-day hour:minute:second timezone format.\n\nnow()\n\n[1] \"2024-01-29 20:45:14 EST\"\n\n\nThis is an important distinction, because the datetime object extends on the date object to include hours, minutes, seconds, and time zone.\n\n\nWe can use a datetime object to track the run time of our code. The run time is how long it takes to run a particular program. We first create an object to store the computer clock before the program runs, then we subtract that object from the computer clock after the program finishes:\n\nBefore&lt;-now()\n\nfor(i in 1:1000){\n  rpois(10,lambda=10)^rnorm(10)\n}\n\nnow()-Before\n\nTime difference of 0.0129571 secs\n\n\nNote that your run time may be different, depending on the specifications of your computer and other processes that are running on it.\nThis technique can be useful for estimating the run time of a large program. Specifically, we run a fraction of the data and/or loop iterations and multiply to get an estimate the run time for the full data set.\n\nQuestion: If you add a print() function to print out the result in each iteration of the for loop, how much does this slow down the run time?\n\nAnswer: Try it to find out!\nFor a small program like this, it’s not a problem to ad a few fractions of a second. But this can scale up exponentially in ‘big data’ applications. For example, imagine you are sequencing a human genome at 100× coverage, which might typically involve one a billion sequences, each 300 bases long. You want to write a program to assemble all of the sequences into a full genome. How much time would it add to your program run time if you add a 1 millisecond step for each sequence, or each base pair in the experiment?\nFor each sequence, it would be \\(10^9sequences \\times 0.001 s = 10^6 s\\), or about 11.5 days! For each base pair, multiply by 300 – it would take more than 10 years!\nThese are problems of scale that don’t matter so much when you are starting out, but developing an efficiency mindset early, it can pay off when we move on to bigger data projects.\nNext, we’ll explore the date and datetime objects in more detail.\n\n\n\nHuman-readable dates come in many different forms, which we can encode as strings. Here are some examples that we might see for encoding the end-date of the most recent 5,126-year-long Mesoamerican calendar used by the ancient Maya civilization:\n\nDate2&lt;-\"2012-12-21\"\nDate3&lt;-\"21.12.2012\"\nDate4&lt;-\"Dec 21, 2012\"\nDate5&lt;-\"21 December, 2012\"\n\nThe lubridate package has a number of related functions that correspond to the order of entry – d for day, m for month, and y for year:\n\nymd(Date2)\n\n[1] \"2012-12-21\"\n\ndmy(Date3)\n\n[1] \"2012-12-21\"\n\nmdy(Date4)\n\n[1] \"2012-12-21\"\n\ndmy(Date5)\n\n[1] \"2012-12-21\"\n\n\nNotice the flexibility here! Some dates have dashes, dots, spaces, and commas! Yet, all are easily converted to a common object type. On the surface, these objects look like simple strings, but compare the structure of the date object with its original input string:\n\nstr(Date2)\n\n chr \"2012-12-21\"\n\nstr(ymd(Date2))\n\n Date[1:1], format: \"2012-12-21\"\n\n\nNotice how the first is a simple chr character object, whereas the second is a Date object. The date object can be treated as a numeric variable, that outputs as a readable date. For example, what if we want to know what the date 90 days before or after?\n\nc(ymd(Date2)-90,ymd(Date2)+90)\n\n[1] \"2012-09-22\" \"2013-03-21\"\n\n\nWhen using date objects, R will even account for the different number of days in each month, and adjust for leap years!\nAs with any function in R, we can apply ymd() to a column in a data frame or any other vector of strings:\n\nDateVec&lt;-c(\"2012-12-1\",\"2012-12-20\",\n           \"2012-12-23\", \"2012-11-05\")\nymd(DateVec)\n\n[1] \"2012-12-01\" \"2012-12-20\" \"2012-12-23\" \"2012-11-05\"\n\n\nThe elements must have the have the same order as the function, but surprisingly they don’t have to have the same format:\n\nMixVec&lt;-c(\"2012-12-11\",\"12-20-2012\",\n          \"2012, December 21\", \"2013, Nov-11\")\nymd(MixVec)\n\nWarning: 1 failed to parse.\n\n\n[1] \"2012-12-11\" NA           \"2012-12-21\" \"2013-11-11\"\n\n\nNote the warning and the resulting output. The first, third, and fourth elements are converted, even though they have different formats. The second element is replaced with NA because the order is not year, month, day, as required by ymd().\nBecause these objects are numeric, we can also use them for plotting:\n\nggplot() + \n  geom_histogram(aes(ymd(DateVec)),\n                 binwidth=1)\n\n\n\n\n\n\n\n\n\nQuestion: What do you notice about the x-axis? How would it differ if we used strings instead?\n\nAnswer: Try to run the above with DateVec as a string instead of date.\nYou should get an error, because you can’t bin values of a character variable to generate a frequency histogram. This shows us that R is able to treat the date object as a continuous variable, spacing the bins based on the number of days separating them.\n\n\n\nThe datetime object adds a time element to the date. Just as there are different functions to translate different date formats, there are different datetime functions. Each datetime function starts with one of the three date functions we used earlier, but then adds time elements after an underscore _ to define nine different functions. Here are a few examples:\n\nmdy_h(\"Dec 21, 2012 -- 10am\")\n\n[1] \"2012-12-21 10:00:00 UTC\"\n\nymd_hm(\"2012-12-21, 08:30\")\n\n[1] \"2012-12-21 08:30:00 UTC\"\n\ndmy_hms(\"21 December, 2012; 11:59:59\")\n\n[1] \"2012-12-21 11:59:59 UTC\"\n\n\n\n\n\nWe can extract components from date and datetime objects. For example, we can extract only the year:\n\nyear(Date2)\n\n[1] 2012\n\n\nor the month:\n\nmonth(Date2)\n\n[1] 12\n\n\nWe have several options for days.\nFirst, we can get the day of the year, also known as the Julian day:\n\nyday(Date2)\n\n[1] 356\n\n\nOr the day of the month\n\nmday(Date2)\n\n[1] 21\n\n\nor the day of the week (rather than month) using wday()\n\nwday(Date2)\n\n[1] 6\n\n\nWe can use the label=T parameter rather than number, to get the name of the specific month or day, rather than the number:\n\nmonth(Date2, label=T)\n\n[1] Dec\n12 Levels: Jan &lt; Feb &lt; Mar &lt; Apr &lt; May &lt; Jun &lt; Jul &lt; Aug &lt; Sep &lt; ... &lt; Dec\n\nwday(Date2, label=T)\n\n[1] Fri\nLevels: Sun &lt; Mon &lt; Tue &lt; Wed &lt; Thu &lt; Fri &lt; Sat\n\n\n\n\n\nWe’ve seen above how dates are more like numeric objects rather than strings, but we can also treat dates as categorical data.\nOne example that is becoming more and more common in biology is the analysis of data from data loggers, which automatically save observations over time. Think of climate stations that measure temperature and precipitation as a common example. Another example might be location information of a study organism using image analysis or PIT tags (i.e. Passive Integrated Transponders).\nIn many cases, the timescale of collection is much shorter than what we need for our analysis. We end up with too much data! Luckily, dplyr with lubridate offer a great combination for summarizing these data.\n\n\nHere’s an example that takes advantage that takes advantage of our ability to treat components of datetime objects as categorical variables as well as continuous variables.\nImagine we have observations taken every minute, and we just want to calculate the average for each hour . To see how to do this, we will generate a toy data set in R using the tibble function, and then assigning column names: DayTime for the day-time object and Obs for the observation (i.e. measurement):\nFirst, we create the imaginary dataset, using the replicate function and a random number generator:\n\nTestData&lt;-tibble(\n  DayTime=now()+minutes(rep(c(0:359),100)),\n  Obs=rnorm(36000))\n\nWe can calculate the hourly average by piping our TestData tibble (above) through a group_by and then a summarise() function.\n\nTestData %&gt;%\n  group_by(yday(DayTime),hour(DayTime)) %&gt;%\n  summarise(Mean=mean(Obs))\n\n`summarise()` has grouped output by 'yday(DayTime)'. You can override using the\n`.groups` argument.\n\n\n# A tibble: 7 × 3\n# Groups:   yday(DayTime) [2]\n  `yday(DayTime)` `hour(DayTime)`      Mean\n            &lt;dbl&gt;           &lt;int&gt;     &lt;dbl&gt;\n1              29              20  0.0161  \n2              29              21 -0.0170  \n3              29              22  0.0111  \n4              29              23 -0.0108  \n5              30               0 -0.0159  \n6              30               1 -0.0283  \n7              30               2 -0.000328\n\n\n\nQuestion Why do we include yday() in the group_by function?\n\nAnswer: Remove yday(DayTime) and compare the output."
  },
  {
    "objectID": "customfunctions.html",
    "href": "customfunctions.html",
    "title": "Custom Functions",
    "section": "",
    "text": "So far, just about everything you have done in this book has involved the use of functions. From basic c() o advanced functions like ggplot(), somebody had to sit down and write the code to make these functions do something useful. Now it is your turn.\nCustom functions tend to beuseful whenever you find yourself repeating parts of your code. For example, maybe you are repeating a calculation across different data columns of data, or across different input data sets, or you might want to compare different models by tweaking slightly different parameters. Condensing repeated code into functions can help make your code more concise, organized, and understandable. Functions can also make repeated code run faster, as we very briefly introduced in the Flow Control chapter. Functions can also help to avoid errors, because you only change parameters in one place – when you run the code – instead of replacing parameters in all of the steps of the function.\nGiving your custom function a clear name and specifying the arguments that it takes, will help make your code easier to understand. This is important for collaboration, even if you are collaborating with yourself at some time in the future.\nCustom functions can be a little tricky to master at first, but you’ve already taken the biggest step by learning how to run functions that others have written. By digging into the help files of the functions you know, you should get a good feel for what functions can do and how they are organized with parameters. In this chapter, you will learn how to build your own functions, and some of the ways they may be particularly useful to you.\n\n\n\nYou should already have a good sense of how functions work from all of the other tutorials/chapters. Now let’s work through a real example. Don’t type this out, but read through it:\n\nfunctionName&lt;-function(var1=Default1,var2=Default2){\n  ## Meat and potatoes script\n  return(output)\n}\n\nThis is called pseudo-code, which means that it is not working R code, but rather a sort of fake code to act as a framework for understanding the basics. You’ll come across this if you decide you want to dive deeper into computer science. The purpose of this pseudo-code is to give a general sense of functions work, regardless of the specific programming language you use (e.g., R, Python, Unix). We have variables (var1 and var2) and we can assign default values (Default1 and Default2). The comment ## Meat and potatoes script represents the main steps of the function. The final return(output) defines which objects from the function are output or returned to the user.\nTake some time to think about this pseudo-code, and compare it to the help for some functions you know, like ?mean(), ?ggplot() or ?read.csv(). By now you are quite familiar with the idea that each function has a name and a set of parameters (or arguments in R lingo). The default parameters are the ones that are written inside the help file of the function. For example the sep = \",\" argument in the read.csv() function tells us that the function will assume you want the sep agrument to be a comma unless you explicitly define it. Some arguments do not have a default. For example the n argument in the rnorm() function does not have a default value. That’s why you get an error if you try to run rnorm() but not if you run rnorm(1).\nThat’s everything you need to know to start writing your own functions!\n\n\n\nWe’ll start by converterting the above pesudo-code into functional R code. We’ll set default arguments, where the user will input two numeric objects, just to see how it works. The idea is that we want to calculate some basic math for a pair of input vectors, and output the results as a list.\n\nmy.function&lt;-function(var1=0,var2=0){\n  # We can make new variables within a function\n  add&lt;-var1+var2\n  subt&lt;-var1-var2\n  mult&lt;-var1*var2\n  div&lt;-var1/var2 \n  # And put them together into a list for output\n  outlist&lt;-list(input1=var1, input2=var2, \n                addition=add, subtraction=subt, \n                multiplication=mult, division=div)\n  # So far, everything is contained within the function. \n  # Use return() to generate output\n  return(outlist)\n}\n\nOn the first line, we define the function by giving it a name (my.function) and setting parameters for the function. In this function we have just two parameters, one for each input variables. If one or both are left blank when we call the function, then R will replace these values with the default (zero). These variables may be individual numbers, or we may input vectors and R will automatically apply them to each element.\nInside of the function, we generate five objects, the first four representing simple mathematical equations applied to the two input variables. A fifth object (outlist) is simply a list object containing the two input variables and the output of each of the four equation objects.\nThe final line return() contains the object that is output from running the custom function. In this case, it is the outlist list object.\nAs usual, be sure to type out this function in a .R script or a code chunk in a .Rmd file. Try running the first line of the function. You’ll see a + sign in your R console. This is R telling you that it is expecting more lines of code. This happens when you have open brackets or an unfinished pipe (%&gt;%) or ggplot (+) command. Run each of the remaining lines of the function, and you should see the R Console return to &gt; after you run the last line of the function.\n\nQuestion: Why is there no output to the R Console?\n\nAnswer: You have just loaded the function into memory. Think of this like when you use library() to load a package into memory. The functions from that package are now available for use.\nLook at the Environment tab in R Studio. This is a tab in one of the R Studio windows – usually in the top-right window by default. You should see a new item here called my.function followed by function (var1 = 0, var2 = 0). This tells us that the my.function() function is available for use, and it has two input parameters with default values.\n\n\n\nBefore running your function, first note what is missing from the Environment tab: none of the objects that are created inside the function are listed here. For example, add, subt and outlist. Even when we run the function, we won’t see those objects in the Environment here. These objects are local objects because they only exist within the function that contains them.\nBy contrast, a global object is created when we make an object in the main code.\n\nGlobal&lt;-\"Object\"\n\nGlobal objects are saved in memory and can be accessed by any function that you run. Local objects can only be used inside of the function that contains them.\n\n\n\nRunning custom functions is no different from running any of the other functions you are familiar with. Try running the function on its own, with default values:\n\nmy.function()\n\n$input1\n[1] 0\n\n$input2\n[1] 0\n\n$addition\n[1] 0\n\n$subtraction\n[1] 0\n\n$multiplication\n[1] 0\n\n$division\n[1] NaN\n\n\nNow try specifying the input parameters and compare the output.\n\nmy.function(var1=10,var2=0.1)\n\n$input1\n[1] 10\n\n$input2\n[1] 0.1\n\n$addition\n[1] 10.1\n\n$subtraction\n[1] 9.9\n\n$multiplication\n[1] 1\n\n$division\n[1] 100\n\nmy.function(var1=c(1:10),var2=c(10:1))\n\n$input1\n [1]  1  2  3  4  5  6  7  8  9 10\n\n$input2\n [1] 10  9  8  7  6  5  4  3  2  1\n\n$addition\n [1] 11 11 11 11 11 11 11 11 11 11\n\n$subtraction\n [1] -9 -7 -5 -3 -1  1  3  5  7  9\n\n$multiplication\n [1] 10 18 24 28 30 30 28 24 18 10\n\n$division\n [1]  0.1000000  0.2222222  0.3750000  0.5714286  0.8333333  1.2000000\n [7]  1.7500000  2.6666667  4.5000000 10.0000000\n\n\n\n\n\nFor more complicated functions that take a long time to run, consider using print() or cat() to indicate the steps that are being run. This can help a lot with troubleshooting custom functions. The cat function is similar to print but lets you print directly to screen rather than passing through a data object. Recall from the Regular Expression Chapter that \\n is the new line character. If we include \\n in the cat() output, then it will print to a new line. Here is an example:\n\nmy.function&lt;-function(var1=0,var2=0){\n  cat(\"\\nInput variables:\\nvar1 =\", var1,\"\\nvar2 =\", var2,\"\\n\")\n  cat(\"\\nCalculating functions...\\n\")\n  cat(\"\\nAdding...\\n\")    \n\n  add&lt;-var1+var2\n  \n  cat(\"\\nSubtracting...\\n\")\n\n  subt&lt;-var1-var2\n  \n  cat(\"\\nMultiplying...\\n\")\n  \n  mult&lt;-var1*var2\n  \n  cat(\"\\nDividing...\\n\")\n  \n  div&lt;-var1/var2  \n  \n  cat(\"\\nGenerating output...\\n\\n\")\n  \n  outlist&lt;-list(input1=var1, input2=var2, \n                addition=add, subtraction=subt, \n                multiplication=mult, division=div)\n\n  return(outlist)\n}\n\n## Run\nmy.function(var1=10,var2=0.1)\n\n\nInput variables:\nvar1 = 10 \nvar2 = 0.1 \n\nCalculating functions...\n\nAdding...\n\nSubtracting...\n\nMultiplying...\n\nDividing...\n\nGenerating output...\n\n\n$input1\n[1] 10\n\n$input2\n[1] 0.1\n\n$addition\n[1] 10.1\n\n$subtraction\n[1] 9.9\n\n$multiplication\n[1] 1\n\n$division\n[1] 100\n\n\n\n\n\nPrinting text to the screen can slow down your function considerably, as we saw in the Flow Control Chapter. A good practice is to provide output as a user-defined option by adding a ‘verbose’ parameter and an if() statement.\n\nmy.function&lt;-function(var1=0,var2=0,verbose=FALSE){\n  if(verbose==T){\n    cat(\"\\nInput variables:\\nvar1 =\", var1,\"\\nvar2 =\", var2,\"\\n\")\n    cat(\"\\nCalculating functions...\\n\")\n    cat(\"\\nAdding...\\n\")    \n  }\n  \n  add&lt;-var1+var2\n  \n  if(verbose==T){\n    cat(\"\\nSubtracting...\\n\")\n  }\n  \n  subt&lt;-var1-var2\n  \n  if(verbose==T){\n    cat(\"\\nMultiplying...\\n\")\n  }\n  \n  mult&lt;-var1*var2\n  \n  if(verbose==T){\n    cat(\"\\nDividing...\\n\")\n  }\n  \n  div&lt;-var1/var2  \n  \n  if(verbose==T){\n    cat(\"\\nGenerating output...\\n\")\n  }\n  \n  outlist&lt;-list(input1=var1, input2=var2,\n                addition=add, subtraction=subt,\n                multiplication=mult, division=div)\n\n  return(outlist)\n}\n\nNow the Outlist is returned, but the cat() functions are only run if Verbose=T is selected when running the function.\n\n\n\nIn the Basic Customization Chapter, we saw how to create a custom plotting theme and save it as a file that we could load to apply the theme. The same is true for custom functions.\nIf you have a custom function that you would like to use frequently, or if it is too big to include in your main R Script or R Markdown file, then a dedicated .R file may be a good option.\n\nSave in a separate file, typically with a .R extension. For example, we might make new R Script called myfunction.R containing just the lines of the my.function() function that we created earlier.\nLoad using source(\"PathName.FileName.R\"). For example, we may have a directory called scripts inside of our working directory, in which case we could load the custom function with source(\"./scripts/myfunction.R\")."
  },
  {
    "objectID": "customfunctions.html#introduction",
    "href": "customfunctions.html#introduction",
    "title": "Custom Functions",
    "section": "",
    "text": "So far, just about everything you have done in this book has involved the use of functions. From basic c() o advanced functions like ggplot(), somebody had to sit down and write the code to make these functions do something useful. Now it is your turn.\nCustom functions tend to beuseful whenever you find yourself repeating parts of your code. For example, maybe you are repeating a calculation across different data columns of data, or across different input data sets, or you might want to compare different models by tweaking slightly different parameters. Condensing repeated code into functions can help make your code more concise, organized, and understandable. Functions can also make repeated code run faster, as we very briefly introduced in the Flow Control chapter. Functions can also help to avoid errors, because you only change parameters in one place – when you run the code – instead of replacing parameters in all of the steps of the function.\nGiving your custom function a clear name and specifying the arguments that it takes, will help make your code easier to understand. This is important for collaboration, even if you are collaborating with yourself at some time in the future.\nCustom functions can be a little tricky to master at first, but you’ve already taken the biggest step by learning how to run functions that others have written. By digging into the help files of the functions you know, you should get a good feel for what functions can do and how they are organized with parameters. In this chapter, you will learn how to build your own functions, and some of the ways they may be particularly useful to you."
  },
  {
    "objectID": "customfunctions.html#general-form",
    "href": "customfunctions.html#general-form",
    "title": "Custom Functions",
    "section": "",
    "text": "You should already have a good sense of how functions work from all of the other tutorials/chapters. Now let’s work through a real example. Don’t type this out, but read through it:\n\nfunctionName&lt;-function(var1=Default1,var2=Default2){\n  ## Meat and potatoes script\n  return(output)\n}\n\nThis is called pseudo-code, which means that it is not working R code, but rather a sort of fake code to act as a framework for understanding the basics. You’ll come across this if you decide you want to dive deeper into computer science. The purpose of this pseudo-code is to give a general sense of functions work, regardless of the specific programming language you use (e.g., R, Python, Unix). We have variables (var1 and var2) and we can assign default values (Default1 and Default2). The comment ## Meat and potatoes script represents the main steps of the function. The final return(output) defines which objects from the function are output or returned to the user.\nTake some time to think about this pseudo-code, and compare it to the help for some functions you know, like ?mean(), ?ggplot() or ?read.csv(). By now you are quite familiar with the idea that each function has a name and a set of parameters (or arguments in R lingo). The default parameters are the ones that are written inside the help file of the function. For example the sep = \",\" argument in the read.csv() function tells us that the function will assume you want the sep agrument to be a comma unless you explicitly define it. Some arguments do not have a default. For example the n argument in the rnorm() function does not have a default value. That’s why you get an error if you try to run rnorm() but not if you run rnorm(1).\nThat’s everything you need to know to start writing your own functions!"
  },
  {
    "objectID": "customfunctions.html#example-function",
    "href": "customfunctions.html#example-function",
    "title": "Custom Functions",
    "section": "",
    "text": "We’ll start by converterting the above pesudo-code into functional R code. We’ll set default arguments, where the user will input two numeric objects, just to see how it works. The idea is that we want to calculate some basic math for a pair of input vectors, and output the results as a list.\n\nmy.function&lt;-function(var1=0,var2=0){\n  # We can make new variables within a function\n  add&lt;-var1+var2\n  subt&lt;-var1-var2\n  mult&lt;-var1*var2\n  div&lt;-var1/var2 \n  # And put them together into a list for output\n  outlist&lt;-list(input1=var1, input2=var2, \n                addition=add, subtraction=subt, \n                multiplication=mult, division=div)\n  # So far, everything is contained within the function. \n  # Use return() to generate output\n  return(outlist)\n}\n\nOn the first line, we define the function by giving it a name (my.function) and setting parameters for the function. In this function we have just two parameters, one for each input variables. If one or both are left blank when we call the function, then R will replace these values with the default (zero). These variables may be individual numbers, or we may input vectors and R will automatically apply them to each element.\nInside of the function, we generate five objects, the first four representing simple mathematical equations applied to the two input variables. A fifth object (outlist) is simply a list object containing the two input variables and the output of each of the four equation objects.\nThe final line return() contains the object that is output from running the custom function. In this case, it is the outlist list object.\nAs usual, be sure to type out this function in a .R script or a code chunk in a .Rmd file. Try running the first line of the function. You’ll see a + sign in your R console. This is R telling you that it is expecting more lines of code. This happens when you have open brackets or an unfinished pipe (%&gt;%) or ggplot (+) command. Run each of the remaining lines of the function, and you should see the R Console return to &gt; after you run the last line of the function.\n\nQuestion: Why is there no output to the R Console?\n\nAnswer: You have just loaded the function into memory. Think of this like when you use library() to load a package into memory. The functions from that package are now available for use.\nLook at the Environment tab in R Studio. This is a tab in one of the R Studio windows – usually in the top-right window by default. You should see a new item here called my.function followed by function (var1 = 0, var2 = 0). This tells us that the my.function() function is available for use, and it has two input parameters with default values."
  },
  {
    "objectID": "customfunctions.html#local-vs-global",
    "href": "customfunctions.html#local-vs-global",
    "title": "Custom Functions",
    "section": "",
    "text": "Before running your function, first note what is missing from the Environment tab: none of the objects that are created inside the function are listed here. For example, add, subt and outlist. Even when we run the function, we won’t see those objects in the Environment here. These objects are local objects because they only exist within the function that contains them.\nBy contrast, a global object is created when we make an object in the main code.\n\nGlobal&lt;-\"Object\"\n\nGlobal objects are saved in memory and can be accessed by any function that you run. Local objects can only be used inside of the function that contains them."
  },
  {
    "objectID": "customfunctions.html#running-functions",
    "href": "customfunctions.html#running-functions",
    "title": "Custom Functions",
    "section": "",
    "text": "Running custom functions is no different from running any of the other functions you are familiar with. Try running the function on its own, with default values:\n\nmy.function()\n\n$input1\n[1] 0\n\n$input2\n[1] 0\n\n$addition\n[1] 0\n\n$subtraction\n[1] 0\n\n$multiplication\n[1] 0\n\n$division\n[1] NaN\n\n\nNow try specifying the input parameters and compare the output.\n\nmy.function(var1=10,var2=0.1)\n\n$input1\n[1] 10\n\n$input2\n[1] 0.1\n\n$addition\n[1] 10.1\n\n$subtraction\n[1] 9.9\n\n$multiplication\n[1] 1\n\n$division\n[1] 100\n\nmy.function(var1=c(1:10),var2=c(10:1))\n\n$input1\n [1]  1  2  3  4  5  6  7  8  9 10\n\n$input2\n [1] 10  9  8  7  6  5  4  3  2  1\n\n$addition\n [1] 11 11 11 11 11 11 11 11 11 11\n\n$subtraction\n [1] -9 -7 -5 -3 -1  1  3  5  7  9\n\n$multiplication\n [1] 10 18 24 28 30 30 28 24 18 10\n\n$division\n [1]  0.1000000  0.2222222  0.3750000  0.5714286  0.8333333  1.2000000\n [7]  1.7500000  2.6666667  4.5000000 10.0000000"
  },
  {
    "objectID": "customfunctions.html#annotation",
    "href": "customfunctions.html#annotation",
    "title": "Custom Functions",
    "section": "",
    "text": "For more complicated functions that take a long time to run, consider using print() or cat() to indicate the steps that are being run. This can help a lot with troubleshooting custom functions. The cat function is similar to print but lets you print directly to screen rather than passing through a data object. Recall from the Regular Expression Chapter that \\n is the new line character. If we include \\n in the cat() output, then it will print to a new line. Here is an example:\n\nmy.function&lt;-function(var1=0,var2=0){\n  cat(\"\\nInput variables:\\nvar1 =\", var1,\"\\nvar2 =\", var2,\"\\n\")\n  cat(\"\\nCalculating functions...\\n\")\n  cat(\"\\nAdding...\\n\")    \n\n  add&lt;-var1+var2\n  \n  cat(\"\\nSubtracting...\\n\")\n\n  subt&lt;-var1-var2\n  \n  cat(\"\\nMultiplying...\\n\")\n  \n  mult&lt;-var1*var2\n  \n  cat(\"\\nDividing...\\n\")\n  \n  div&lt;-var1/var2  \n  \n  cat(\"\\nGenerating output...\\n\\n\")\n  \n  outlist&lt;-list(input1=var1, input2=var2, \n                addition=add, subtraction=subt, \n                multiplication=mult, division=div)\n\n  return(outlist)\n}\n\n## Run\nmy.function(var1=10,var2=0.1)\n\n\nInput variables:\nvar1 = 10 \nvar2 = 0.1 \n\nCalculating functions...\n\nAdding...\n\nSubtracting...\n\nMultiplying...\n\nDividing...\n\nGenerating output...\n\n\n$input1\n[1] 10\n\n$input2\n[1] 0.1\n\n$addition\n[1] 10.1\n\n$subtraction\n[1] 9.9\n\n$multiplication\n[1] 1\n\n$division\n[1] 100"
  },
  {
    "objectID": "customfunctions.html#verbose-parameter",
    "href": "customfunctions.html#verbose-parameter",
    "title": "Custom Functions",
    "section": "",
    "text": "Printing text to the screen can slow down your function considerably, as we saw in the Flow Control Chapter. A good practice is to provide output as a user-defined option by adding a ‘verbose’ parameter and an if() statement.\n\nmy.function&lt;-function(var1=0,var2=0,verbose=FALSE){\n  if(verbose==T){\n    cat(\"\\nInput variables:\\nvar1 =\", var1,\"\\nvar2 =\", var2,\"\\n\")\n    cat(\"\\nCalculating functions...\\n\")\n    cat(\"\\nAdding...\\n\")    \n  }\n  \n  add&lt;-var1+var2\n  \n  if(verbose==T){\n    cat(\"\\nSubtracting...\\n\")\n  }\n  \n  subt&lt;-var1-var2\n  \n  if(verbose==T){\n    cat(\"\\nMultiplying...\\n\")\n  }\n  \n  mult&lt;-var1*var2\n  \n  if(verbose==T){\n    cat(\"\\nDividing...\\n\")\n  }\n  \n  div&lt;-var1/var2  \n  \n  if(verbose==T){\n    cat(\"\\nGenerating output...\\n\")\n  }\n  \n  outlist&lt;-list(input1=var1, input2=var2,\n                addition=add, subtraction=subt,\n                multiplication=mult, division=div)\n\n  return(outlist)\n}\n\nNow the Outlist is returned, but the cat() functions are only run if Verbose=T is selected when running the function."
  },
  {
    "objectID": "customfunctions.html#external-files",
    "href": "customfunctions.html#external-files",
    "title": "Custom Functions",
    "section": "",
    "text": "In the Basic Customization Chapter, we saw how to create a custom plotting theme and save it as a file that we could load to apply the theme. The same is true for custom functions.\nIf you have a custom function that you would like to use frequently, or if it is too big to include in your main R Script or R Markdown file, then a dedicated .R file may be a good option.\n\nSave in a separate file, typically with a .R extension. For example, we might make new R Script called myfunction.R containing just the lines of the my.function() function that we created earlier.\nLoad using source(\"PathName.FileName.R\"). For example, we may have a directory called scripts inside of our working directory, in which case we could load the custom function with source(\"./scripts/myfunction.R\")."
  },
  {
    "objectID": "bookinfo.html",
    "href": "bookinfo.html",
    "title": "Book Info",
    "section": "",
    "text": "Book Info\nThis website contains the complete text of the second edition of R Crash Course for Biologists: An introduction to R for bioinformatics and biostatistics.\nThis is the first book in the Quantitative Biology Series, based on lecture from Dr. Robert I. Colautti given at Queen’s University in Canada (Lab website: https://EcoEvoGeno.org).\nThis text is offered free of charge, but please consider purchasing e-book and/or print versions of this textbook to support development of future content.\nIf you can’t afford any of the above, please consider donating your time to provide an honest review: colauttilab@gmail.com. Please let us know what you liked, what worked well, and anything that you think can be improved."
  },
  {
    "objectID": "advanced_vis.html",
    "href": "advanced_vis.html",
    "title": "Advanced Visualizations",
    "section": "",
    "text": "Before continuing with this tutorial/chapter, you should be familiar with the basics of ggplot from the previous chapters, and you should have lots of practice making graphs with different formatting options.\nIn this chapter, we look at some important rules of thumb for making professional and effective visualizations. Then, we will work through a detailed example with more advanced options for visualizations with the ggplot function. This includes everything you will need to make professional-grade figures.\nThe ggplot cheat sheet may be useful for you, both for this chapter and in the future when making your own figures. A cheat sheet is a printable file that provides a good summary and quick-reference guide for a particular function or activity. In this case, the ggplot cheat sheet provides a quick summary of a lot of the main graph types and parameters in the ggplot2 library. It can be found along with other useful cheat sheets on the Posit website: https://posit.co/resources/cheatsheets/\n\n\n\nFirst, we’ll load the ggplot2 library and set a custom plotting theme as described in the previous chapter.\n\nlibrary(ggplot2)\nsource(\"http://bit.ly/theme_pub\")\ntheme_set(theme_pub())\n\nThe source function loads an external file, in this case from the internet. The file is just a script saved as .R file with a custom function defining different aspects of the graph (e.g. text size, line width, etc.) You can open the link in a web browser or download and open in a text editor to see the script\nThe theme_set() function sets our custom theme (theme_pub) as the default plotting theme. Since the theme is a function in R, we need to include the brackets, even though twe don’t want to change anything in the function: theme_pub()\n\n\n\nBefore we dig into the code, it’s worth reviewing some more general graphical concepts. Standards of practice for published graphs in professional journals can vary depending on format (e.g. print vs online), audience, and historical precedent. Nevertheless, there are a number of useful ‘rules of thumb’ to keep in mind. These are not hard and fast rules but helpful for new researchers who aren’t sure how or where to start. In making decisions, always think of your audience and remember that the main goal is to communicate information as clearly and efficientl as possible.\n1. Minimize ‘ink’\nIn the old days, when most papers were actually printed and mailed to journal subscribers, black ink was expensive and printing in colour was very expensive. Printing is still expensive but of course most research articles are available online where there is no additional cost for colour or extra ink. However, the concept of minimizing ink (or pixels) can go a long way toward keeping a graph free from clutter and unnecessary distraction.\n2. Use space wisely\nEmpty space is not necessarily bad, but ask yourself if it is necessary and what you want the reader to take away. Consider the next two graphs:\n\n\nWarning: package 'dplyr' was built under R version 4.3.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the first example, the Y-axis is scaled to the data. In the second case, Y-axis scaled between 0 and 100.\n\nQuestion: What are the benefits/drawbacks of scaling the axes? When might you choose to use one over the other?\n\n3. Choose a colour palette\nColour has three basic components:\n\nHue – the relative proportion of red vs green vs blue light\nSaturation – how vivid the colour is\nBrightness – the amount of white (vs black) in the colour\n\nThe abbreviation HSB is often used, or HSL (L = ‘Lightness’) or HSV (V = ‘Value’).\nIn R these can be easily defined with the rgb() function. For example:\n\nrgb(1,0,0) – a saturated red\nrgb(0.1,0,0) – a dark red (low brightness, low saturation)\nrgb(1,0.9,0.9) – a light red (high brightness, low saturation)\n\nDon’t underestimate the impact of choosing a good colour palette, especially for presentations. Colour theory can get a bit overwhelming but here are a few good websites to help:\n\nQuickly generate your own palette using Coolors: https://coolors.co\nUse a colour wheel to find complementary colours using Adobe: https://color.adobe.com/create\n\n4. Colours have meaning\nTry running this code and veiw the output in colour:\n\nX&lt;-rnorm(100)\nY&lt;-X+seq_along(X)\nD&lt;-data.frame(Temperature=Y,Location=X,Temp=Y/3)\nqplot(Location, Temperature,colour=Temp, data=D) + \n  scale_color_gradient(high=\"blue\", low=\"red\")\n\n\nQuestion: What strikes you as odd about this graph (not shown)?\n\nTechnically, there is nothing wrong. But we naturally associate colours with particular feelings. In this case, intuitively we associate red with hot and blue with cold, which is the opposite of what is shown in this graph. Be mindful of these associations when choosing a colour palette.\nAnother important consideration is that not everyone sees colour the same way. About 5% to 10% of the population has colour blindness. In order to make colour graphs readable to everyone, you should make sure to use colours that can still be interpreted when printed in greyscale, as explained in the Quick Visualizations Chapter.\n5. Maximize contrast\nColours that are too similar will be hard to distinguish.\n\n\n\n\n\n\n\n\n\nCan you see the gradient of colours? The difference among colours is called contrast, and generally a high-contrast palette is more informative than a low-contrast palette. Here is a plot of the same data, plotted with a wider range of colours:\n\nggplot(aes(Latitude,Longitude,colour=Precip), data=D) + \n  geom_point() +\n  scale_color_gradient(high=\"cyan\", low=\"red\")\n\n\n\n\n\n\n\n\n6. Keep relevant information\nMake sure to include proper axis labels (i.e. names) and tick marks (i.e. numbers or categories showing the different values). These labels, along with the figure caption, should act as a stand-alone unit. The reader should be able to understand the figure without having to read through the rest of the paper.\n7. Choose the right graph\nOften the same data can be presented in different ways but some are easier to interpret than others. Think carefully about the story you want to present and the main ideas you want your reader to get from your figures. Look at these two graphs that show the same data.\n\nADat&lt;-data.frame(Biomass=rnorm(100), Treatment=\"Treatment A\")\nBDat&lt;-data.frame(Biomass=5 + rnorm(100) *5 + \n                   ADat$Biomass * 5, Treatment=\"Treatment B\")\nPDat&lt;-rbind(ADat,BDat)\nggplot(aes(Biomass, fill=Treatment), data=PDat)+\n  geom_histogram(posit=\"dodge\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThe first graph tells a story about the distributions – the mean and variance of each treatment\n\nggplot() + geom_point(aes(ADat$Biomass,BDat$Biomass)) +\n  xlab(\"Biomass in Treatment A\") + ylab (\"Biomass in Treatment B\")\n\n\n\n\n\n\n\n\nThe second graph tells a story about the correlated relationship between Treatment A and Treatment B.\nOne graph is not necessarily better than the other. It depends on the story you want to tell.\n\n\n\nNow that we have gone over some basic graphing concepts, let’s look at how to build a professional-grade figure. In fact, we’ll reconstruct a figure published in a paper by Colautti & Lau in the journal Molecular Ecology (2015): https://doi.org/10.1111/mec.13162\n\n\nThe paper is a meta-analysis and review of evolution occurring during biological invasions. We will recreate Figure 2, which shows the result of a meta-analysis of selection gradients (\\(\\beta\\)) and selection differentials (\\(s\\)). First, we’ll just recreate the top panel, and then we’ll look at ways to make more advanced multi-panel graphs like this.\nThe data from the paper are archived on Dryad: https://datadryad.org/stash/dataset/doi:10.5061/dryad.gt678\nYou could download the zip file and look for the file called Selection_Data.csv and save it to your working directory. But I have also put it on Github, so that you can download it directly to R:\n\nSelData&lt;-read.csv(\n  \"https://colauttilab.github.io/RCrashCourse/Selection_Data.csv\")\n\nWe are also going to change the column names in the file to make them a bit more intuitive and easier to work with in R.\n\nnames(SelData)&lt;-c(\"Collector\", \"Author\", \"Year\", \"Journal\", \n                  \"Vol\", \"Species\", \"Native\", \"N\", \n                  \"Fitness.measure\", \"Trait\", \"s\", \n                  \"s.SE\", \"s.P\", \"B\", \"B.SE\", \"B.P\")\n\n\n\n\nLet’s take a quick look at the data\n\nhead(SelData)\n\n          Collector               Author Year             Journal\n1 KingsolverDiamond Alatalo and Lundberg 1986           Evolution\n2 KingsolverDiamond Alatalo and Lundberg 1986           Evolution\n3 KingsolverDiamond Alatalo and Lundberg 1986           Evolution\n4 KingsolverDiamond       Alatalo et al. 1990 American Naturalist\n5 KingsolverDiamond       Alatalo et al. 1990 American Naturalist\n6 KingsolverDiamond       Alatalo et al. 1990 American Naturalist\n             Vol             Species Native    N       Fitness.measure\n1     40:574-583  Ficedula hypoleuca    yes  641   male mating success\n2     40:574-583  Ficedula hypoleuca    yes  713 female mating success\n3     40:574-583  Ficedula hypoleuca    yes 1705              survival\n4 135(3):464-471 Ficedula albicollis    yes &lt;NA&gt;              survival\n5 135(3):464-471 Ficedula albicollis    yes &lt;NA&gt;              survival\n6 135(3):464-471 Ficedula albicollis    yes &lt;NA&gt;              survival\n          Trait     s s.SE s.P     B B.SE B.P\n1 tarsus length -0.01       ns    NA         \n2 tarsus length  0.01      sig    NA         \n3 tarsus length  0.04       ns    NA         \n4  tarus length  0.02       ns -0.06         \n5  tarus length  0.08       ns -0.01         \n6  tarus length  0.19      sig  0.01         \n\n\nIt’s worth taking some time to look at this to understand how to encode data for a meta-analysis. The collector column indicates the paper that the data came from. The Author indicates the author(s) of the original paper that reported the data. The Year, Journal, and Vol give information about the publication that the data came from originally.\nWe can see above the collector KingsolverDiamond, which represents a paper from Kingsolver and Diamond that was itself a meta-analysis of natural selection. Most of the studies came from this meta-analysis, but a few of the more recent papers were added by grad students, denoted by initials:\n\nunique(SelData$Collector)\n\n[1] \"KingsolverDiamond\" \"JAL\"               \"DJW\"              \n[4] \"CPT\"              \n\n\nSpecies is the study species, and Native is its status as a binary yes/no variable. N is the sample size and Fitness.measure is the specific trait that defines fitness. Trait is the trait on which selection was measured. Finally, \\(s\\) is the selection differential and \\(\\beta\\) is the selection gradient. Note that these are slopes in units of relative fitness per trait standard deviation. This is explained in more detail below.\n\n\n\nIn this analysis, we are interested in the magnitude but not the direction of natural selection. In other words we would want to treat a slope of -4 the same as a slope of +4 because they have the same magnitude. Therefore, we can replace the \\(s\\) column with \\(|s|\\)\n\nSelData$s&lt;-abs(SelData$s)\n\nWe’ll also add a couple of columns with random variables that we can use later to explore additional plotting options.\nFirst, a column of values sampled from a z-distribution – this is a Gaussian (a.k.a. ‘normal’) distribution with mean = 0 and sd = 1.\n\nSelData$Rpoint&lt;-rnorm(nrow(SelData))\n\nSecond, a columnn of 1 and 0 sampled randomly with equal frequency (\\(p = 0.5\\))\n\nSelData$Rgroup&lt;-sample(c(0,1), nrow(SelData), replace=T)\n\n\nQuestion: Do you remember rnorm() and sample() from the R Fundamentals Chapter?\n\nIf not, it may be a good time for a quick review. Remember to keep practicing – recognizing code is not the same as being able to write it from scratch.\n\n\n\nBefore we plot the selection data, we should take a quick look at the values to check for potential errors.\n\nprint(SelData$s)\n\nThe output isn’t shown here, but note that NA is used to denote missing data in the output.\nWe can subset to remove missing data:\n\nSelData&lt;-SelData[!is.na(SelData$s),]\n\nRecall from the R Fundamentals Chapter that ! means ‘not’ or ‘invert’\nThere is also a convenient drop_na function in the tidyr package:\n\nlibrary(tidyr)\nSelData&lt;-SelData %&gt;%\n  drop_na(s)\n\n\n\n\n\n\n\nWe’re going to get a bit technical here. Don’t worry if you don’t completely understand all of the stuff below about measuring selection. Keep it for reference in case you decide you want to use it for your own research. For now, just try to understand it as well as you can and focus on the code used to produce the figures.\nAn analysis of phenotypic selection was proposed by Lande & Arnold (1983) as a simple but powerful tool for measuring natural selection. It is just a linear model with relative fitness on the y-axis and the standardized trait value(s) on the x-axis.\n\n\n\nFitness can be measured in many ways, such as survival or lifetime seed or egg production. Check out the list of specific fitness measures used in these studies:\n\nunique(SelData$Fitness.measure)\n\n(output not shown)\nAbsolute fitness is just the observed value (e.g. seed set or survival yes/no). Technicallly, we call these fitness components because they are not fitness per se, but they represent measurements of survival and reproduction, which are the key components that jointly determine fitness.\nRelative fitness is just the absolute fitness divided by the mean. Absolute fitness is usually denoted by the capital letter \\(W\\) and relative fitness is usually represented by a lower-case \\(w\\) or omega \\(\\omega\\). Expressing this in mathematical terms:\n\\[ \\omega = W_i/\\bar W \\]\nwhere \\(W_i\\) is the mean of the study sample.\n\n\n\nA Trait Value is just the measured trait on which selection may act. Use unique(SelData$Trait) to see the list of specific traits that were measured in these studies. The Standardized Trait Value is the traits z-score. See the Distributions Chapter in the book R STATS Crash Course for more information about z-scores. To calculate the z-score, we take each value, subtract the mean, and then divide by the standard deviation of the sample (\\(sd\\)):\n\\[\\frac{x_i-\\bar X}{sd}\\]\nSince traits have different metrics, they are hard to compare: e.g. days to flower, egg biomass, foraging intensity, aggression. But standardizing traits to z-scores puts them all on the same scale for comparison. Specifically, the scale of selection will be in standard deviations from the mean.\n\n\n\nSelection differentials (\\(s\\)) and selection gradients (\\(\\beta\\)) measure selection using linear models but represent slightly different measurements. Linear models are covered in the Linear Models Chapter in the book R STATS Crash Course.\nBoth models use relative fitness (\\(\\omega\\)) as the response variable.\nSelection differentials (\\(s\\)) measure selection on only a single trait, ignoring all other traits. In theory, the response to selection is a simple function of the genetic correlation between a trait and fitness.\nFitness differences among individuals can depend on a lot of things – genetic variation for the trait itself, but also environmental effects on the trait as well as effects on other traits that are under selection and correlated with the trait of interest.\nSelection gradients (\\(\\beta\\)) measure selection on a trait of interest while also accounting for selection on other correlated traits. This is done via a multiple regression – a linear model with multiple predictors.\nNow that we have reviewed the relevant biological background, we can plot \\(s\\) and \\(\\beta\\) to compare their distributions.\n\n\n\n\nLet’s start with a simple ggplot function. We are going to be adding layers to build up to more complex graphs, so we’ll start by creating a base plotting object to build on.\n\nBarPlot&lt;-ggplot(aes(x=s, fill=Native), data=SelData)\n\n\n\nRecall from the Basic Visualizations Chapter, the use of aesthetic function aes(). This defines the data that we want to use for our ggplot graph. We will see how we do this by adding layers to our plot, similar to the way old-timey cartoons were made by layering multiple clear pages of cellophane with characters painted on them. The aes function inside of the ggplot function defines that data that will be shared among all of the layers. In addition, we can have separate aes functions inside different geom_ layers that define and restrict the plotting data to that specific layer.\nLet’s look at the ggplot object so far:\n\nprint(BarPlot)\n\n\n\n\n\n\n\n\nNo data!\n\nQuestion: Why are there no data plotted?\n\nAnswer: We didn’t define a geom_ for the data yet.\n\n\n\nSo far, we have only loaded in the data for plotting. We have to specify which geom(s) we want. We’ll start with a bar plot:\n\nBarPlot&lt;- BarPlot + geom_bar() \nBarPlot\n\n\n\n\n\n\n\n\nLet’s explore the components of our Bar Plot object.\n\nsummary(BarPlot)\n\ndata: Collector, Author, Year, Journal, Vol, Species, Native, N,\n  Fitness.measure, Trait, s, s.SE, s.P, B, B.SE, B.P, Rpoint, Rgroup\n  [2766x18]\nmapping:  x = ~s, fill = ~Native\nfaceting: &lt;ggproto object: Class FacetNull, Facet, gg&gt;\n    compute_layout: function\n    draw_back: function\n    draw_front: function\n    draw_labels: function\n    draw_panels: function\n    finish_data: function\n    init_scales: function\n    map_data: function\n    params: list\n    setup_data: function\n    setup_params: function\n    shrink: TRUE\n    train_scales: function\n    vars: function\n    super:  &lt;ggproto object: Class FacetNull, Facet, gg&gt;\n-----------------------------------\ngeom_bar: just = 0.5, width = NULL, na.rm = FALSE, orientation = NA\nstat_count: width = NULL, na.rm = FALSE, orientation = NA\nposition_stack \n\n\nIt’s worth taking some time to understand the structure of this ggplot object how it relates to what gets plotted to the output.\n\nData shows which data are available for plotting. These are just the column names of the data.frame object we input with the data= parameter in the ggplot() function.\nMapping shows the variables from the aes() function, the scaling of the x-axis and the variable for the fill= colours.\nFaceting contains information for multiple plots. We’ll explore this more later, but a single graph has a facet_null() function.\nDashed line separates the ggplot() function from the other functions linked with the plus (+) in our plotting function. In our graph, there was + geom_bar(), which we can see below the dashed line.\ngeom_bar shows the (default) parameters used in our function\nstat_count shows our stat function. This was created by default with our + geom_bar() function; it determines how the data are transformed to geometric shapes for plotting (e.g., points, lines, or bars).\n\nThe output also shows some of the functions and parameters used to generate the graph. At the bottom we see parameters for geom_bar and stat_count. Note that there are more parameters listed than what we explicitly put into the ggplot() function. These extra parameters are the default parameters for the function.\n\n\n\nIf geoms is the geometry of the shapes in the plot, then stats is the statistic or mathematical functions that create the geoms. In the above case, the vertical bars in geom_bar are created by counting the number of observations in each bin. The stat_count function is responsible for this calculation, and it is called by default when we use the geom_bar function. Specifically, stat_count counts the number of observations in each histogram bin.\nMore generally, we can change the geometry of the plotted shapes with geom_&lt;NAME&gt;, and we can define different functions for generating the geometric shapes with stat_&lt;NAME&gt;. To make things easier on us, there is a default stat for each geom. In most cases we can just focus on which geometry we want for our graph, and use the default stat.\nFor more information on the default parameters and stat of geom_bar() or any other geom, use the R help function.\n\n?geom_bar\n\n\n\n\nLet’s explore a few more plotting options to get a better feel for our plotting parameters. Use summary() on each graph and compare it to the summary() output we examined earlier. Here we’ll use the random normal values we generated above so that we can make a bivariate plot:\n\nBivPlot&lt;-ggplot(data=SelData, aes(x=s, y=Rpoint)) + \n  geom_point()\n\nFirst, take a quick look at the summary() of the plotting function and compare to the earlier graph (data not shown).\n\nsummary(BivPlot)\n\nNow plot the graph:\n\nprint(BivPlot)\n\n\n\n\n\n\n\n\nNotice how the points are all clustered to the left. This looks like a classic log-normal variable, so let’s log-transform \\(s\\) (x-axis)\n\nBivPlot&lt;-ggplot(aes(x=log(s+1), y=Rpoint), data=SelData) + \n  geom_point()\nprint(BivPlot)\n\n\n\n\n\n\n\n\nOnce again, compare the summary with the untransformed x-axis.\n\nQuestion: What does the summary() show as the difference for a raw vs log-tranformed x-axis in the aes() function inside of ggplot()\n\n\n\n\nA really handy feature of ggplot is the geom_smooth function, which has several options for calculating and plotting a statistical model to the observations.\nHere’s a simple linear regression slope (lm = linear model):\n\nBivPlot + \n  geom_smooth(method=\"lm\", colour=\"steelblue\",\n              formula = y~x, linewidth=2)\n\n\n\n\n\n\n\n\nWe can use a grouping variable to add separate regression lines for each group.\n\nBivPlot + \n  geom_smooth(aes(colour=Native),linewidth=2,\n              method=\"lm\", formula=y~x)\n\n\n\n\n\n\n\n\n\n\n\n\nNow that we have a better understanding of ggplot(), let’s try to recreate the selection histograms in Figure 2 of Colautti & Lau (2015). This will involve three main steps:\n\nSeparate data for native vs. introduced species into two data sets for custom plotting.\nUse a bootstrap model to estimate non-parametric mean and 95% confidence intervals for each group.\nPlot all of the components on a single graph\n\nOne technical note. We are going to deviate from the published code slightly. The published figure uses frequency data, whereas we are going to use density data.\n\n\nSince this is a relatively simple resampling model, we’ll use two separate vectors to store data for our plots and boostrap sampling: one for native and one for non-native.\n\nNatSVals&lt;-SelData$s[SelData$Native==\"yes\"] \nIntSVals&lt;-SelData$s[SelData$Native==\"no\"] \n\nAn alternative would be to set up a data frame and keep track of values as separate columns, with a different row for each iteration.\n\n\n\n\nThe graph includes a bootstrap model to estimate the mean and variance for each group (native = \"yes\" vs \"no\"). A bootstrap is just a computational approach to generating confidence limits on the sample. It makes no assumptions about the underlying distribution of the population from which the sample is drawn, so it is a robust method for non-parametric data. In our case, we will randomly sample from each vector, and calculate the mean. We repeat this many times to get a range of values from which we can estimate the confidence interval.\nThe example below is not the most efficient approach but it is a good opportunity to practice our for(){} loops from the Flow Control section in the R Fundamentals Chapter.\n\n\nFirst we define the number of iterations and set up two objects to hold the data from our bootstrap iterations.\n\nIterN&lt;-100 # Number of iterations\nNatSims&lt;-{} # Dummy objects to hold output\nIntSims&lt;-{}\n\n\n\n\nWe will use the for loop to resample the data, calcuate the sample mean, and repeat N times. This involves just three key steps.\n\nSample, with replacement.\nCalculate the average.\nSave the average in a vector: NatSims for native species or IntSims for non-native species.\n\n\nfor (i in 1:IterN){\n  NatSims[i]&lt;-mean(sample(\n    NatSVals, length(NatSVals), replace=T))\n  IntSims[i]&lt;-mean(sample(\n    IntSVals, length(IntSVals), replace=T))\n}\n\nNote in the above code we use ‘nested’ functions. The sample() function is nested inside the the mean() function, which is faster than using nested loop.\nAlso note that we can include both datasets (native + non-native) in the same for loop.\n\n\n\nNon-parametric Confidence Intervals (CI) are calculated directly from the bootstrap output. Let’s try finding our 95% CI range, which goes from the lower 2.5% to the upper 97.5% of values.\nFirst, sort the datea from low to high\n\nNatSims&lt;-sort(NatSims)\nIntSims&lt;-sort(IntSims)\n\nEach of the output vectors contains a number of values equal to our Iter variable, as defined earlier in our code. Now we identify the lower 2.5% and upper 97.5% values in each vector. For example, with 1000 iterations our 2.5% would be the 25th value in the sorted vector and the upper 97.5% would be the 975th value in the sorted vector.\nWe use this number to index the vector with square brackets. We make sure to round to a whole number since we can’t have a fractional cell position.\n\nCIs&lt;-c(NatSims[round(IterN*0.025,0)], # Lower 2.5%\n       NatSims[round(IterN*0.975,0)], # Upper 97.5%\n       IntSims[round(IterN*0.025,0)], # Lower 2.5%\n       IntSims[round(IterN*0.975,0)]) # Upper 97.5%\n\nThe output (CIs) as is therefore a vector of four elements.\n\nprint(CIs)\n\n[1] 0.1820612 0.1998140 0.2259135 0.3058595\n\n\nNote: your numbers should be similar but won’t be exact because you won’t have the exact same random sample when you run your code.\n\nQuestion: What line of code could we add above to ensure that these numbers were exactly the same for everyone who ran this code?\n\nAnswer: If you aren’t sure, then it’s a good time to review the R Fundamentals Chapter.\n\n\n\n\nWe’ll set up a new data frame object for plotting data, to make it easier to write our plotting functions.\n\nHistData&lt;-data.frame(s=SelData$s,Native=SelData$Native)\n\nand set up a minimal ggplot code:\n\np &lt;- ggplot()\n\nNow we can add layers to the plot. We’ll print out each layer as we go, so that we can see what each layer adds to the overall graph. The coding is a bit complex here, so don’t worry if it’s hard to follow everything. The key thing to understand is how the different geoms contribute to the final plot.\n\np &lt;- p + geom_density(aes(x=s),\n                      data=HistData[HistData$Native==\"yes\",],\n                 colour=\"#1fcebd66\", size=2)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\nprint(p)\n\n\n\n\n\n\n\n\nHere, we’ve added a geom_density geometry, which gives a smoothed line, like we saw in the Quick Visualizations Chapter. This works well for large datasets with many bins. For example, compare this graph with the geom_box() graph that we did on this data earlier in this chapter.\nNote that the y-axis goes above 1 because the total probability is the area under the curve, which must equal one. Since there are many values that are less than one, the density must have values larger than 1 in order for the area to equal 1.\nNext, we can add a similar graph for the non-native species, with a contrasting colour.\n\np &lt;- p + geom_density(aes(x=s),\n                      data=HistData[HistData$Native==\"no\",],\n                 colour=\"#f5375166\", size=2)\nprint(p)\n\n\n\n\n\n\n\n\nAlternatively, we could use a single geom with group= and colour=. However, using separate geom layers makes it easier to specify colours. This is yet another example of how different programming approaches can yield the same output, and one approach is not necessarily superior to the other.\nNext, we add our CI data. CI is a range of values for each group, which we can represent as a rectangle.\n\np &lt;- p + geom_rect(aes(xmin=CIs[1],xmax=CIs[2],ymin=0,ymax=0.25),\n                   fill=\"#1fcebd88\")\nprint(p) # native species 95% CI bar\n\n\n\n\n\n\n\n\nWith geom_rect we define 4 points of the rectangle. The x-axes coordinates are the CI values from our bootstrap algorithm, and the y-axes values are arbitrary numbers that determine the height of the rectangle.\nNow try the same thing for the non-native species.\n\np &lt;- p + geom_rect(aes(xmin=CIs[3],xmax=CIs[4],ymin=0,ymax=0.25),\n                   fill=\"#f5375188\")\nprint(p) # introduced species 95% CI bar\n\n\n\n\n\n\n\n\nNow we add the bootstrap mean for each group. The bootstrap mean is just the mean of the bootstrap iterations – that is, a mean of means.\n\np &lt;- p + geom_line(aes(x=mean(NatSims),y=c(0,0.25)),\n                   colour=\"#1d76bf\",size=1)\np &lt;- p + geom_line(aes(x=mean(IntSims),y=c(0,0.25)),\n                   colour=\"#f53751\",size=1)\nprint(p)\n\n\n\n\n\n\n\n\nFinally, we tweak the axis titles and zoom in along the x-axis to make it easier to see the differences between the two distributions. Note how the y-axis label changed when we added geom_line.\n\np &lt;- p + ylab(\"Density\") + \n  scale_x_continuous(limits = c(0, 1.5))\nprint(p) # labels added, truncated x-axis\n\nWarning: Removed 12 rows containing non-finite values (`stat_density()`).\n\n\nWarning: Removed 1 rows containing non-finite values (`stat_density()`).\n\n\n\n\n\n\n\n\n\nNote the warnings about missing data, which seems to be due to the fact that we zoomed in on the x-axis. Despite the warnings, everything looks okay if we compare this graph to the previous one.\nThere we have it! We succesffuly created a complex plot overlapping layers by breaking down ggplot() into individual components."
  },
  {
    "objectID": "advanced_vis.html#introduction",
    "href": "advanced_vis.html#introduction",
    "title": "Advanced Visualizations",
    "section": "",
    "text": "Before continuing with this tutorial/chapter, you should be familiar with the basics of ggplot from the previous chapters, and you should have lots of practice making graphs with different formatting options.\nIn this chapter, we look at some important rules of thumb for making professional and effective visualizations. Then, we will work through a detailed example with more advanced options for visualizations with the ggplot function. This includes everything you will need to make professional-grade figures.\nThe ggplot cheat sheet may be useful for you, both for this chapter and in the future when making your own figures. A cheat sheet is a printable file that provides a good summary and quick-reference guide for a particular function or activity. In this case, the ggplot cheat sheet provides a quick summary of a lot of the main graph types and parameters in the ggplot2 library. It can be found along with other useful cheat sheets on the Posit website: https://posit.co/resources/cheatsheets/"
  },
  {
    "objectID": "advanced_vis.html#getting-started",
    "href": "advanced_vis.html#getting-started",
    "title": "Advanced Visualizations",
    "section": "",
    "text": "First, we’ll load the ggplot2 library and set a custom plotting theme as described in the previous chapter.\n\nlibrary(ggplot2)\nsource(\"http://bit.ly/theme_pub\")\ntheme_set(theme_pub())\n\nThe source function loads an external file, in this case from the internet. The file is just a script saved as .R file with a custom function defining different aspects of the graph (e.g. text size, line width, etc.) You can open the link in a web browser or download and open in a text editor to see the script\nThe theme_set() function sets our custom theme (theme_pub) as the default plotting theme. Since the theme is a function in R, we need to include the brackets, even though twe don’t want to change anything in the function: theme_pub()"
  },
  {
    "objectID": "advanced_vis.html#rules-of-thumb",
    "href": "advanced_vis.html#rules-of-thumb",
    "title": "Advanced Visualizations",
    "section": "",
    "text": "Before we dig into the code, it’s worth reviewing some more general graphical concepts. Standards of practice for published graphs in professional journals can vary depending on format (e.g. print vs online), audience, and historical precedent. Nevertheless, there are a number of useful ‘rules of thumb’ to keep in mind. These are not hard and fast rules but helpful for new researchers who aren’t sure how or where to start. In making decisions, always think of your audience and remember that the main goal is to communicate information as clearly and efficientl as possible.\n1. Minimize ‘ink’\nIn the old days, when most papers were actually printed and mailed to journal subscribers, black ink was expensive and printing in colour was very expensive. Printing is still expensive but of course most research articles are available online where there is no additional cost for colour or extra ink. However, the concept of minimizing ink (or pixels) can go a long way toward keeping a graph free from clutter and unnecessary distraction.\n2. Use space wisely\nEmpty space is not necessarily bad, but ask yourself if it is necessary and what you want the reader to take away. Consider the next two graphs:\n\n\nWarning: package 'dplyr' was built under R version 4.3.2\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the first example, the Y-axis is scaled to the data. In the second case, Y-axis scaled between 0 and 100.\n\nQuestion: What are the benefits/drawbacks of scaling the axes? When might you choose to use one over the other?\n\n3. Choose a colour palette\nColour has three basic components:\n\nHue – the relative proportion of red vs green vs blue light\nSaturation – how vivid the colour is\nBrightness – the amount of white (vs black) in the colour\n\nThe abbreviation HSB is often used, or HSL (L = ‘Lightness’) or HSV (V = ‘Value’).\nIn R these can be easily defined with the rgb() function. For example:\n\nrgb(1,0,0) – a saturated red\nrgb(0.1,0,0) – a dark red (low brightness, low saturation)\nrgb(1,0.9,0.9) – a light red (high brightness, low saturation)\n\nDon’t underestimate the impact of choosing a good colour palette, especially for presentations. Colour theory can get a bit overwhelming but here are a few good websites to help:\n\nQuickly generate your own palette using Coolors: https://coolors.co\nUse a colour wheel to find complementary colours using Adobe: https://color.adobe.com/create\n\n4. Colours have meaning\nTry running this code and veiw the output in colour:\n\nX&lt;-rnorm(100)\nY&lt;-X+seq_along(X)\nD&lt;-data.frame(Temperature=Y,Location=X,Temp=Y/3)\nqplot(Location, Temperature,colour=Temp, data=D) + \n  scale_color_gradient(high=\"blue\", low=\"red\")\n\n\nQuestion: What strikes you as odd about this graph (not shown)?\n\nTechnically, there is nothing wrong. But we naturally associate colours with particular feelings. In this case, intuitively we associate red with hot and blue with cold, which is the opposite of what is shown in this graph. Be mindful of these associations when choosing a colour palette.\nAnother important consideration is that not everyone sees colour the same way. About 5% to 10% of the population has colour blindness. In order to make colour graphs readable to everyone, you should make sure to use colours that can still be interpreted when printed in greyscale, as explained in the Quick Visualizations Chapter.\n5. Maximize contrast\nColours that are too similar will be hard to distinguish.\n\n\n\n\n\n\n\n\n\nCan you see the gradient of colours? The difference among colours is called contrast, and generally a high-contrast palette is more informative than a low-contrast palette. Here is a plot of the same data, plotted with a wider range of colours:\n\nggplot(aes(Latitude,Longitude,colour=Precip), data=D) + \n  geom_point() +\n  scale_color_gradient(high=\"cyan\", low=\"red\")\n\n\n\n\n\n\n\n\n6. Keep relevant information\nMake sure to include proper axis labels (i.e. names) and tick marks (i.e. numbers or categories showing the different values). These labels, along with the figure caption, should act as a stand-alone unit. The reader should be able to understand the figure without having to read through the rest of the paper.\n7. Choose the right graph\nOften the same data can be presented in different ways but some are easier to interpret than others. Think carefully about the story you want to present and the main ideas you want your reader to get from your figures. Look at these two graphs that show the same data.\n\nADat&lt;-data.frame(Biomass=rnorm(100), Treatment=\"Treatment A\")\nBDat&lt;-data.frame(Biomass=5 + rnorm(100) *5 + \n                   ADat$Biomass * 5, Treatment=\"Treatment B\")\nPDat&lt;-rbind(ADat,BDat)\nggplot(aes(Biomass, fill=Treatment), data=PDat)+\n  geom_histogram(posit=\"dodge\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nThe first graph tells a story about the distributions – the mean and variance of each treatment\n\nggplot() + geom_point(aes(ADat$Biomass,BDat$Biomass)) +\n  xlab(\"Biomass in Treatment A\") + ylab (\"Biomass in Treatment B\")\n\n\n\n\n\n\n\n\nThe second graph tells a story about the correlated relationship between Treatment A and Treatment B.\nOne graph is not necessarily better than the other. It depends on the story you want to tell."
  },
  {
    "objectID": "advanced_vis.html#example",
    "href": "advanced_vis.html#example",
    "title": "Advanced Visualizations",
    "section": "",
    "text": "Now that we have gone over some basic graphing concepts, let’s look at how to build a professional-grade figure. In fact, we’ll reconstruct a figure published in a paper by Colautti & Lau in the journal Molecular Ecology (2015): https://doi.org/10.1111/mec.13162\n\n\nThe paper is a meta-analysis and review of evolution occurring during biological invasions. We will recreate Figure 2, which shows the result of a meta-analysis of selection gradients (\\(\\beta\\)) and selection differentials (\\(s\\)). First, we’ll just recreate the top panel, and then we’ll look at ways to make more advanced multi-panel graphs like this.\nThe data from the paper are archived on Dryad: https://datadryad.org/stash/dataset/doi:10.5061/dryad.gt678\nYou could download the zip file and look for the file called Selection_Data.csv and save it to your working directory. But I have also put it on Github, so that you can download it directly to R:\n\nSelData&lt;-read.csv(\n  \"https://colauttilab.github.io/RCrashCourse/Selection_Data.csv\")\n\nWe are also going to change the column names in the file to make them a bit more intuitive and easier to work with in R.\n\nnames(SelData)&lt;-c(\"Collector\", \"Author\", \"Year\", \"Journal\", \n                  \"Vol\", \"Species\", \"Native\", \"N\", \n                  \"Fitness.measure\", \"Trait\", \"s\", \n                  \"s.SE\", \"s.P\", \"B\", \"B.SE\", \"B.P\")\n\n\n\n\nLet’s take a quick look at the data\n\nhead(SelData)\n\n          Collector               Author Year             Journal\n1 KingsolverDiamond Alatalo and Lundberg 1986           Evolution\n2 KingsolverDiamond Alatalo and Lundberg 1986           Evolution\n3 KingsolverDiamond Alatalo and Lundberg 1986           Evolution\n4 KingsolverDiamond       Alatalo et al. 1990 American Naturalist\n5 KingsolverDiamond       Alatalo et al. 1990 American Naturalist\n6 KingsolverDiamond       Alatalo et al. 1990 American Naturalist\n             Vol             Species Native    N       Fitness.measure\n1     40:574-583  Ficedula hypoleuca    yes  641   male mating success\n2     40:574-583  Ficedula hypoleuca    yes  713 female mating success\n3     40:574-583  Ficedula hypoleuca    yes 1705              survival\n4 135(3):464-471 Ficedula albicollis    yes &lt;NA&gt;              survival\n5 135(3):464-471 Ficedula albicollis    yes &lt;NA&gt;              survival\n6 135(3):464-471 Ficedula albicollis    yes &lt;NA&gt;              survival\n          Trait     s s.SE s.P     B B.SE B.P\n1 tarsus length -0.01       ns    NA         \n2 tarsus length  0.01      sig    NA         \n3 tarsus length  0.04       ns    NA         \n4  tarus length  0.02       ns -0.06         \n5  tarus length  0.08       ns -0.01         \n6  tarus length  0.19      sig  0.01         \n\n\nIt’s worth taking some time to look at this to understand how to encode data for a meta-analysis. The collector column indicates the paper that the data came from. The Author indicates the author(s) of the original paper that reported the data. The Year, Journal, and Vol give information about the publication that the data came from originally.\nWe can see above the collector KingsolverDiamond, which represents a paper from Kingsolver and Diamond that was itself a meta-analysis of natural selection. Most of the studies came from this meta-analysis, but a few of the more recent papers were added by grad students, denoted by initials:\n\nunique(SelData$Collector)\n\n[1] \"KingsolverDiamond\" \"JAL\"               \"DJW\"              \n[4] \"CPT\"              \n\n\nSpecies is the study species, and Native is its status as a binary yes/no variable. N is the sample size and Fitness.measure is the specific trait that defines fitness. Trait is the trait on which selection was measured. Finally, \\(s\\) is the selection differential and \\(\\beta\\) is the selection gradient. Note that these are slopes in units of relative fitness per trait standard deviation. This is explained in more detail below.\n\n\n\nIn this analysis, we are interested in the magnitude but not the direction of natural selection. In other words we would want to treat a slope of -4 the same as a slope of +4 because they have the same magnitude. Therefore, we can replace the \\(s\\) column with \\(|s|\\)\n\nSelData$s&lt;-abs(SelData$s)\n\nWe’ll also add a couple of columns with random variables that we can use later to explore additional plotting options.\nFirst, a column of values sampled from a z-distribution – this is a Gaussian (a.k.a. ‘normal’) distribution with mean = 0 and sd = 1.\n\nSelData$Rpoint&lt;-rnorm(nrow(SelData))\n\nSecond, a columnn of 1 and 0 sampled randomly with equal frequency (\\(p = 0.5\\))\n\nSelData$Rgroup&lt;-sample(c(0,1), nrow(SelData), replace=T)\n\n\nQuestion: Do you remember rnorm() and sample() from the R Fundamentals Chapter?\n\nIf not, it may be a good time for a quick review. Remember to keep practicing – recognizing code is not the same as being able to write it from scratch.\n\n\n\nBefore we plot the selection data, we should take a quick look at the values to check for potential errors.\n\nprint(SelData$s)\n\nThe output isn’t shown here, but note that NA is used to denote missing data in the output.\nWe can subset to remove missing data:\n\nSelData&lt;-SelData[!is.na(SelData$s),]\n\nRecall from the R Fundamentals Chapter that ! means ‘not’ or ‘invert’\nThere is also a convenient drop_na function in the tidyr package:\n\nlibrary(tidyr)\nSelData&lt;-SelData %&gt;%\n  drop_na(s)"
  },
  {
    "objectID": "advanced_vis.html#measuring-selection",
    "href": "advanced_vis.html#measuring-selection",
    "title": "Advanced Visualizations",
    "section": "",
    "text": "We’re going to get a bit technical here. Don’t worry if you don’t completely understand all of the stuff below about measuring selection. Keep it for reference in case you decide you want to use it for your own research. For now, just try to understand it as well as you can and focus on the code used to produce the figures.\nAn analysis of phenotypic selection was proposed by Lande & Arnold (1983) as a simple but powerful tool for measuring natural selection. It is just a linear model with relative fitness on the y-axis and the standardized trait value(s) on the x-axis.\n\n\n\nFitness can be measured in many ways, such as survival or lifetime seed or egg production. Check out the list of specific fitness measures used in these studies:\n\nunique(SelData$Fitness.measure)\n\n(output not shown)\nAbsolute fitness is just the observed value (e.g. seed set or survival yes/no). Technicallly, we call these fitness components because they are not fitness per se, but they represent measurements of survival and reproduction, which are the key components that jointly determine fitness.\nRelative fitness is just the absolute fitness divided by the mean. Absolute fitness is usually denoted by the capital letter \\(W\\) and relative fitness is usually represented by a lower-case \\(w\\) or omega \\(\\omega\\). Expressing this in mathematical terms:\n\\[ \\omega = W_i/\\bar W \\]\nwhere \\(W_i\\) is the mean of the study sample.\n\n\n\nA Trait Value is just the measured trait on which selection may act. Use unique(SelData$Trait) to see the list of specific traits that were measured in these studies. The Standardized Trait Value is the traits z-score. See the Distributions Chapter in the book R STATS Crash Course for more information about z-scores. To calculate the z-score, we take each value, subtract the mean, and then divide by the standard deviation of the sample (\\(sd\\)):\n\\[\\frac{x_i-\\bar X}{sd}\\]\nSince traits have different metrics, they are hard to compare: e.g. days to flower, egg biomass, foraging intensity, aggression. But standardizing traits to z-scores puts them all on the same scale for comparison. Specifically, the scale of selection will be in standard deviations from the mean.\n\n\n\nSelection differentials (\\(s\\)) and selection gradients (\\(\\beta\\)) measure selection using linear models but represent slightly different measurements. Linear models are covered in the Linear Models Chapter in the book R STATS Crash Course.\nBoth models use relative fitness (\\(\\omega\\)) as the response variable.\nSelection differentials (\\(s\\)) measure selection on only a single trait, ignoring all other traits. In theory, the response to selection is a simple function of the genetic correlation between a trait and fitness.\nFitness differences among individuals can depend on a lot of things – genetic variation for the trait itself, but also environmental effects on the trait as well as effects on other traits that are under selection and correlated with the trait of interest.\nSelection gradients (\\(\\beta\\)) measure selection on a trait of interest while also accounting for selection on other correlated traits. This is done via a multiple regression – a linear model with multiple predictors.\nNow that we have reviewed the relevant biological background, we can plot \\(s\\) and \\(\\beta\\) to compare their distributions."
  },
  {
    "objectID": "advanced_vis.html#distribution-plots",
    "href": "advanced_vis.html#distribution-plots",
    "title": "Advanced Visualizations",
    "section": "",
    "text": "Let’s start with a simple ggplot function. We are going to be adding layers to build up to more complex graphs, so we’ll start by creating a base plotting object to build on.\n\nBarPlot&lt;-ggplot(aes(x=s, fill=Native), data=SelData)\n\n\n\nRecall from the Basic Visualizations Chapter, the use of aesthetic function aes(). This defines the data that we want to use for our ggplot graph. We will see how we do this by adding layers to our plot, similar to the way old-timey cartoons were made by layering multiple clear pages of cellophane with characters painted on them. The aes function inside of the ggplot function defines that data that will be shared among all of the layers. In addition, we can have separate aes functions inside different geom_ layers that define and restrict the plotting data to that specific layer.\nLet’s look at the ggplot object so far:\n\nprint(BarPlot)\n\n\n\n\n\n\n\n\nNo data!\n\nQuestion: Why are there no data plotted?\n\nAnswer: We didn’t define a geom_ for the data yet.\n\n\n\nSo far, we have only loaded in the data for plotting. We have to specify which geom(s) we want. We’ll start with a bar plot:\n\nBarPlot&lt;- BarPlot + geom_bar() \nBarPlot\n\n\n\n\n\n\n\n\nLet’s explore the components of our Bar Plot object.\n\nsummary(BarPlot)\n\ndata: Collector, Author, Year, Journal, Vol, Species, Native, N,\n  Fitness.measure, Trait, s, s.SE, s.P, B, B.SE, B.P, Rpoint, Rgroup\n  [2766x18]\nmapping:  x = ~s, fill = ~Native\nfaceting: &lt;ggproto object: Class FacetNull, Facet, gg&gt;\n    compute_layout: function\n    draw_back: function\n    draw_front: function\n    draw_labels: function\n    draw_panels: function\n    finish_data: function\n    init_scales: function\n    map_data: function\n    params: list\n    setup_data: function\n    setup_params: function\n    shrink: TRUE\n    train_scales: function\n    vars: function\n    super:  &lt;ggproto object: Class FacetNull, Facet, gg&gt;\n-----------------------------------\ngeom_bar: just = 0.5, width = NULL, na.rm = FALSE, orientation = NA\nstat_count: width = NULL, na.rm = FALSE, orientation = NA\nposition_stack \n\n\nIt’s worth taking some time to understand the structure of this ggplot object how it relates to what gets plotted to the output.\n\nData shows which data are available for plotting. These are just the column names of the data.frame object we input with the data= parameter in the ggplot() function.\nMapping shows the variables from the aes() function, the scaling of the x-axis and the variable for the fill= colours.\nFaceting contains information for multiple plots. We’ll explore this more later, but a single graph has a facet_null() function.\nDashed line separates the ggplot() function from the other functions linked with the plus (+) in our plotting function. In our graph, there was + geom_bar(), which we can see below the dashed line.\ngeom_bar shows the (default) parameters used in our function\nstat_count shows our stat function. This was created by default with our + geom_bar() function; it determines how the data are transformed to geometric shapes for plotting (e.g., points, lines, or bars).\n\nThe output also shows some of the functions and parameters used to generate the graph. At the bottom we see parameters for geom_bar and stat_count. Note that there are more parameters listed than what we explicitly put into the ggplot() function. These extra parameters are the default parameters for the function.\n\n\n\nIf geoms is the geometry of the shapes in the plot, then stats is the statistic or mathematical functions that create the geoms. In the above case, the vertical bars in geom_bar are created by counting the number of observations in each bin. The stat_count function is responsible for this calculation, and it is called by default when we use the geom_bar function. Specifically, stat_count counts the number of observations in each histogram bin.\nMore generally, we can change the geometry of the plotted shapes with geom_&lt;NAME&gt;, and we can define different functions for generating the geometric shapes with stat_&lt;NAME&gt;. To make things easier on us, there is a default stat for each geom. In most cases we can just focus on which geometry we want for our graph, and use the default stat.\nFor more information on the default parameters and stat of geom_bar() or any other geom, use the R help function.\n\n?geom_bar\n\n\n\n\nLet’s explore a few more plotting options to get a better feel for our plotting parameters. Use summary() on each graph and compare it to the summary() output we examined earlier. Here we’ll use the random normal values we generated above so that we can make a bivariate plot:\n\nBivPlot&lt;-ggplot(data=SelData, aes(x=s, y=Rpoint)) + \n  geom_point()\n\nFirst, take a quick look at the summary() of the plotting function and compare to the earlier graph (data not shown).\n\nsummary(BivPlot)\n\nNow plot the graph:\n\nprint(BivPlot)\n\n\n\n\n\n\n\n\nNotice how the points are all clustered to the left. This looks like a classic log-normal variable, so let’s log-transform \\(s\\) (x-axis)\n\nBivPlot&lt;-ggplot(aes(x=log(s+1), y=Rpoint), data=SelData) + \n  geom_point()\nprint(BivPlot)\n\n\n\n\n\n\n\n\nOnce again, compare the summary with the untransformed x-axis.\n\nQuestion: What does the summary() show as the difference for a raw vs log-tranformed x-axis in the aes() function inside of ggplot()\n\n\n\n\nA really handy feature of ggplot is the geom_smooth function, which has several options for calculating and plotting a statistical model to the observations.\nHere’s a simple linear regression slope (lm = linear model):\n\nBivPlot + \n  geom_smooth(method=\"lm\", colour=\"steelblue\",\n              formula = y~x, linewidth=2)\n\n\n\n\n\n\n\n\nWe can use a grouping variable to add separate regression lines for each group.\n\nBivPlot + \n  geom_smooth(aes(colour=Native),linewidth=2,\n              method=\"lm\", formula=y~x)"
  },
  {
    "objectID": "advanced_vis.html#full-ggplot",
    "href": "advanced_vis.html#full-ggplot",
    "title": "Advanced Visualizations",
    "section": "",
    "text": "Now that we have a better understanding of ggplot(), let’s try to recreate the selection histograms in Figure 2 of Colautti & Lau (2015). This will involve three main steps:\n\nSeparate data for native vs. introduced species into two data sets for custom plotting.\nUse a bootstrap model to estimate non-parametric mean and 95% confidence intervals for each group.\nPlot all of the components on a single graph\n\nOne technical note. We are going to deviate from the published code slightly. The published figure uses frequency data, whereas we are going to use density data.\n\n\nSince this is a relatively simple resampling model, we’ll use two separate vectors to store data for our plots and boostrap sampling: one for native and one for non-native.\n\nNatSVals&lt;-SelData$s[SelData$Native==\"yes\"] \nIntSVals&lt;-SelData$s[SelData$Native==\"no\"] \n\nAn alternative would be to set up a data frame and keep track of values as separate columns, with a different row for each iteration."
  },
  {
    "objectID": "advanced_vis.html#bootstrap",
    "href": "advanced_vis.html#bootstrap",
    "title": "Advanced Visualizations",
    "section": "",
    "text": "The graph includes a bootstrap model to estimate the mean and variance for each group (native = \"yes\" vs \"no\"). A bootstrap is just a computational approach to generating confidence limits on the sample. It makes no assumptions about the underlying distribution of the population from which the sample is drawn, so it is a robust method for non-parametric data. In our case, we will randomly sample from each vector, and calculate the mean. We repeat this many times to get a range of values from which we can estimate the confidence interval.\nThe example below is not the most efficient approach but it is a good opportunity to practice our for(){} loops from the Flow Control section in the R Fundamentals Chapter.\n\n\nFirst we define the number of iterations and set up two objects to hold the data from our bootstrap iterations.\n\nIterN&lt;-100 # Number of iterations\nNatSims&lt;-{} # Dummy objects to hold output\nIntSims&lt;-{}\n\n\n\n\nWe will use the for loop to resample the data, calcuate the sample mean, and repeat N times. This involves just three key steps.\n\nSample, with replacement.\nCalculate the average.\nSave the average in a vector: NatSims for native species or IntSims for non-native species.\n\n\nfor (i in 1:IterN){\n  NatSims[i]&lt;-mean(sample(\n    NatSVals, length(NatSVals), replace=T))\n  IntSims[i]&lt;-mean(sample(\n    IntSVals, length(IntSVals), replace=T))\n}\n\nNote in the above code we use ‘nested’ functions. The sample() function is nested inside the the mean() function, which is faster than using nested loop.\nAlso note that we can include both datasets (native + non-native) in the same for loop.\n\n\n\nNon-parametric Confidence Intervals (CI) are calculated directly from the bootstrap output. Let’s try finding our 95% CI range, which goes from the lower 2.5% to the upper 97.5% of values.\nFirst, sort the datea from low to high\n\nNatSims&lt;-sort(NatSims)\nIntSims&lt;-sort(IntSims)\n\nEach of the output vectors contains a number of values equal to our Iter variable, as defined earlier in our code. Now we identify the lower 2.5% and upper 97.5% values in each vector. For example, with 1000 iterations our 2.5% would be the 25th value in the sorted vector and the upper 97.5% would be the 975th value in the sorted vector.\nWe use this number to index the vector with square brackets. We make sure to round to a whole number since we can’t have a fractional cell position.\n\nCIs&lt;-c(NatSims[round(IterN*0.025,0)], # Lower 2.5%\n       NatSims[round(IterN*0.975,0)], # Upper 97.5%\n       IntSims[round(IterN*0.025,0)], # Lower 2.5%\n       IntSims[round(IterN*0.975,0)]) # Upper 97.5%\n\nThe output (CIs) as is therefore a vector of four elements.\n\nprint(CIs)\n\n[1] 0.1820612 0.1998140 0.2259135 0.3058595\n\n\nNote: your numbers should be similar but won’t be exact because you won’t have the exact same random sample when you run your code.\n\nQuestion: What line of code could we add above to ensure that these numbers were exactly the same for everyone who ran this code?\n\nAnswer: If you aren’t sure, then it’s a good time to review the R Fundamentals Chapter."
  },
  {
    "objectID": "advanced_vis.html#plot-data",
    "href": "advanced_vis.html#plot-data",
    "title": "Advanced Visualizations",
    "section": "",
    "text": "We’ll set up a new data frame object for plotting data, to make it easier to write our plotting functions.\n\nHistData&lt;-data.frame(s=SelData$s,Native=SelData$Native)\n\nand set up a minimal ggplot code:\n\np &lt;- ggplot()\n\nNow we can add layers to the plot. We’ll print out each layer as we go, so that we can see what each layer adds to the overall graph. The coding is a bit complex here, so don’t worry if it’s hard to follow everything. The key thing to understand is how the different geoms contribute to the final plot.\n\np &lt;- p + geom_density(aes(x=s),\n                      data=HistData[HistData$Native==\"yes\",],\n                 colour=\"#1fcebd66\", size=2)\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\nprint(p)\n\n\n\n\n\n\n\n\nHere, we’ve added a geom_density geometry, which gives a smoothed line, like we saw in the Quick Visualizations Chapter. This works well for large datasets with many bins. For example, compare this graph with the geom_box() graph that we did on this data earlier in this chapter.\nNote that the y-axis goes above 1 because the total probability is the area under the curve, which must equal one. Since there are many values that are less than one, the density must have values larger than 1 in order for the area to equal 1.\nNext, we can add a similar graph for the non-native species, with a contrasting colour.\n\np &lt;- p + geom_density(aes(x=s),\n                      data=HistData[HistData$Native==\"no\",],\n                 colour=\"#f5375166\", size=2)\nprint(p)\n\n\n\n\n\n\n\n\nAlternatively, we could use a single geom with group= and colour=. However, using separate geom layers makes it easier to specify colours. This is yet another example of how different programming approaches can yield the same output, and one approach is not necessarily superior to the other.\nNext, we add our CI data. CI is a range of values for each group, which we can represent as a rectangle.\n\np &lt;- p + geom_rect(aes(xmin=CIs[1],xmax=CIs[2],ymin=0,ymax=0.25),\n                   fill=\"#1fcebd88\")\nprint(p) # native species 95% CI bar\n\n\n\n\n\n\n\n\nWith geom_rect we define 4 points of the rectangle. The x-axes coordinates are the CI values from our bootstrap algorithm, and the y-axes values are arbitrary numbers that determine the height of the rectangle.\nNow try the same thing for the non-native species.\n\np &lt;- p + geom_rect(aes(xmin=CIs[3],xmax=CIs[4],ymin=0,ymax=0.25),\n                   fill=\"#f5375188\")\nprint(p) # introduced species 95% CI bar\n\n\n\n\n\n\n\n\nNow we add the bootstrap mean for each group. The bootstrap mean is just the mean of the bootstrap iterations – that is, a mean of means.\n\np &lt;- p + geom_line(aes(x=mean(NatSims),y=c(0,0.25)),\n                   colour=\"#1d76bf\",size=1)\np &lt;- p + geom_line(aes(x=mean(IntSims),y=c(0,0.25)),\n                   colour=\"#f53751\",size=1)\nprint(p)\n\n\n\n\n\n\n\n\nFinally, we tweak the axis titles and zoom in along the x-axis to make it easier to see the differences between the two distributions. Note how the y-axis label changed when we added geom_line.\n\np &lt;- p + ylab(\"Density\") + \n  scale_x_continuous(limits = c(0, 1.5))\nprint(p) # labels added, truncated x-axis\n\nWarning: Removed 12 rows containing non-finite values (`stat_density()`).\n\n\nWarning: Removed 1 rows containing non-finite values (`stat_density()`).\n\n\n\n\n\n\n\n\n\nNote the warnings about missing data, which seems to be due to the fact that we zoomed in on the x-axis. Despite the warnings, everything looks okay if we compare this graph to the previous one.\nThere we have it! We succesffuly created a complex plot overlapping layers by breaking down ggplot() into individual components."
  },
  {
    "objectID": "advanced.html",
    "href": "advanced.html",
    "title": "Advanced Coding",
    "section": "",
    "text": "Advanced Coding\nThis section contains more advanced concepts and techniques in R. The first chapter introduces tools for working with data sets in R to produce reproducible workflows. The second and third chapters are useful for simplifying your code when you have iterative (i.e., repetitive) code. We end with regular expressions, which is probably the least intuitive and most challenging section of the book. There is a steep learning curve, but it is an invaluable tool for large and complex data sets."
  },
  {
    "objectID": "basics.html",
    "href": "basics.html",
    "title": "Fundamentals",
    "section": "",
    "text": "Fundamentals\nThis section contains everything you need to get started in R for reproducible reports with embedded code and visualizations. There are only three chapters here, but these form the foundation for all the future chapters. For that reason, I suggest you take your time to work through the code and absorb as much as you can. The better you understand these chapters, the easier it will be to work through the rest of this book."
  },
  {
    "objectID": "conclusion.html",
    "href": "conclusion.html",
    "title": "Conclusion",
    "section": "",
    "text": "Congratulations, you made it to the end of the book! Now you know all of the R fundamentals that I wish I had learned when I was an undergraduate working on my first research project. You may not feel like an ‘expert’ yet. However, take a moment to reflect on everything that you’ve accomplished by working through this book. Seriously, sit down with a pen or pencil and paper and try to make a list of everything you’ve learned. This is an important self-reflection exercise. You will hopefully surprise yourself to see how much you have learned in such a short time!\nOkay, now I’ll try. First, you learned how to program in the command line by typing commands into the R Console in R Studio to produce an output. That alone is a major hurdle for many biologists! But you went even further, perhaps confronting hesitation or self-doubt about your ability to work with mathematical equations. You’ve learned how to translate equations into code, from simple addition, subtraction and multiplication to specific functions for the absolute value, square root, log, average, variance, and others. More importantly, you’ve learned the principles of coding that allow you to translate just about any equation into your very own custom R function. You got tied up with brackets, and brackets within brackets, within brackets to code more complicated equations.\nYou learned all about data frames (i.e. flat data or 2-dimensional data) and all the useful functions from the dplyr library, like sub-setting, joining, sorting, grouping, and summarizing. You may not realize it, but this taught you how to work with relational data. Maybe you don’t call it this, but relational data are just data that relate to each other, which you learned to do with dplyr and join_. This is not much different than what advanced coders and data scientists do with large database protocols like Hadoop or SQL. These lie at the heart of user-friendly web tools for interacting with large online databases you may be familiar with, like Climate Data Online (CDO), the Global Biodiversity Information Facility (GBIF), or the many genetic and genomic databases maintained by the European Molecular Biology Laboratory (EMBL) and the National Center for Biotechnology Information (NCBI).\nYou learned how to use logic operators with flow control and dplyr commands to string your functions together into an automated and reproducible workflow. You even learned how to write your own custom functions!\nYou learned how to work with naughty data, like missing values, mis-coded entries, and dates. Oh man, dates probably caused you so many problems until you leaned how to deal with them in R. You will never, ever, ever, work with dates in Microsoft Excel, if you can help it!\n\n\n\nA popular internet meme about Excel’s propensity to recode data as dates\n\n\nYou ventured into the intimidating yet enchanting world of regular expressions. Once you start looking for opportunities to use regular expressions, you will start to see them in just about every data project you have, whether you are pulling out data with specific characteristics, or reformatting database entries by those who don’t understand the importance of strict data encoding practices. It’s unlikely you’ve masted regular expressions, especially if this was the first time you were exposed to these powerful spells. You just need to practice. The ability to code regular expressions will come with practice, which will enhance your power to help others and bring good to the world, not unlike the Expecto Patronum.\nYou learned everything you need to know to produce professional, publication-worthy visualizations of your data. You learned about important graphical concepts like data formats and colour palettes for publishing, and the layered ‘grammar of graphics’ philosophy. Even better, you learned how to wrap it all together in a reproducible and professional report with R Markdown.\nAbove all, you leaned how to embrace mistakes and troubleshoot your coding problems.\nAlthough you have learned a lot, it’s natural to feel confused or unclear about some of the concepts and techniques you have encountered. In many cases, you may not yet be aware of your knowledge gaps.\nThat’s okay! You are now, officially, a real, bona fide coder. Coding for research is a lifelong learning process, and you will continue to encounter new challenges and opportunities to learn new tricks and techniques as you progress along your coding journey. You didn’t just learn how to code, you learned how to learn how to code. And, you deserve a massive, giant congratulations!\nSure, you have much to learn, but so does everybody else. Maybe you’ll meet a coder with a degree in computer science and a decade of experience coding for a major tech corporation. They will understand coding better than you, but you’ll probably be able to discuss coding better than they can discuss biology. Don’t get intimidated. Embrace the journey, not the destination.\n\n\n\nWhere do you go from here? First, treat yourself. Maybe a vacation, or at least a dessert? You deserve it.\nDon’t ever doubt that you are now a competent coder. You have a lot to learn, but don’t have to wait until you feel like an ‘expert’ to put your skills to good use. Look for opportunities to write and proofread code to hone your skills.\nIf you found that this book suited your learning style, then you might want to check out some of the other books and resources that we are developing.\n\nEcoEvoGeno.org is our main, public-facing website and it contains links to our latest book releases as well as general information about our research and lab activities.\nColauttiLab.github.io is our training ‘resources’ website, with a variety of advice and helpful links for new and established researchers. It’s directed at the graduate students in our lab, but many of these resources may be useful to you.\ngithub.com/ColauttiLab is our github page, where you can search our repositories to see what we are currently working on.\nR STATS Crash Course for Biologists is the next book in this series. If you are going to work with biological data in R, then you probably will want to learn how to run statistical models. The R STATS Crash Course for Biologists covers this, from the most basic ANOVA and linear regression to advanced Generalized Additive Mixed Effects Models.\nThe third book planned in this series, or possibly published by the time you read this, is R Machine Learning Crash Course for Biologists. Once you understand statistical models, the R Machine Learning Crash Course for Biologists will guide you through common supervised and unsupervised machine learning models, including a deep-dive into the Principal Components Analysis (PCA) mentioned briefly in the R Fundamentals Chapter. In addition, you’ll learn how to run Regularized Discriminant Analysis, Support Vector Machines, and Decision Trees to make predictions.\nThe fourth book planned is Bioinformatics Crash Course for Biologists which adds training in Python and Unix for high-performance computing, and then focuses on specific applications in bioinformatics like genome assembly and annotation, gene expression analysis, microbiome analysis, and population genetics.\n\nIf you have any thoughts you would like to share, good or bad, please get in touch. If you have criticisms, please send them to me so that we can improve future editions of the book. You can find up-to-date contact information on our lab website: https://EcoEvoGeno.org\n\n\n\nIf found this book helpful, please consider supporting us. We work hard to keep the cost low for our printed versions and the proprietary electronic versions (e.g. Kindle, Kobo, Apple, Google). Rather than pay publishing cartels or professional editors, all of the proceeds from these versions support graduate students to help make new content available, including translations to other languages and beta testing new tutorials for future books.\nOur team is passionate about demystifying math and coding for biologists, and we want to make these skills more accessible to the next generation of biologists, empowering students of all backgrounds and historically under-represented groups in particular.\nIf you would like to support us, please consider buying a copy to gift to a friend or colleague, if it is within your means. If your budget is tight, then please consider posting a thoughtful and supportive review on Amazon, Barnes & Noble, Apple Books, Google Play Books, or wherever you read this. A positive review will help others to find the book, which will help to build our small community of biology coders. If you aren’t comfortable posting a 5-star review, please contact us to let us know what we can do to bring the next iteration of this book up to your standards.\nAs you develop your coding skills, consider making recommendations to help improve our books. The most efficient way to do this is by posting an issue in Github. Alternatively, you can find up-to-date contact information on our lab website.\nAs you continue on your journey, remember that learning to code is different from most biology that you’ve learned. To really learn to code, you must continue to immerse yourself, study, read, try something new, fail, correct, and repeat. And of course: practice, Practice PRACTICE!\n\n\n\nThinking back on what I’ve learned in coding in R, Python, and Unix since 2009, one thing sticks out as particularly helpful for solidifying my understanding of code: helping others. This book began as a series of self tutorials to teach coding to biologists. This came on the heels of helping with full-day coding workshops with what is now called the Centre for Advanced Computing at Queen’s University. It continues all the way back to my experience as a graduate TA helping with statistics, and offering help to other graduate students who were new to R. All of these experiences helped to reveal my own knowledge gaps and offered opportunities to practice my skills. If you found this book helpful, and you want to continue to develop your skills, this is the best advice I can offer: Pay it forward.\nPlease share your knowledge and experiences with others. As you continue to learn and explore R programming, consider sharing your insights and discoveries with your peers and colleagues. This will help to build a supportive community, and you will probably find that helping others helps you hone your own skills.\nLet us conclude by reviewing your answer to the preface of this book when you were asked to Think of a computer programmer or data scientist.\n\nQuestion: What does a computer programmer look like?\n\nCan you picture yourself in that role? If you completed this book, you should!"
  },
  {
    "objectID": "conclusion.html#you-made-it",
    "href": "conclusion.html#you-made-it",
    "title": "Conclusion",
    "section": "",
    "text": "Congratulations, you made it to the end of the book! Now you know all of the R fundamentals that I wish I had learned when I was an undergraduate working on my first research project. You may not feel like an ‘expert’ yet. However, take a moment to reflect on everything that you’ve accomplished by working through this book. Seriously, sit down with a pen or pencil and paper and try to make a list of everything you’ve learned. This is an important self-reflection exercise. You will hopefully surprise yourself to see how much you have learned in such a short time!\nOkay, now I’ll try. First, you learned how to program in the command line by typing commands into the R Console in R Studio to produce an output. That alone is a major hurdle for many biologists! But you went even further, perhaps confronting hesitation or self-doubt about your ability to work with mathematical equations. You’ve learned how to translate equations into code, from simple addition, subtraction and multiplication to specific functions for the absolute value, square root, log, average, variance, and others. More importantly, you’ve learned the principles of coding that allow you to translate just about any equation into your very own custom R function. You got tied up with brackets, and brackets within brackets, within brackets to code more complicated equations.\nYou learned all about data frames (i.e. flat data or 2-dimensional data) and all the useful functions from the dplyr library, like sub-setting, joining, sorting, grouping, and summarizing. You may not realize it, but this taught you how to work with relational data. Maybe you don’t call it this, but relational data are just data that relate to each other, which you learned to do with dplyr and join_. This is not much different than what advanced coders and data scientists do with large database protocols like Hadoop or SQL. These lie at the heart of user-friendly web tools for interacting with large online databases you may be familiar with, like Climate Data Online (CDO), the Global Biodiversity Information Facility (GBIF), or the many genetic and genomic databases maintained by the European Molecular Biology Laboratory (EMBL) and the National Center for Biotechnology Information (NCBI).\nYou learned how to use logic operators with flow control and dplyr commands to string your functions together into an automated and reproducible workflow. You even learned how to write your own custom functions!\nYou learned how to work with naughty data, like missing values, mis-coded entries, and dates. Oh man, dates probably caused you so many problems until you leaned how to deal with them in R. You will never, ever, ever, work with dates in Microsoft Excel, if you can help it!\n\n\n\nA popular internet meme about Excel’s propensity to recode data as dates\n\n\nYou ventured into the intimidating yet enchanting world of regular expressions. Once you start looking for opportunities to use regular expressions, you will start to see them in just about every data project you have, whether you are pulling out data with specific characteristics, or reformatting database entries by those who don’t understand the importance of strict data encoding practices. It’s unlikely you’ve masted regular expressions, especially if this was the first time you were exposed to these powerful spells. You just need to practice. The ability to code regular expressions will come with practice, which will enhance your power to help others and bring good to the world, not unlike the Expecto Patronum.\nYou learned everything you need to know to produce professional, publication-worthy visualizations of your data. You learned about important graphical concepts like data formats and colour palettes for publishing, and the layered ‘grammar of graphics’ philosophy. Even better, you learned how to wrap it all together in a reproducible and professional report with R Markdown.\nAbove all, you leaned how to embrace mistakes and troubleshoot your coding problems.\nAlthough you have learned a lot, it’s natural to feel confused or unclear about some of the concepts and techniques you have encountered. In many cases, you may not yet be aware of your knowledge gaps.\nThat’s okay! You are now, officially, a real, bona fide coder. Coding for research is a lifelong learning process, and you will continue to encounter new challenges and opportunities to learn new tricks and techniques as you progress along your coding journey. You didn’t just learn how to code, you learned how to learn how to code. And, you deserve a massive, giant congratulations!\nSure, you have much to learn, but so does everybody else. Maybe you’ll meet a coder with a degree in computer science and a decade of experience coding for a major tech corporation. They will understand coding better than you, but you’ll probably be able to discuss coding better than they can discuss biology. Don’t get intimidated. Embrace the journey, not the destination."
  },
  {
    "objectID": "conclusion.html#what-next",
    "href": "conclusion.html#what-next",
    "title": "Conclusion",
    "section": "",
    "text": "Where do you go from here? First, treat yourself. Maybe a vacation, or at least a dessert? You deserve it.\nDon’t ever doubt that you are now a competent coder. You have a lot to learn, but don’t have to wait until you feel like an ‘expert’ to put your skills to good use. Look for opportunities to write and proofread code to hone your skills.\nIf you found that this book suited your learning style, then you might want to check out some of the other books and resources that we are developing.\n\nEcoEvoGeno.org is our main, public-facing website and it contains links to our latest book releases as well as general information about our research and lab activities.\nColauttiLab.github.io is our training ‘resources’ website, with a variety of advice and helpful links for new and established researchers. It’s directed at the graduate students in our lab, but many of these resources may be useful to you.\ngithub.com/ColauttiLab is our github page, where you can search our repositories to see what we are currently working on.\nR STATS Crash Course for Biologists is the next book in this series. If you are going to work with biological data in R, then you probably will want to learn how to run statistical models. The R STATS Crash Course for Biologists covers this, from the most basic ANOVA and linear regression to advanced Generalized Additive Mixed Effects Models.\nThe third book planned in this series, or possibly published by the time you read this, is R Machine Learning Crash Course for Biologists. Once you understand statistical models, the R Machine Learning Crash Course for Biologists will guide you through common supervised and unsupervised machine learning models, including a deep-dive into the Principal Components Analysis (PCA) mentioned briefly in the R Fundamentals Chapter. In addition, you’ll learn how to run Regularized Discriminant Analysis, Support Vector Machines, and Decision Trees to make predictions.\nThe fourth book planned is Bioinformatics Crash Course for Biologists which adds training in Python and Unix for high-performance computing, and then focuses on specific applications in bioinformatics like genome assembly and annotation, gene expression analysis, microbiome analysis, and population genetics.\n\nIf you have any thoughts you would like to share, good or bad, please get in touch. If you have criticisms, please send them to me so that we can improve future editions of the book. You can find up-to-date contact information on our lab website: https://EcoEvoGeno.org"
  },
  {
    "objectID": "conclusion.html#support-open-accessible-science",
    "href": "conclusion.html#support-open-accessible-science",
    "title": "Conclusion",
    "section": "",
    "text": "If found this book helpful, please consider supporting us. We work hard to keep the cost low for our printed versions and the proprietary electronic versions (e.g. Kindle, Kobo, Apple, Google). Rather than pay publishing cartels or professional editors, all of the proceeds from these versions support graduate students to help make new content available, including translations to other languages and beta testing new tutorials for future books.\nOur team is passionate about demystifying math and coding for biologists, and we want to make these skills more accessible to the next generation of biologists, empowering students of all backgrounds and historically under-represented groups in particular.\nIf you would like to support us, please consider buying a copy to gift to a friend or colleague, if it is within your means. If your budget is tight, then please consider posting a thoughtful and supportive review on Amazon, Barnes & Noble, Apple Books, Google Play Books, or wherever you read this. A positive review will help others to find the book, which will help to build our small community of biology coders. If you aren’t comfortable posting a 5-star review, please contact us to let us know what we can do to bring the next iteration of this book up to your standards.\nAs you develop your coding skills, consider making recommendations to help improve our books. The most efficient way to do this is by posting an issue in Github. Alternatively, you can find up-to-date contact information on our lab website.\nAs you continue on your journey, remember that learning to code is different from most biology that you’ve learned. To really learn to code, you must continue to immerse yourself, study, read, try something new, fail, correct, and repeat. And of course: practice, Practice PRACTICE!"
  },
  {
    "objectID": "conclusion.html#picture-a-coder",
    "href": "conclusion.html#picture-a-coder",
    "title": "Conclusion",
    "section": "",
    "text": "Thinking back on what I’ve learned in coding in R, Python, and Unix since 2009, one thing sticks out as particularly helpful for solidifying my understanding of code: helping others. This book began as a series of self tutorials to teach coding to biologists. This came on the heels of helping with full-day coding workshops with what is now called the Centre for Advanced Computing at Queen’s University. It continues all the way back to my experience as a graduate TA helping with statistics, and offering help to other graduate students who were new to R. All of these experiences helped to reveal my own knowledge gaps and offered opportunities to practice my skills. If you found this book helpful, and you want to continue to develop your skills, this is the best advice I can offer: Pay it forward.\nPlease share your knowledge and experiences with others. As you continue to learn and explore R programming, consider sharing your insights and discoveries with your peers and colleagues. This will help to build a supportive community, and you will probably find that helping others helps you hone your own skills.\nLet us conclude by reviewing your answer to the preface of this book when you were asked to Think of a computer programmer or data scientist.\n\nQuestion: What does a computer programmer look like?\n\nCan you picture yourself in that role? If you completed this book, you should!"
  },
  {
    "objectID": "customizations.html",
    "href": "customizations.html",
    "title": "Basic Customization",
    "section": "",
    "text": "Now that you know how to use R to to quickly generate graphs, we’ll explore how to customize our graphs to make high-quality figures for publication.\nThere are a number of parameters and other functions available with ggplot() that you can use to quickly customize your graphs. First, we’ll look at the different ways to customize the look and feel of our graphs. Then, we’ll combine multiple graphs in the same multi-panel figure. In addition to generating more complex figures for publication, these multi-panel graphs can come in quite handy for exploring more complex data set.\n\n\n\nContinuing from the last chapter, load the ggplot2 library and set up our plotting data.\n\nlibrary(ggplot2)\nMyData&lt;-read.csv(\n  \"https://colauttilab.github.io/RCrashCourse/FallopiaData.csv\")\n\n\n\n\nAs we noted briefly in the last chapter, we can use binwidth with the histogram graph type to alter the size of the ‘bins’ along the x-axis. A bin is defined by a range of values (x-axis). The bin count or frequency (y-axis) shows the number observations (or fraction) that fall within each bin range.\nThe binwidth defines the range of values (i.e. width) of each bin. Here are a couple of examples for comparison.\n\nlibrary(ggplot2)\nggplot(aes(x=Total), data=MyData) + geom_histogram(binwidth=9)\n\n\n\n\n\n\n\nggplot(aes(x=Total), data=MyData) + geom_histogram(binwidth=0.5)\n\n\n\n\n\n\n\n\nCompare the code for each graph to understand how binwidth affects both the y-axis values and the width of the blocks along the x-axis. Wider bins contain more observations, just like larger barrels catch more rain.\n\n\n\nThis controls the point size. Importantly, size values can be interpreted by R in two ways, which can cause some confusion:\n\nAs a single value: To assign a specific size to all points. This is assigned in the geom_point() function\n\n\nggplot(aes(x=Silene, y=Total), data=MyData) + \n  geom_point(size=5)\n\n\n\n\n\n\n\n\n\nAs a set of values defined in a vector: Scale size based on a column of data (e.g. number of observations). This is defined in the aes() function.\n\nFrom the perspective of the R console, these are pretty much the same thing since a single value can be treated as a vector with just one element.\n\nggplot(aes(x=Silene, y=Fallopia), data=MyData) +\n  geom_point(aes(size=Total))\n\n\n\n\n\n\n\n\nNOTE: The following code will produce the exact same graph.\n\nggplot(aes(x=Silene, y=Fallopia, size=Total), data=MyData) +\n  geom_point()\n\nCompare this ggplot() function with the two previous.\n\nQuestion: What do you think is the difference between putting an aes function inside of ggplot() vs inside of geom_point()?\n\nAnswer: It’s important to understand the difference, even though in this specific example it doesn’t change the graph. Here is a short summary:\n\nIf we put a variable inside of ggplot() then the parameter applies to ALL of the geom functions that follow it.\nIf we put a variable inside of a geom like geom_point(), then the parameter applies ONLY to that specific geometric shape layer.\nWe use aes() when refereincing a column from our input data.\n\nWe’ll dive into these ideas in more detail in the next chapter, when we start to produce more complicated graphs with multiple, overlapping geoms.\nBefore continuing, take a moment to make sure you understand the three different examples of code and resulting output above.\n\n\n\nThink of alpha as a measure of opacity, ranging from 0 to 1 with 1 being the default – a solid point or line.\nThis is particularly useful for visualizing overlapping points.\n\nggplot(aes(x=Silene, y=Total), data=MyData) +\n  geom_point(aes(colour=Nutrients), size=5, alpha=0.3)\n\n\n\n\n\n\n\n\n\n\n\nAnother nice feature of ggplot is that you can use alternate English spelling for some of the parameters. For example, you can use colour= or color= add colour to your color graphs.\nSimilar to point sizes, you can use colours in two main ways.\n\nYou can colour points based on a factor.\n\n\nggplot(aes(x=Silene, y=Fallopia), data=MyData) +\n  geom_point(aes(colour=Nutrients))\n\n\n\n\n\n\n\n\n… or a continuous variable.\n\nggplot(aes(x=Silene, y=Fallopia), data=MyData) +\n  geom_point(aes(colour=Total))\n\n\n\n\n\n\n\n\n\nYou can choose a specific colour to apply to all points.\n\n\nggplot(aes(x=Silene, y=Fallopia), data=MyData) +\n  geom_point(colour=\"grey60\")\n\n\n\n\n\n\n\n\n\n\n\nSeveral colours are available as strings (e.g. \"red\", \"blue\", \"aquamarine\", \"coral\", \"grey20\", \"grey60\"), but if you can’t find one that you want, you can make just about any colour with the rgb() function. The rgb function takes three values corresponding to the intensity of red, green and blue light, respectively. Values range from 0 (no colour) to 1 (brightest intensity).\n\nggplot(aes(x=Silene, y=Fallopia), data=MyData) +\n  geom_point(colour=rgb(1,0.7,0.9))\n\n\n\n\n\n\n\n\nSome colouring systems use a 256-bit scale (0 to 255) instead of 0 to 1, which you can specify in the rgb() function with the maxColorValue = 255 parameter. See ?rgb for more information.\n\n\nAnother common format for colour uses a hexadecimal system. In fact, the hexadecimal code is the output of the rgb() function that R uses for plotting:\n\nrgb(0.1,0.3,1) \n\n[1] \"#1A4DFF\"\n\nI(rgb(255,0,0, maxColorValue=255))\n\n[1] \"#FF0000\"\n\n\nThe hexadecimal system is a base-16 alphanumeric code that is common in computing. It uses the numerical digits 0-9 followed by the letters A (11) through F (16) as the 16 characters.\nHexadecimal colour codes are used by a variety of computer programs. For colouring visualizations with ggplot, we use a 6 OR 8-character hexadecimal code, starting with the hash mark # and saved as a string using quotation marks.\nThe 6-digit hexadecimal colour code uses two digits for each base colour: red (r), green (g) and blue (b), or #&lt;rrggbb&gt;. We’ll see an example to help clarify this.\nThis 6-digit code results in \\(16 × 16 = 256\\) shades of each colour, or \\(256^3 = 16,777,216\\) total colour combinations\nThe 8-digit hexadecimal colour code is similar, with the additional two digits at the end to define the level of alpha/transparency.\nThe rgb() function converts a vector of red, green, blue, (and optional alpha) to the 6- or 8-digit hexadecimal equivalent.\n\nrgb(1,1,1,0.5)\n\n[1] \"#FFFFFF80\"\n\n\nAlternatively, transparency can be specified with the alpha parameter, as noted earlier.\n\n\n\nNote what happens when we use the colour parameter for a histogram.\n\nggplot(aes(x=Total), data=MyData) +\n  geom_histogram(aes(colour=Nutrients), bins=10)\n\n\n\n\n\n\n\n\nThe coloured outlines might be useful in some cases, but we usually want the entire bars coloured. We can use the fill parameter for this.\n\n\n\n\nThis parameter is used for histogram boxes and other geometic shapes that have a separate outline (colour=) and interior (fill=).\n\nggplot(aes(x=Total), data=MyData) +\n  geom_histogram(aes(fill=Nutrients), bins=10)\n\n\n\n\n\n\n\n\n\n\n\nUse this to adjust the position, usually for histograms or bar graphs. For example, in the previous graph the bars are ‘stacked’ on top of each other. It can be hard to interpret a histogram with stacked bars, but we can shift the position using dodge.\n\nggplot(aes(x=Total), data=MyData) +\n  geom_histogram(aes(fill=Nutrients), bins=10, position=\"dodge\")\n\n\n\n\n\n\n\n\n\n\n\nYou can also change the shape of your points, again using a column of data or a specific value.\n\nggplot(aes(x=Silene, y=Total), data=MyData) +\n  geom_point(aes(shape=Nutrients))\n\n\n\n\n\n\n\nggplot(aes(x=Silene, y=Total), data=MyData) +\n  geom_point(shape=17)\n\n\n\n\n\n\n\n\nThere are a number of different shapes available, by specifying a number from 0 through 25.\n\n\n\n\n\n\n\n\n\nNote that the shapes with grey in the above figure can be coloured with fill= parameter, while all of the black parts (lines and fill) can be coloured with the colour= parameter.\nYou can use fill and colour to customize these separately.\n\nggplot(aes(x=Silene, y=Total), data=MyData) +\n  geom_point(shape=21, size=5, colour=\"purple\", fill=\"yellow\")\n\n\n\n\n\n\n\n\nNote how a solid outline can help your points ‘pop’.\nSimilarly, specifying a solid colour can definition to a histogram graph.\n\nggplot(aes(x=Silene), data=MyData) +\n  geom_histogram(bins=20,colour=\"darkred\",fill=\"aquamarine\")\n\n\n\n\n\n\n\n\n\n\n\nUse these to customize your axis labels.\n\nggplot(aes(x=Silene, y=Total), data=MyData) +\n  geom_point() +\n  xlab(\"Silene Biomass\") + ylab(\"Total Biomass\")\n\n\n\n\n\n\n\n\n\n\n\nThis will add other labels to your plot. Usually you wouldn’t use this for a figure intended for publication – for this you would need a detailed caption, usually just a paragraph of text below the figure. However, these can be useful for other documents: reports, websites, presentations, supplementary material, appendices, etc.\n\nggplot(aes(x=Silene, y=Total), data=MyData) +\n  geom_point() + labs(title=\"Biomass\", subtitle=\"More info here\",\n                      caption=\"Appears after the figure\")\n\n\n\n\n\n\n\n\n\n\n\nWe have already explored a few of the many Geoms available. These determine the geometry of your graph, which is how your data are mathematically mapped to the graphing space.\nThemes define the look and ‘feel’ of your graphs.\nIn ggplot(), themes and geoms are added with a separate function linked to the graph by using the plus sign +.\n\n\nWe explored a few geoms above, but there are many more available on the ggplot2 website, with helpful examples: https://ggplot2.tidyverse.org/reference/\n\n\n\nThere are a number of available themes, defined by changing the &lt;name&gt; part of theme_&lt;name&gt;(). We’ll try potting these different themes on the same graph. Rather than type out the same ggplot() and geom_ functions every time, we can define an object to hold the data for the plot, and then just change the theme.\n\n\n\nPlot1&lt;-ggplot(aes(x=Silene, y=Total), data=MyData) + geom_point()\nPlot1 + theme_grey()\n\n\n\n\n\n\n\n\n\n\n\n\nPlot1 + theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\nPlot1 + theme_linedraw()\n\n\n\n\n\n\n\n\n\n\n\n\nPlot1 + theme_light()\n\n\n\n\n\n\n\n\n\n\n\n\nPlot1 + theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nThis is closest to what you would see in a published paper, with x- and y-axis lines only\n\nPlot1 + theme_classic()\n\n\n\n\n\n\n\n\nThese can be further customized. Or you can create a completely new theme.\n\n\n\n\nHere is a simplified and cleaner version of theme_classic but with bigger axis labels that are more suitable for figures in presentation or publication. The theme is a function, which can be customized. Custom functions are covered in the Advanced R Chapter. For now you can just copy the code block below.\n\n# Clean theme for presentations & publications\ntheme_pub &lt;- function (base_size = 12, base_family = \"\") {\n  theme_classic(base_size = base_size, \n                base_family = base_family) %+replace% \n    theme(\n      axis.text = element_text(colour = \"black\"),\n      axis.title.x = element_text(size=18),\n      axis.text.x = element_text(size=12),\n      axis.title.y = element_text(size=18,angle=90),\n      axis.text.y = element_text(size=12),\n      axis.ticks = element_blank(), \n      panel.background = element_rect(fill=\"white\"),\n      panel.border = element_blank(),\n      plot.title=element_text(face=\"bold\", size=24),\n      legend.position=\"none\"\n    ) \n}\n\nTo use this theme, you have to make sure you run the entire function (e.g. highlight every line and press Ctl + R or click Run in R Studio).\nAlternatively, you could save it as a separate .R file (e.g. theme.R) and then load it with the source() function (e.g. source(\"./theme.R\"))\n\n\n\nA third, even easier option, is to load the version of this code that is available online.\n\nsource(\"http://bit.ly/theme_pub\")\n\nThe theme is called theme_pub (pub is short for publication). To use it, run the above line, and then add it to your graphing functions:\n\nPlot1 + theme_pub() \n\n\n\n\n\n\n\nggplot(aes(x=Silene),data=MyData) + \n  geom_histogram(binwidth=2) + theme_pub()\n\n\n\n\n\n\n\n\n\n\n\nIf you want to use the same theme throughout your code, you can use the theme_set function.\n\ntheme_set(theme_pub())\nPlot1\n\n\n\n\n\n\n\n\nNow that we have run the source and theme_set functions, all of the graphs we make in this session will use the improved formatting. No more ugly grey background and tiny axis labels!\n\n\n\n\nIt is often handy to plot separate graphs for different categories of a grouping variable. This can be done with facets in qplot.\n\n\nFacets have the general form VERTICAL ~ HORIZONTAL. Note the use of the tilde (~), not the dash (-). Use a period (.) to indicate ‘all data’ or ‘do not separate my data’, as shown in the fullowing examples.\n\n\n\nPlot2&lt;-ggplot(aes(x=Silene),data=MyData) +\n  geom_histogram(binwidth=2)\n\nPlot2 + facet_grid(Nutrients~.)\n\n\n\n\n\n\n\n\n\n\n\n\nPlot2 + facet_wrap(.~Nutrients) \n\n\n\n\n\n\n\n\n\n\n\n\nPlot2 + facet_grid(Taxon~Nutrients)\n\n\n\n\n\n\n\n\n\n\n\n\n\nGraphing in R studio is okay for exploration but eventually you are going to want to save those beautiful figures you made, and this can be part of your reproducible workflow.\nWriting code in R to save your graphs to an external file requires three important steps:\n\nOpen a file using a function like pdf or svg for the vector format, or png for the raster format. Remember that you usually will want to stick with a vector format, for reasons discussed in the Graphical Concepts section earlier.\nRun the code to produce the graph. Instead of seeing a graph in your R interface, you will not see anything because the graph is being sent to the file.\nIMPORTANT: Close the file! Do this with the dev.off() function.\n\nFailing to close the file is a common source of error when saving graphs. If you are having problems with graphing outputs, try running the dev.off() function a few times to make sure you close any files that are ‘hanging’ open.\nHere’s an example code for making a pdf output of a graph. When you run it you should see a file appear in your working folder (you may have to refresh).\n\npdf(\"SileneHist.pdf\") # 1. Open\n  Plot2 + facet_grid(Taxon~Nutrients) # 2. Write\ndev.off() # 3. Close\n\nNote how the plotting function on the second line does not open in the plots window when you run this. This is because the info is sent to SileneHist.pdf file instead of the graphing area in R Studio.\n\n\n\nGraphing may seem slow and tedious at first, but the more you practice, the faster you will be able to produce meaningful visualizations.\nDon’t be afraid to try new things. Try mixing up components and see what happens. At worst you will just get an error message.\nOnce you have a good understanding of these basics, you can see how to build more advanced plots in the next chapter."
  },
  {
    "objectID": "customizations.html#introduction",
    "href": "customizations.html#introduction",
    "title": "Basic Customization",
    "section": "",
    "text": "Now that you know how to use R to to quickly generate graphs, we’ll explore how to customize our graphs to make high-quality figures for publication.\nThere are a number of parameters and other functions available with ggplot() that you can use to quickly customize your graphs. First, we’ll look at the different ways to customize the look and feel of our graphs. Then, we’ll combine multiple graphs in the same multi-panel figure. In addition to generating more complex figures for publication, these multi-panel graphs can come in quite handy for exploring more complex data set."
  },
  {
    "objectID": "customizations.html#setup",
    "href": "customizations.html#setup",
    "title": "Basic Customization",
    "section": "",
    "text": "Continuing from the last chapter, load the ggplot2 library and set up our plotting data.\n\nlibrary(ggplot2)\nMyData&lt;-read.csv(\n  \"https://colauttilab.github.io/RCrashCourse/FallopiaData.csv\")"
  },
  {
    "objectID": "customizations.html#binwidth",
    "href": "customizations.html#binwidth",
    "title": "Basic Customization",
    "section": "",
    "text": "As we noted briefly in the last chapter, we can use binwidth with the histogram graph type to alter the size of the ‘bins’ along the x-axis. A bin is defined by a range of values (x-axis). The bin count or frequency (y-axis) shows the number observations (or fraction) that fall within each bin range.\nThe binwidth defines the range of values (i.e. width) of each bin. Here are a couple of examples for comparison.\n\nlibrary(ggplot2)\nggplot(aes(x=Total), data=MyData) + geom_histogram(binwidth=9)\n\n\n\n\n\n\n\nggplot(aes(x=Total), data=MyData) + geom_histogram(binwidth=0.5)\n\n\n\n\n\n\n\n\nCompare the code for each graph to understand how binwidth affects both the y-axis values and the width of the blocks along the x-axis. Wider bins contain more observations, just like larger barrels catch more rain."
  },
  {
    "objectID": "customizations.html#size",
    "href": "customizations.html#size",
    "title": "Basic Customization",
    "section": "",
    "text": "This controls the point size. Importantly, size values can be interpreted by R in two ways, which can cause some confusion:\n\nAs a single value: To assign a specific size to all points. This is assigned in the geom_point() function\n\n\nggplot(aes(x=Silene, y=Total), data=MyData) + \n  geom_point(size=5)\n\n\n\n\n\n\n\n\n\nAs a set of values defined in a vector: Scale size based on a column of data (e.g. number of observations). This is defined in the aes() function.\n\nFrom the perspective of the R console, these are pretty much the same thing since a single value can be treated as a vector with just one element.\n\nggplot(aes(x=Silene, y=Fallopia), data=MyData) +\n  geom_point(aes(size=Total))\n\n\n\n\n\n\n\n\nNOTE: The following code will produce the exact same graph.\n\nggplot(aes(x=Silene, y=Fallopia, size=Total), data=MyData) +\n  geom_point()\n\nCompare this ggplot() function with the two previous.\n\nQuestion: What do you think is the difference between putting an aes function inside of ggplot() vs inside of geom_point()?\n\nAnswer: It’s important to understand the difference, even though in this specific example it doesn’t change the graph. Here is a short summary:\n\nIf we put a variable inside of ggplot() then the parameter applies to ALL of the geom functions that follow it.\nIf we put a variable inside of a geom like geom_point(), then the parameter applies ONLY to that specific geometric shape layer.\nWe use aes() when refereincing a column from our input data.\n\nWe’ll dive into these ideas in more detail in the next chapter, when we start to produce more complicated graphs with multiple, overlapping geoms.\nBefore continuing, take a moment to make sure you understand the three different examples of code and resulting output above."
  },
  {
    "objectID": "customizations.html#alpha",
    "href": "customizations.html#alpha",
    "title": "Basic Customization",
    "section": "",
    "text": "Think of alpha as a measure of opacity, ranging from 0 to 1 with 1 being the default – a solid point or line.\nThis is particularly useful for visualizing overlapping points.\n\nggplot(aes(x=Silene, y=Total), data=MyData) +\n  geom_point(aes(colour=Nutrients), size=5, alpha=0.3)"
  },
  {
    "objectID": "customizations.html#colour-or-color",
    "href": "customizations.html#colour-or-color",
    "title": "Basic Customization",
    "section": "",
    "text": "Another nice feature of ggplot is that you can use alternate English spelling for some of the parameters. For example, you can use colour= or color= add colour to your color graphs.\nSimilar to point sizes, you can use colours in two main ways.\n\nYou can colour points based on a factor.\n\n\nggplot(aes(x=Silene, y=Fallopia), data=MyData) +\n  geom_point(aes(colour=Nutrients))\n\n\n\n\n\n\n\n\n… or a continuous variable.\n\nggplot(aes(x=Silene, y=Fallopia), data=MyData) +\n  geom_point(aes(colour=Total))\n\n\n\n\n\n\n\n\n\nYou can choose a specific colour to apply to all points.\n\n\nggplot(aes(x=Silene, y=Fallopia), data=MyData) +\n  geom_point(colour=\"grey60\")"
  },
  {
    "objectID": "customizations.html#colour-with-rgb",
    "href": "customizations.html#colour-with-rgb",
    "title": "Basic Customization",
    "section": "",
    "text": "Several colours are available as strings (e.g. \"red\", \"blue\", \"aquamarine\", \"coral\", \"grey20\", \"grey60\"), but if you can’t find one that you want, you can make just about any colour with the rgb() function. The rgb function takes three values corresponding to the intensity of red, green and blue light, respectively. Values range from 0 (no colour) to 1 (brightest intensity).\n\nggplot(aes(x=Silene, y=Fallopia), data=MyData) +\n  geom_point(colour=rgb(1,0.7,0.9))\n\n\n\n\n\n\n\n\nSome colouring systems use a 256-bit scale (0 to 255) instead of 0 to 1, which you can specify in the rgb() function with the maxColorValue = 255 parameter. See ?rgb for more information.\n\n\nAnother common format for colour uses a hexadecimal system. In fact, the hexadecimal code is the output of the rgb() function that R uses for plotting:\n\nrgb(0.1,0.3,1) \n\n[1] \"#1A4DFF\"\n\nI(rgb(255,0,0, maxColorValue=255))\n\n[1] \"#FF0000\"\n\n\nThe hexadecimal system is a base-16 alphanumeric code that is common in computing. It uses the numerical digits 0-9 followed by the letters A (11) through F (16) as the 16 characters.\nHexadecimal colour codes are used by a variety of computer programs. For colouring visualizations with ggplot, we use a 6 OR 8-character hexadecimal code, starting with the hash mark # and saved as a string using quotation marks.\nThe 6-digit hexadecimal colour code uses two digits for each base colour: red (r), green (g) and blue (b), or #&lt;rrggbb&gt;. We’ll see an example to help clarify this.\nThis 6-digit code results in \\(16 × 16 = 256\\) shades of each colour, or \\(256^3 = 16,777,216\\) total colour combinations\nThe 8-digit hexadecimal colour code is similar, with the additional two digits at the end to define the level of alpha/transparency.\nThe rgb() function converts a vector of red, green, blue, (and optional alpha) to the 6- or 8-digit hexadecimal equivalent.\n\nrgb(1,1,1,0.5)\n\n[1] \"#FFFFFF80\"\n\n\nAlternatively, transparency can be specified with the alpha parameter, as noted earlier.\n\n\n\nNote what happens when we use the colour parameter for a histogram.\n\nggplot(aes(x=Total), data=MyData) +\n  geom_histogram(aes(colour=Nutrients), bins=10)\n\n\n\n\n\n\n\n\nThe coloured outlines might be useful in some cases, but we usually want the entire bars coloured. We can use the fill parameter for this."
  },
  {
    "objectID": "customizations.html#fill",
    "href": "customizations.html#fill",
    "title": "Basic Customization",
    "section": "",
    "text": "This parameter is used for histogram boxes and other geometic shapes that have a separate outline (colour=) and interior (fill=).\n\nggplot(aes(x=Total), data=MyData) +\n  geom_histogram(aes(fill=Nutrients), bins=10)"
  },
  {
    "objectID": "customizations.html#position",
    "href": "customizations.html#position",
    "title": "Basic Customization",
    "section": "",
    "text": "Use this to adjust the position, usually for histograms or bar graphs. For example, in the previous graph the bars are ‘stacked’ on top of each other. It can be hard to interpret a histogram with stacked bars, but we can shift the position using dodge.\n\nggplot(aes(x=Total), data=MyData) +\n  geom_histogram(aes(fill=Nutrients), bins=10, position=\"dodge\")"
  },
  {
    "objectID": "customizations.html#shape",
    "href": "customizations.html#shape",
    "title": "Basic Customization",
    "section": "",
    "text": "You can also change the shape of your points, again using a column of data or a specific value.\n\nggplot(aes(x=Silene, y=Total), data=MyData) +\n  geom_point(aes(shape=Nutrients))\n\n\n\n\n\n\n\nggplot(aes(x=Silene, y=Total), data=MyData) +\n  geom_point(shape=17)\n\n\n\n\n\n\n\n\nThere are a number of different shapes available, by specifying a number from 0 through 25.\n\n\n\n\n\n\n\n\n\nNote that the shapes with grey in the above figure can be coloured with fill= parameter, while all of the black parts (lines and fill) can be coloured with the colour= parameter.\nYou can use fill and colour to customize these separately.\n\nggplot(aes(x=Silene, y=Total), data=MyData) +\n  geom_point(shape=21, size=5, colour=\"purple\", fill=\"yellow\")\n\n\n\n\n\n\n\n\nNote how a solid outline can help your points ‘pop’.\nSimilarly, specifying a solid colour can definition to a histogram graph.\n\nggplot(aes(x=Silene), data=MyData) +\n  geom_histogram(bins=20,colour=\"darkred\",fill=\"aquamarine\")"
  },
  {
    "objectID": "customizations.html#lab-xlab-and-ylab",
    "href": "customizations.html#lab-xlab-and-ylab",
    "title": "Basic Customization",
    "section": "",
    "text": "Use these to customize your axis labels.\n\nggplot(aes(x=Silene, y=Total), data=MyData) +\n  geom_point() +\n  xlab(\"Silene Biomass\") + ylab(\"Total Biomass\")"
  },
  {
    "objectID": "customizations.html#labs",
    "href": "customizations.html#labs",
    "title": "Basic Customization",
    "section": "",
    "text": "This will add other labels to your plot. Usually you wouldn’t use this for a figure intended for publication – for this you would need a detailed caption, usually just a paragraph of text below the figure. However, these can be useful for other documents: reports, websites, presentations, supplementary material, appendices, etc.\n\nggplot(aes(x=Silene, y=Total), data=MyData) +\n  geom_point() + labs(title=\"Biomass\", subtitle=\"More info here\",\n                      caption=\"Appears after the figure\")"
  },
  {
    "objectID": "customizations.html#themes-and-geoms",
    "href": "customizations.html#themes-and-geoms",
    "title": "Basic Customization",
    "section": "",
    "text": "We have already explored a few of the many Geoms available. These determine the geometry of your graph, which is how your data are mathematically mapped to the graphing space.\nThemes define the look and ‘feel’ of your graphs.\nIn ggplot(), themes and geoms are added with a separate function linked to the graph by using the plus sign +.\n\n\nWe explored a few geoms above, but there are many more available on the ggplot2 website, with helpful examples: https://ggplot2.tidyverse.org/reference/\n\n\n\nThere are a number of available themes, defined by changing the &lt;name&gt; part of theme_&lt;name&gt;(). We’ll try potting these different themes on the same graph. Rather than type out the same ggplot() and geom_ functions every time, we can define an object to hold the data for the plot, and then just change the theme.\n\n\n\nPlot1&lt;-ggplot(aes(x=Silene, y=Total), data=MyData) + geom_point()\nPlot1 + theme_grey()\n\n\n\n\n\n\n\n\n\n\n\n\nPlot1 + theme_bw()\n\n\n\n\n\n\n\n\n\n\n\n\nPlot1 + theme_linedraw()\n\n\n\n\n\n\n\n\n\n\n\n\nPlot1 + theme_light()\n\n\n\n\n\n\n\n\n\n\n\n\nPlot1 + theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\nThis is closest to what you would see in a published paper, with x- and y-axis lines only\n\nPlot1 + theme_classic()\n\n\n\n\n\n\n\n\nThese can be further customized. Or you can create a completely new theme.\n\n\n\n\nHere is a simplified and cleaner version of theme_classic but with bigger axis labels that are more suitable for figures in presentation or publication. The theme is a function, which can be customized. Custom functions are covered in the Advanced R Chapter. For now you can just copy the code block below.\n\n# Clean theme for presentations & publications\ntheme_pub &lt;- function (base_size = 12, base_family = \"\") {\n  theme_classic(base_size = base_size, \n                base_family = base_family) %+replace% \n    theme(\n      axis.text = element_text(colour = \"black\"),\n      axis.title.x = element_text(size=18),\n      axis.text.x = element_text(size=12),\n      axis.title.y = element_text(size=18,angle=90),\n      axis.text.y = element_text(size=12),\n      axis.ticks = element_blank(), \n      panel.background = element_rect(fill=\"white\"),\n      panel.border = element_blank(),\n      plot.title=element_text(face=\"bold\", size=24),\n      legend.position=\"none\"\n    ) \n}\n\nTo use this theme, you have to make sure you run the entire function (e.g. highlight every line and press Ctl + R or click Run in R Studio).\nAlternatively, you could save it as a separate .R file (e.g. theme.R) and then load it with the source() function (e.g. source(\"./theme.R\"))\n\n\n\nA third, even easier option, is to load the version of this code that is available online.\n\nsource(\"http://bit.ly/theme_pub\")\n\nThe theme is called theme_pub (pub is short for publication). To use it, run the above line, and then add it to your graphing functions:\n\nPlot1 + theme_pub() \n\n\n\n\n\n\n\nggplot(aes(x=Silene),data=MyData) + \n  geom_histogram(binwidth=2) + theme_pub()\n\n\n\n\n\n\n\n\n\n\n\nIf you want to use the same theme throughout your code, you can use the theme_set function.\n\ntheme_set(theme_pub())\nPlot1\n\n\n\n\n\n\n\n\nNow that we have run the source and theme_set functions, all of the graphs we make in this session will use the improved formatting. No more ugly grey background and tiny axis labels!"
  },
  {
    "objectID": "customizations.html#basic-multi-plot-graphs",
    "href": "customizations.html#basic-multi-plot-graphs",
    "title": "Basic Customization",
    "section": "",
    "text": "It is often handy to plot separate graphs for different categories of a grouping variable. This can be done with facets in qplot.\n\n\nFacets have the general form VERTICAL ~ HORIZONTAL. Note the use of the tilde (~), not the dash (-). Use a period (.) to indicate ‘all data’ or ‘do not separate my data’, as shown in the fullowing examples.\n\n\n\nPlot2&lt;-ggplot(aes(x=Silene),data=MyData) +\n  geom_histogram(binwidth=2)\n\nPlot2 + facet_grid(Nutrients~.)\n\n\n\n\n\n\n\n\n\n\n\n\nPlot2 + facet_wrap(.~Nutrients) \n\n\n\n\n\n\n\n\n\n\n\n\nPlot2 + facet_grid(Taxon~Nutrients)"
  },
  {
    "objectID": "customizations.html#graph-output",
    "href": "customizations.html#graph-output",
    "title": "Basic Customization",
    "section": "",
    "text": "Graphing in R studio is okay for exploration but eventually you are going to want to save those beautiful figures you made, and this can be part of your reproducible workflow.\nWriting code in R to save your graphs to an external file requires three important steps:\n\nOpen a file using a function like pdf or svg for the vector format, or png for the raster format. Remember that you usually will want to stick with a vector format, for reasons discussed in the Graphical Concepts section earlier.\nRun the code to produce the graph. Instead of seeing a graph in your R interface, you will not see anything because the graph is being sent to the file.\nIMPORTANT: Close the file! Do this with the dev.off() function.\n\nFailing to close the file is a common source of error when saving graphs. If you are having problems with graphing outputs, try running the dev.off() function a few times to make sure you close any files that are ‘hanging’ open.\nHere’s an example code for making a pdf output of a graph. When you run it you should see a file appear in your working folder (you may have to refresh).\n\npdf(\"SileneHist.pdf\") # 1. Open\n  Plot2 + facet_grid(Taxon~Nutrients) # 2. Write\ndev.off() # 3. Close\n\nNote how the plotting function on the second line does not open in the plots window when you run this. This is because the info is sent to SileneHist.pdf file instead of the graphing area in R Studio."
  },
  {
    "objectID": "customizations.html#practice",
    "href": "customizations.html#practice",
    "title": "Basic Customization",
    "section": "",
    "text": "Graphing may seem slow and tedious at first, but the more you practice, the faster you will be able to produce meaningful visualizations.\nDon’t be afraid to try new things. Try mixing up components and see what happens. At worst you will just get an error message.\nOnce you have a good understanding of these basics, you can see how to build more advanced plots in the next chapter."
  },
  {
    "objectID": "flowcontrol.html",
    "href": "flowcontrol.html",
    "title": "Flow Control",
    "section": "",
    "text": "Think of your data analysis as a stream flowing from the raw data at the headwaters down to the river mouth, exiting as a full analysis with graphics, statistical analyses, and biological interpretation.\nThere are different ways we can control the flow of our code. The simplest is just to write a sequence of lines of code, with the output of one line of code forming the input of the next. A pseudo-code example might be:\nA&lt;-functionA()\nB&lt;-functionB(A)\nC&lt;-functionC(B)\nBut sometimes we may want to do the same function or analysis only if the input meets certain criteria. Or we may want to reiterate the same analysis multiple times on different inputs. This is where more advanced flow control comes in handy.\nTo start, let’s make up a couple of objects to play with:\n\nX&lt;-21\nXvec&lt;-c(1:10,\"string\")\n\n\n\n\nThe if(){} statement uses an operator (see above) to asses whether the value is TRUE or FALSE:\n\nif(X &gt; 100){ # Is X greater than 100?\n  print(\"X &gt; 100\") # If TRUE\n} else { \n  print(\"X &lt;= 100\") # If FALSE\n}\n\n[1] \"X &lt;= 100\"\n\n\nA common ‘rookie’ mistake is to leave out a bracket or use the wrong type of bracket. Use regular brackets for the if function if() followed by two sets of curly brackets containing the code to run {run if true}else{run if false}.\nBreak up across multiple lines to improve readability. Note that you don’t need an else{} part if you just want to do nothing when FALSE.\n\nif(X &gt; 0){print (\"yup\")}\n\n[1] \"yup\"\n\n\n\n\n\nThe ifelse() is a more compact version for simple comparisons. The following code does the same as above.\n\nifelse(X &gt; 100,\"X &gt; 100\", \"X &lt;= 100\")\n\n[1] \"X &lt;= 100\"\n\n\n\n\n\nYou can also nest if and ifelse statements to account for more outcomes. Conceptually think of it as a bifurcating tree, starting at the top (root) and then splitting in two for every if statement.\n\nif(X &gt; 100){\n  print(\"X &gt; 100\")\n  if(X &gt; 200){\n    print(\"X &gt; 200\")\n  }\n} else {\n  if(X == 100){\n    print(\"X = 100\")\n  } else {\n    print(\"X &lt; 100\")\n  }\n}\n\n[1] \"X &lt; 100\"\n\n\nDon’t get intimidated. It just takes time to work through all of the possibilities. Try to draw a bifurcating diagram to represent each true/false outcome for the above code.\n\n\n\nA loop does the same thing over and over again until some condition is met. In the case of a for loop, we set a ‘counter’ variable and loop through each value of the counter variable. Here are a few examples:\n\nLoop through numbers from 1 to 5\n\n\nfor (i in 1:5){\n  print(paste(X,i,sep=\":\"))\n}\n\n[1] \"21:1\"\n[1] \"21:2\"\n[1] \"21:3\"\n[1] \"21:4\"\n[1] \"21:5\"\n\n\n\nLoop through the elements of a vector directly\n\n\nfor (i in Xvec){\n  print(i)\n}\n\n[1] \"1\"\n[1] \"2\"\n[1] \"3\"\n[1] \"4\"\n[1] \"5\"\n[1] \"6\"\n[1] \"7\"\n[1] \"8\"\n[1] \"9\"\n[1] \"10\"\n[1] \"string\"\n\n\n\nUse an index object to index the elements of a vector\n\n\nfor (i in 1:length(Xvec)){\n  print(Xvec[i])\n}\n\n[1] \"1\"\n[1] \"2\"\n[1] \"3\"\n[1] \"4\"\n[1] \"5\"\n[1] \"6\"\n[1] \"7\"\n[1] \"8\"\n[1] \"9\"\n[1] \"10\"\n[1] \"string\"\n\n\nNote that in each case there is a vector and the loop goes through each cell in the vector. The i variable is an object that gets replaced with a new number in each iteration of the loop.\nLoops can be tricky, and the only way to really learn them is to practice as much as possible. Whenever you find yourself writing similar code more than 2 or 3 times, challenge yourself to try to re-write it as a loop.\nIn addition to looping through a vector, it can often be useful to include a counter variable. This can be especially useful for more complicated loops, but be careful to decide where in your code to update the counter variable. USUALLY it will be either\n\nAt the end, setting the initial value to 1 before the loop begins.\n\n\ncount1&lt;-1\ncount10&lt;-1\n\nfor(i in 1:5){\n  print(paste(\"count1 =\",count1))\n  print(paste(\"count10 =\",count10))\n  count1&lt;-count1+1\n  count10&lt;-count10*10\n}\n\n[1] \"count1 = 1\"\n[1] \"count10 = 1\"\n[1] \"count1 = 2\"\n[1] \"count10 = 10\"\n[1] \"count1 = 3\"\n[1] \"count10 = 100\"\n[1] \"count1 = 4\"\n[1] \"count10 = 1000\"\n[1] \"count1 = 5\"\n[1] \"count10 = 10000\"\n\n\n\nAt the start, setting the initial value to 0 before the loop begins.\n\n\ncountbefore&lt;-0\ncountafter&lt;-0\n\nfor(i in 1:5){\n  countbefore&lt;-countbefore+1\n  print(paste(\"before =\",countbefore))\n  print(paste(\"after =\",countafter))\n  countafter&lt;-countafter+1\n}\n\n[1] \"before = 1\"\n[1] \"after = 0\"\n[1] \"before = 2\"\n[1] \"after = 1\"\n[1] \"before = 3\"\n[1] \"after = 2\"\n[1] \"before = 4\"\n[1] \"after = 3\"\n[1] \"before = 5\"\n[1] \"after = 4\"\n\n\nThis is yet another example of how two different coding approaches can produce the same result.\nRead through the outputs above carefully to make sure you understand how the loops work. When you are confident you understand, then write a new loop and write down the predicted output. Run the loop to check if you were right.\n\n\n\nCounters are particularly valuable when you have a nested loop, which is just one loop inside of another. But note that this can complicate decisions about where to place your counter variable.\nIn the example below, we are first looping through a vector of length 3, tracked with i. Then for each i we do a second loop, tracked by j.\nThis time, try to predict the output BEFORE you run the loop. Write it down, then run the loop to check your answer.\n\nLoopCount&lt;-0\nfor(i in 1:3){\n  for(j in 1:2){\n    LoopCount&lt;-LoopCount+1\n    print(paste(\"Loop =\",LoopCount))\n    print(paste(\"i = \",i))\n    print(paste(\"j = \",j))\n  }\n}\n\n[1] \"Loop = 1\"\n[1] \"i =  1\"\n[1] \"j =  1\"\n[1] \"Loop = 2\"\n[1] \"i =  1\"\n[1] \"j =  2\"\n[1] \"Loop = 3\"\n[1] \"i =  2\"\n[1] \"j =  1\"\n[1] \"Loop = 4\"\n[1] \"i =  2\"\n[1] \"j =  2\"\n[1] \"Loop = 5\"\n[1] \"i =  3\"\n[1] \"j =  1\"\n[1] \"Loop = 6\"\n[1] \"i =  3\"\n[1] \"j =  2\"\n\n\n\n\n\nThe while function is another kind of loop, but instead of looping through a predefined set of variables, we iterate until some condition is met inside of the loop. This is called the exit condition.\nIn biology, the while loop is often used in optimization simulations, where many calculations are run until some optimum or threshold value is reached. Examples may include equilibrium simulations like the Evolutionarily Stable Strategy (ESS), population growth trajectories, or mutation-selection equilibrium. Other examples may include advanced statistical analyses based on maximizing the likelihood or fit of particular statistical models.\nOne common coding error associated with while loops is that the exit condition is never reached, causing your computer to run an infinite loop.\nHere’s a simple while loop, which will continue until count is greater than or equal to X.\n\ncount&lt;-0\nwhile(count &lt; X){\n  print(count)\n  count&lt;-count+1\n}\n\n[1] 0\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n[1] 11\n[1] 12\n[1] 13\n[1] 14\n[1] 15\n[1] 16\n[1] 17\n[1] 18\n[1] 19\n[1] 20\n\n\n\n\n\nEarlier in this chapter, we saw the modulo (%%) function, which returns the remainder of a division equation. This comes in handy for loops. For example, if you want to do something every 2, or 3, or N iterations of a loop, you can divide by N and determine if the dividend is zero. Here’s an example:\n\nfor(i in 1:9){\n  if(i %% 3 ==0 ){\n    print(paste(\"Iteration:\",i))\n  }\n}\n\n[1] \"Iteration: 3\"\n[1] \"Iteration: 6\"\n[1] \"Iteration: 9\"\n\n\n\nQuestion: What will the output look like?\n\nBefore you run the code, take time to work through the for loop and the if statement to predict the output. This will help you develop a better understanding of these tricky but highly useful coding tools.\n\n\n\nThere are many cases where you may want to add or change vectors, arrays, or data frame objects inside of a loop. By default, R will create a new object each time you add a new element. This can make loops very slow.\nSlow loops are usually fine, especially when you are starting out. Does it really matter if your code takes 0.2 seconds or 0.12 seconds? Even if it takes 5 minutes to run, that might be a better use of your time than spending 2 hours finding a faster version.\nHowever, as you advance to code larger projects, you will find that these time differences start to become important. There are some good tricks in R for faster.\nThis subsection is going to get advanced pretty quickly. Don’t worry if you struggle to understand it. Just give it a shot and if you need to move on, file it away for the future. Then, come back to this part of the book if find yourself struggling with loops that are taking too long to run.\nHere is a slow loop example demonstrating the central limit theorem. First, we want to sample 1000 numbers from a random normal distribution and calculate the average. Second, we want to repeat this process for 5000 iterations. Finally, we want to calculate the average of these 500 iterations. That is, the mean of means.\n\nIters&lt;-500 # Number of iterations\nOutVector&lt;-NA\nStart&lt;-Sys.time()\nfor(i in 1:Iters){\n  TempMean&lt;-NA\n  for(j in 1:1000){ # One loop per sample\n    TempMean[j]&lt;-rnorm(1)\n  }\n  OutVector[i]&lt;-mean(TempMean)\n}\nSys.time()-Start\n\nTime difference of 1.103925 secs\n\npaste(\"Mean of means =\",mean(OutVector))\n\n[1] \"Mean of means = -0.000636993195912495\"\n\n\nNote the use of Sys.time() to keep track of the loop run time. Also, note that the Time difference for your computer will depend on the specifications of your computer and memory use. This is a handy technique for large bioinfomatics projects. For example, imagine looping through millions or billions of DNA sequences. How long will the loop take? Try running it on a subset of a few thousand and then multiply to estimate the total run time.\nHere is an example of the same calculation in a fast loop. Note the only changes are the addition of the vector() functions.\n\nIters&lt;-500 # Number of iterations\nOutVector&lt;-vector(\"numeric\",Iters)\nStart&lt;-Sys.time()\nfor(i in 1:Iters){\n  TempMean&lt;-vector(\"numeric\",1000)\n  for(j in 1:100){ # One loop per sample\n    TempMean[j]&lt;-rnorm(1)\n  }\n  OutVector[i]&lt;-mean(TempMean)\n}\nSys.time()-Start\n\nTime difference of 0.1123781 secs\n\npaste(\"Mean of means =\",mean(OutVector))\n\n[1] \"Mean of means = 0.000421493909076394\"\n\n\nThe reason this is faster is a bit technical, but the key is that we are pre-defining the size of our output vectors before we run the loop. This allows R to assign an appropriate amount of computer memory to keep track of changes to the output vector. If we don’t do this, R has to constantly update the output memory by creating new objects each time.\nBut there is an even faster way to do the same calculation. Often if we are outputting to vectors in a for loop, then there is a way to re-write the same function by using sapply() or tapply().\nHere is an example of an even faster loop.\n\nIters&lt;-500 # Number of iterations\nOutVector&lt;-vector(\"numeric\",Iters)\nOutMean&lt;-function(x){\n  return(mean(rnorm(1000)))\n}\nStart&lt;-Sys.time()\nOutVector&lt;-sapply(OutVector,FUN=OutMean)\nSys.time()-Start\n\nTime difference of 0.02665401 secs\n\npaste(mean(OutVector))\n\n[1] \"0.000901234129891329\"\n\n\nWe’ve used three tricks here. First, we simply generate a vector of 1000 and take the mean with mean(rnorm(1000)), instead of making a nested loop with a vector to hold each of our 1000 random numbers. Second, we put this into a custom function called OutMean, which allows us to apply the same function along a vector. We’ll look at custom functions in more detail in a later chapter. Finally, we use the sapply function to apply our custom functionOutMean for each element of OutVector."
  },
  {
    "objectID": "flowcontrol.html#introduction",
    "href": "flowcontrol.html#introduction",
    "title": "Flow Control",
    "section": "",
    "text": "Think of your data analysis as a stream flowing from the raw data at the headwaters down to the river mouth, exiting as a full analysis with graphics, statistical analyses, and biological interpretation.\nThere are different ways we can control the flow of our code. The simplest is just to write a sequence of lines of code, with the output of one line of code forming the input of the next. A pseudo-code example might be:\nA&lt;-functionA()\nB&lt;-functionB(A)\nC&lt;-functionC(B)\nBut sometimes we may want to do the same function or analysis only if the input meets certain criteria. Or we may want to reiterate the same analysis multiple times on different inputs. This is where more advanced flow control comes in handy.\nTo start, let’s make up a couple of objects to play with:\n\nX&lt;-21\nXvec&lt;-c(1:10,\"string\")"
  },
  {
    "objectID": "flowcontrol.html#if",
    "href": "flowcontrol.html#if",
    "title": "Flow Control",
    "section": "",
    "text": "The if(){} statement uses an operator (see above) to asses whether the value is TRUE or FALSE:\n\nif(X &gt; 100){ # Is X greater than 100?\n  print(\"X &gt; 100\") # If TRUE\n} else { \n  print(\"X &lt;= 100\") # If FALSE\n}\n\n[1] \"X &lt;= 100\"\n\n\nA common ‘rookie’ mistake is to leave out a bracket or use the wrong type of bracket. Use regular brackets for the if function if() followed by two sets of curly brackets containing the code to run {run if true}else{run if false}.\nBreak up across multiple lines to improve readability. Note that you don’t need an else{} part if you just want to do nothing when FALSE.\n\nif(X &gt; 0){print (\"yup\")}\n\n[1] \"yup\""
  },
  {
    "objectID": "flowcontrol.html#ifelse",
    "href": "flowcontrol.html#ifelse",
    "title": "Flow Control",
    "section": "",
    "text": "The ifelse() is a more compact version for simple comparisons. The following code does the same as above.\n\nifelse(X &gt; 100,\"X &gt; 100\", \"X &lt;= 100\")\n\n[1] \"X &lt;= 100\""
  },
  {
    "objectID": "flowcontrol.html#nested-if",
    "href": "flowcontrol.html#nested-if",
    "title": "Flow Control",
    "section": "",
    "text": "You can also nest if and ifelse statements to account for more outcomes. Conceptually think of it as a bifurcating tree, starting at the top (root) and then splitting in two for every if statement.\n\nif(X &gt; 100){\n  print(\"X &gt; 100\")\n  if(X &gt; 200){\n    print(\"X &gt; 200\")\n  }\n} else {\n  if(X == 100){\n    print(\"X = 100\")\n  } else {\n    print(\"X &lt; 100\")\n  }\n}\n\n[1] \"X &lt; 100\"\n\n\nDon’t get intimidated. It just takes time to work through all of the possibilities. Try to draw a bifurcating diagram to represent each true/false outcome for the above code."
  },
  {
    "objectID": "flowcontrol.html#for-loop",
    "href": "flowcontrol.html#for-loop",
    "title": "Flow Control",
    "section": "",
    "text": "A loop does the same thing over and over again until some condition is met. In the case of a for loop, we set a ‘counter’ variable and loop through each value of the counter variable. Here are a few examples:\n\nLoop through numbers from 1 to 5\n\n\nfor (i in 1:5){\n  print(paste(X,i,sep=\":\"))\n}\n\n[1] \"21:1\"\n[1] \"21:2\"\n[1] \"21:3\"\n[1] \"21:4\"\n[1] \"21:5\"\n\n\n\nLoop through the elements of a vector directly\n\n\nfor (i in Xvec){\n  print(i)\n}\n\n[1] \"1\"\n[1] \"2\"\n[1] \"3\"\n[1] \"4\"\n[1] \"5\"\n[1] \"6\"\n[1] \"7\"\n[1] \"8\"\n[1] \"9\"\n[1] \"10\"\n[1] \"string\"\n\n\n\nUse an index object to index the elements of a vector\n\n\nfor (i in 1:length(Xvec)){\n  print(Xvec[i])\n}\n\n[1] \"1\"\n[1] \"2\"\n[1] \"3\"\n[1] \"4\"\n[1] \"5\"\n[1] \"6\"\n[1] \"7\"\n[1] \"8\"\n[1] \"9\"\n[1] \"10\"\n[1] \"string\"\n\n\nNote that in each case there is a vector and the loop goes through each cell in the vector. The i variable is an object that gets replaced with a new number in each iteration of the loop.\nLoops can be tricky, and the only way to really learn them is to practice as much as possible. Whenever you find yourself writing similar code more than 2 or 3 times, challenge yourself to try to re-write it as a loop.\nIn addition to looping through a vector, it can often be useful to include a counter variable. This can be especially useful for more complicated loops, but be careful to decide where in your code to update the counter variable. USUALLY it will be either\n\nAt the end, setting the initial value to 1 before the loop begins.\n\n\ncount1&lt;-1\ncount10&lt;-1\n\nfor(i in 1:5){\n  print(paste(\"count1 =\",count1))\n  print(paste(\"count10 =\",count10))\n  count1&lt;-count1+1\n  count10&lt;-count10*10\n}\n\n[1] \"count1 = 1\"\n[1] \"count10 = 1\"\n[1] \"count1 = 2\"\n[1] \"count10 = 10\"\n[1] \"count1 = 3\"\n[1] \"count10 = 100\"\n[1] \"count1 = 4\"\n[1] \"count10 = 1000\"\n[1] \"count1 = 5\"\n[1] \"count10 = 10000\"\n\n\n\nAt the start, setting the initial value to 0 before the loop begins.\n\n\ncountbefore&lt;-0\ncountafter&lt;-0\n\nfor(i in 1:5){\n  countbefore&lt;-countbefore+1\n  print(paste(\"before =\",countbefore))\n  print(paste(\"after =\",countafter))\n  countafter&lt;-countafter+1\n}\n\n[1] \"before = 1\"\n[1] \"after = 0\"\n[1] \"before = 2\"\n[1] \"after = 1\"\n[1] \"before = 3\"\n[1] \"after = 2\"\n[1] \"before = 4\"\n[1] \"after = 3\"\n[1] \"before = 5\"\n[1] \"after = 4\"\n\n\nThis is yet another example of how two different coding approaches can produce the same result.\nRead through the outputs above carefully to make sure you understand how the loops work. When you are confident you understand, then write a new loop and write down the predicted output. Run the loop to check if you were right."
  },
  {
    "objectID": "flowcontrol.html#nested-loops",
    "href": "flowcontrol.html#nested-loops",
    "title": "Flow Control",
    "section": "",
    "text": "Counters are particularly valuable when you have a nested loop, which is just one loop inside of another. But note that this can complicate decisions about where to place your counter variable.\nIn the example below, we are first looping through a vector of length 3, tracked with i. Then for each i we do a second loop, tracked by j.\nThis time, try to predict the output BEFORE you run the loop. Write it down, then run the loop to check your answer.\n\nLoopCount&lt;-0\nfor(i in 1:3){\n  for(j in 1:2){\n    LoopCount&lt;-LoopCount+1\n    print(paste(\"Loop =\",LoopCount))\n    print(paste(\"i = \",i))\n    print(paste(\"j = \",j))\n  }\n}\n\n[1] \"Loop = 1\"\n[1] \"i =  1\"\n[1] \"j =  1\"\n[1] \"Loop = 2\"\n[1] \"i =  1\"\n[1] \"j =  2\"\n[1] \"Loop = 3\"\n[1] \"i =  2\"\n[1] \"j =  1\"\n[1] \"Loop = 4\"\n[1] \"i =  2\"\n[1] \"j =  2\"\n[1] \"Loop = 5\"\n[1] \"i =  3\"\n[1] \"j =  1\"\n[1] \"Loop = 6\"\n[1] \"i =  3\"\n[1] \"j =  2\""
  },
  {
    "objectID": "flowcontrol.html#while-loop",
    "href": "flowcontrol.html#while-loop",
    "title": "Flow Control",
    "section": "",
    "text": "The while function is another kind of loop, but instead of looping through a predefined set of variables, we iterate until some condition is met inside of the loop. This is called the exit condition.\nIn biology, the while loop is often used in optimization simulations, where many calculations are run until some optimum or threshold value is reached. Examples may include equilibrium simulations like the Evolutionarily Stable Strategy (ESS), population growth trajectories, or mutation-selection equilibrium. Other examples may include advanced statistical analyses based on maximizing the likelihood or fit of particular statistical models.\nOne common coding error associated with while loops is that the exit condition is never reached, causing your computer to run an infinite loop.\nHere’s a simple while loop, which will continue until count is greater than or equal to X.\n\ncount&lt;-0\nwhile(count &lt; X){\n  print(count)\n  count&lt;-count+1\n}\n\n[1] 0\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n[1] 6\n[1] 7\n[1] 8\n[1] 9\n[1] 10\n[1] 11\n[1] 12\n[1] 13\n[1] 14\n[1] 15\n[1] 16\n[1] 17\n[1] 18\n[1] 19\n[1] 20"
  },
  {
    "objectID": "flowcontrol.html#modulo",
    "href": "flowcontrol.html#modulo",
    "title": "Flow Control",
    "section": "",
    "text": "Earlier in this chapter, we saw the modulo (%%) function, which returns the remainder of a division equation. This comes in handy for loops. For example, if you want to do something every 2, or 3, or N iterations of a loop, you can divide by N and determine if the dividend is zero. Here’s an example:\n\nfor(i in 1:9){\n  if(i %% 3 ==0 ){\n    print(paste(\"Iteration:\",i))\n  }\n}\n\n[1] \"Iteration: 3\"\n[1] \"Iteration: 6\"\n[1] \"Iteration: 9\"\n\n\n\nQuestion: What will the output look like?\n\nBefore you run the code, take time to work through the for loop and the if statement to predict the output. This will help you develop a better understanding of these tricky but highly useful coding tools."
  },
  {
    "objectID": "flowcontrol.html#faster-loops",
    "href": "flowcontrol.html#faster-loops",
    "title": "Flow Control",
    "section": "",
    "text": "There are many cases where you may want to add or change vectors, arrays, or data frame objects inside of a loop. By default, R will create a new object each time you add a new element. This can make loops very slow.\nSlow loops are usually fine, especially when you are starting out. Does it really matter if your code takes 0.2 seconds or 0.12 seconds? Even if it takes 5 minutes to run, that might be a better use of your time than spending 2 hours finding a faster version.\nHowever, as you advance to code larger projects, you will find that these time differences start to become important. There are some good tricks in R for faster.\nThis subsection is going to get advanced pretty quickly. Don’t worry if you struggle to understand it. Just give it a shot and if you need to move on, file it away for the future. Then, come back to this part of the book if find yourself struggling with loops that are taking too long to run.\nHere is a slow loop example demonstrating the central limit theorem. First, we want to sample 1000 numbers from a random normal distribution and calculate the average. Second, we want to repeat this process for 5000 iterations. Finally, we want to calculate the average of these 500 iterations. That is, the mean of means.\n\nIters&lt;-500 # Number of iterations\nOutVector&lt;-NA\nStart&lt;-Sys.time()\nfor(i in 1:Iters){\n  TempMean&lt;-NA\n  for(j in 1:1000){ # One loop per sample\n    TempMean[j]&lt;-rnorm(1)\n  }\n  OutVector[i]&lt;-mean(TempMean)\n}\nSys.time()-Start\n\nTime difference of 1.103925 secs\n\npaste(\"Mean of means =\",mean(OutVector))\n\n[1] \"Mean of means = -0.000636993195912495\"\n\n\nNote the use of Sys.time() to keep track of the loop run time. Also, note that the Time difference for your computer will depend on the specifications of your computer and memory use. This is a handy technique for large bioinfomatics projects. For example, imagine looping through millions or billions of DNA sequences. How long will the loop take? Try running it on a subset of a few thousand and then multiply to estimate the total run time.\nHere is an example of the same calculation in a fast loop. Note the only changes are the addition of the vector() functions.\n\nIters&lt;-500 # Number of iterations\nOutVector&lt;-vector(\"numeric\",Iters)\nStart&lt;-Sys.time()\nfor(i in 1:Iters){\n  TempMean&lt;-vector(\"numeric\",1000)\n  for(j in 1:100){ # One loop per sample\n    TempMean[j]&lt;-rnorm(1)\n  }\n  OutVector[i]&lt;-mean(TempMean)\n}\nSys.time()-Start\n\nTime difference of 0.1123781 secs\n\npaste(\"Mean of means =\",mean(OutVector))\n\n[1] \"Mean of means = 0.000421493909076394\"\n\n\nThe reason this is faster is a bit technical, but the key is that we are pre-defining the size of our output vectors before we run the loop. This allows R to assign an appropriate amount of computer memory to keep track of changes to the output vector. If we don’t do this, R has to constantly update the output memory by creating new objects each time.\nBut there is an even faster way to do the same calculation. Often if we are outputting to vectors in a for loop, then there is a way to re-write the same function by using sapply() or tapply().\nHere is an example of an even faster loop.\n\nIters&lt;-500 # Number of iterations\nOutVector&lt;-vector(\"numeric\",Iters)\nOutMean&lt;-function(x){\n  return(mean(rnorm(1000)))\n}\nStart&lt;-Sys.time()\nOutVector&lt;-sapply(OutVector,FUN=OutMean)\nSys.time()-Start\n\nTime difference of 0.02665401 secs\n\npaste(mean(OutVector))\n\n[1] \"0.000901234129891329\"\n\n\nWe’ve used three tricks here. First, we simply generate a vector of 1000 and take the mean with mean(rnorm(1000)), instead of making a nested loop with a vector to hold each of our 1000 random numbers. Second, we put this into a custom function called OutMean, which allows us to apply the same function along a vector. We’ll look at custom functions in more detail in a later chapter. Finally, we use the sapply function to apply our custom functionOutMean for each element of OutVector."
  },
  {
    "objectID": "github.html",
    "href": "github.html",
    "title": "Version Control",
    "section": "",
    "text": "Take a moment to reflect on everything you learned if you got this far! Congratulations, it really is impressive!\nNow it’s time to learn how to show off your skill to the rest of the world, keep track of important analyses, and collaborate with other R coders. In this chapter you’ll create and maintain your own code repository on a (free) website called GitHub. You’ll learn how to use GitHub with R Studio to create, modify, and maintain your code online.\nAs your coding skills continue to develop, repositories will become crucial for sharing and collaborating on projects. For example, the repository for your a thesis or peer-reviewed manuscript may include many different data sets, R programs, and R markdown documents. As your projects get more complicated with multiple collaborators, it can help to have Version Control to decide which changes to keep and which to exclude.\n\n\n\nA repository, sometimes shortened to just repo, is just a hard drive somewhere that stores your code. GitHub (https://GitHub.com) is a website that makes it easy to create and manage your repositories, and decide which ones to make available to the public. The word GitHub comes from a programming tool called git, which provides a protocol for communicating between your computer and your online repository.\nGit and GitHub are useful for sharing code and collaborating on projects. But you can use your GitHub site as an online CV or Resume to raise your profile and get noticed by potential employers. For example, here is a link to my GitHub site: https://github.com/colauttilab\nAnd here is an example of a public repository for a published paper on SARS-CoV-2 genome evolution during the early stages of the COVID-19 pandemic: https://github.com/ColauttiLab/SARS-CoV_Phylogenomics\nIf you go to the SARS-Cov-2 repository, you’ll see a list of folders and files that make up the GitHub repository. Think of this as your Project Folder in R. Later, we’ll see how to use special protocols to sync this folder to your computer, and decide which changes to keep or reject.\n\n\n\nTake note of one specific file, called Readme.md. All repositories should include a well-documented readme file.\nIf you scroll down, you’ll see a description of the analysis pipeline, including a flowchart diagram and some additional notes. This is all generated from the Readme.md file. The md stands for ‘markdown’ and it is very similar to the R markdown (.Rmd) format discussed in the first part of this book. That’s because R markdown is built on the markdown language. The main difference is that a regular markdown file won’t render R code. However, you can still use different text formatting (including a special font for code), and add images and hyperlinks.\n\n\n\nNow that you understand the basics, Make a (free) account at https://GitHub.com\nYou will need to choose a unique user name. Take some time to add a headshot or picture that represents you, and add some basic details to your profile. Just taking a few minutes to do this will help your account stand out from the many other generic accounts on GitHub. Remember that this is your public-facing profile that might be seen by potential collaborators, employers, or recruiters, so keep it professional.\n\n\n\nThe git program will help us communicate with GitHub to update our files, or search for updates submitted by other users. You can downoad it at http://git-scm.com/downloads\nBe sure to quit R and R Studio before you install git. After it is installed, open R Studio and choose Global Options from the Tools menu. Then, click on Git/SVN on the left and the Enable version control interface for RStudio projects at the top.\n\n\n\nGlobal Options Menu\n\n\nYou may also need to specify the location of your Git executable by clicking the Browse button. Here, you should link to the git (OSX) or git.exe (Windows) that you installed. If you have trouble locating the follow, try this\n\n(Windows): Search for ‘Command Prompt’ from your Start Menu and type: where git\n(Mac OS & Linux): Open the ‘Terminal’ window and type: which git\n\nThat’s it! Now you should be ready to set up your repository.\n\n\n\nopen a web browser and navigate to your GitHub website. It should be something like github.com/username where username is the GitHub user name you chose when you set up your account.\nClick on Repositories near the top of your github page, then click the green button that says New and give your repository a name that you can easily remember.\nAdd a short description, and select the options to add a README and .gitignore file.\nYou may select a license if you would like to protect your code. You should do this when writing original code, but for now we are just creating a repository to explore how GitHub works.\nFinally, click Create repository and copy the full web address for your repository. It should be a web link that starts with GitHub.com and end with .git. This is the link that R Studio will use to sync your GitHub repository with your local computer.\n\n\n\nCreate a new project in R Studio from the menu File--&gt;New Project but be sure to select Version Control and then click Git – note that it is written sideways with a red ‘i’ and green ‘t’.\nPaste the GitHub link into the Repository URL: field. This tells R Studio where to find the repository on GitHub.\n\nNote: If your repository URL didn’t copy properly, you can just add .git to the end of the web link for your repository. For example, if your repository is https://GitHub.com/username/FirstRepo then the link in R Studio should be https://GitHub.com/username/FirstRepo.git\n\nUse the Browse button to choose where you would like to save the project file on your local computer.\nYou can leave Project Directory Name alone so that the project folder name on your computer matches the repository name on GitHub. You can change it, but this may cause you confusion later."
  },
  {
    "objectID": "github.html#introduction",
    "href": "github.html#introduction",
    "title": "Version Control",
    "section": "",
    "text": "Take a moment to reflect on everything you learned if you got this far! Congratulations, it really is impressive!\nNow it’s time to learn how to show off your skill to the rest of the world, keep track of important analyses, and collaborate with other R coders. In this chapter you’ll create and maintain your own code repository on a (free) website called GitHub. You’ll learn how to use GitHub with R Studio to create, modify, and maintain your code online.\nAs your coding skills continue to develop, repositories will become crucial for sharing and collaborating on projects. For example, the repository for your a thesis or peer-reviewed manuscript may include many different data sets, R programs, and R markdown documents. As your projects get more complicated with multiple collaborators, it can help to have Version Control to decide which changes to keep and which to exclude."
  },
  {
    "objectID": "github.html#repository",
    "href": "github.html#repository",
    "title": "Version Control",
    "section": "",
    "text": "A repository, sometimes shortened to just repo, is just a hard drive somewhere that stores your code. GitHub (https://GitHub.com) is a website that makes it easy to create and manage your repositories, and decide which ones to make available to the public. The word GitHub comes from a programming tool called git, which provides a protocol for communicating between your computer and your online repository.\nGit and GitHub are useful for sharing code and collaborating on projects. But you can use your GitHub site as an online CV or Resume to raise your profile and get noticed by potential employers. For example, here is a link to my GitHub site: https://github.com/colauttilab\nAnd here is an example of a public repository for a published paper on SARS-CoV-2 genome evolution during the early stages of the COVID-19 pandemic: https://github.com/ColauttiLab/SARS-CoV_Phylogenomics\nIf you go to the SARS-Cov-2 repository, you’ll see a list of folders and files that make up the GitHub repository. Think of this as your Project Folder in R. Later, we’ll see how to use special protocols to sync this folder to your computer, and decide which changes to keep or reject."
  },
  {
    "objectID": "github.html#readme.md",
    "href": "github.html#readme.md",
    "title": "Version Control",
    "section": "",
    "text": "Take note of one specific file, called Readme.md. All repositories should include a well-documented readme file.\nIf you scroll down, you’ll see a description of the analysis pipeline, including a flowchart diagram and some additional notes. This is all generated from the Readme.md file. The md stands for ‘markdown’ and it is very similar to the R markdown (.Rmd) format discussed in the first part of this book. That’s because R markdown is built on the markdown language. The main difference is that a regular markdown file won’t render R code. However, you can still use different text formatting (including a special font for code), and add images and hyperlinks."
  },
  {
    "objectID": "github.html#github-account",
    "href": "github.html#github-account",
    "title": "Version Control",
    "section": "",
    "text": "Now that you understand the basics, Make a (free) account at https://GitHub.com\nYou will need to choose a unique user name. Take some time to add a headshot or picture that represents you, and add some basic details to your profile. Just taking a few minutes to do this will help your account stand out from the many other generic accounts on GitHub. Remember that this is your public-facing profile that might be seen by potential collaborators, employers, or recruiters, so keep it professional."
  },
  {
    "objectID": "github.html#install-git",
    "href": "github.html#install-git",
    "title": "Version Control",
    "section": "",
    "text": "The git program will help us communicate with GitHub to update our files, or search for updates submitted by other users. You can downoad it at http://git-scm.com/downloads\nBe sure to quit R and R Studio before you install git. After it is installed, open R Studio and choose Global Options from the Tools menu. Then, click on Git/SVN on the left and the Enable version control interface for RStudio projects at the top.\n\n\n\nGlobal Options Menu\n\n\nYou may also need to specify the location of your Git executable by clicking the Browse button. Here, you should link to the git (OSX) or git.exe (Windows) that you installed. If you have trouble locating the follow, try this\n\n(Windows): Search for ‘Command Prompt’ from your Start Menu and type: where git\n(Mac OS & Linux): Open the ‘Terminal’ window and type: which git\n\nThat’s it! Now you should be ready to set up your repository."
  },
  {
    "objectID": "github.html#step-1-github-repo",
    "href": "github.html#step-1-github-repo",
    "title": "Version Control",
    "section": "",
    "text": "open a web browser and navigate to your GitHub website. It should be something like github.com/username where username is the GitHub user name you chose when you set up your account.\nClick on Repositories near the top of your github page, then click the green button that says New and give your repository a name that you can easily remember.\nAdd a short description, and select the options to add a README and .gitignore file.\nYou may select a license if you would like to protect your code. You should do this when writing original code, but for now we are just creating a repository to explore how GitHub works.\nFinally, click Create repository and copy the full web address for your repository. It should be a web link that starts with GitHub.com and end with .git. This is the link that R Studio will use to sync your GitHub repository with your local computer."
  },
  {
    "objectID": "github.html#step-2-r-project",
    "href": "github.html#step-2-r-project",
    "title": "Version Control",
    "section": "",
    "text": "Create a new project in R Studio from the menu File--&gt;New Project but be sure to select Version Control and then click Git – note that it is written sideways with a red ‘i’ and green ‘t’.\nPaste the GitHub link into the Repository URL: field. This tells R Studio where to find the repository on GitHub.\n\nNote: If your repository URL didn’t copy properly, you can just add .git to the end of the web link for your repository. For example, if your repository is https://GitHub.com/username/FirstRepo then the link in R Studio should be https://GitHub.com/username/FirstRepo.git\n\nUse the Browse button to choose where you would like to save the project file on your local computer.\nYou can leave Project Directory Name alone so that the project folder name on your computer matches the repository name on GitHub. You can change it, but this may cause you confusion later."
  },
  {
    "objectID": "github.html#commit",
    "href": "github.html#commit",
    "title": "Version Control",
    "section": "Commit",
    "text": "Commit\nTo commit changes, click on the check box under the Staged column for the files you want to commit. In this case, there is just one. Then, click the Commit button in the menu bar of the Git window.\nA new window will pop up to show you the changes you have made. Additions are highlighted in green and deletions are highlighted in red. You also have to add a short note in the text box under Commit message to explain what changes were made. This can be anything and it doesn’t have to be long, but it will help you keep track of the different versions of your code.\nAfter reviewing the changes and adding a commit message, click the Commit button. You might choose to do this separately for different files, so that each one can have its own commit message. Finally, click the Push button to upload the changes to GitHub.\n\nNote: It can take a few minutes for the GitHub website to update these changes. You also might have to refresh your browser to see the changes."
  },
  {
    "objectID": "github.html#branches",
    "href": "github.html#branches",
    "title": "Version Control",
    "section": "Branches",
    "text": "Branches\nYou can think of a git repository as a sort of timeline, with each commit saved as a separate point along the timeline, or branch. You can see these past versions in R Studio with the History button in the Git tab.\nIf you find that you’ve made a fatal mistake, you revert back to a previous version along the branch.\nIf two or more people work on the same files, they create different branches that each have an origin and an endpoint. Changes in these branches can be merged back to the main (or sometimes master) branch.\nEach version has its own unique ID code, called a Secure Hash Algorithm (SHA) code. This can be helpful with reproducibility since your main branch can change, but each commit is unique. You can see SHA, commit notes, and the version author for each commit in the History window.\nAnd that’s really all you need to know. Like everything else in this book, it can take a bit of practice and a lot of mistakes to get a good understanding of git and GitHub. However, I’ve found it to be an invaluable tool and I use it regularly for my students’ theses and peer-reviewed manuscripts, as well as my ‘lab resourses’ website (Colauttilab.GitHub.io) and this book!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Book Info",
    "section": "",
    "text": "Book Info\nThis website contains the complete text of the second edition of R Crash Course for Biologists: An introduction to R for bioinformatics and biostatistics.\nThis is the first book in the Quantitative Biology Series, based on lecture from Dr. Robert I. Colautti given at Queen’s University in Canada (Lab website: https://EcoEvoGeno.org).\nThis text is offered free of charge, but please consider purchasing e-book and/or print versions of this textbook to support development of future content.\nIf you can’t afford any of the above, please consider donating your time to provide an honest review: colauttilab@gmail.com. Please let us know what you liked, what worked well, and anything that you think can be improved."
  },
  {
    "objectID": "matrices.html",
    "href": "matrices.html",
    "title": "Matrices",
    "section": "",
    "text": "In this chapter, we add a few more tools to your R coding toolkit. This may be your first exposure to multivariate methods, but they are widely used in the analysis of biological data. Some populat examples include community composition data (e.g., plants, animals, microbes), environmental characteristics (e.g., temperature, precipitation, salinity), phenotypes (e.g., height, weight, age), cell and molecular characteristics (e.g., transcriptome, proteome, metabolome). Biologically, these are very different datasets. However, from the R programming perspective they are all very similar because they can be represented by a two-dimensional matrix of rows and columns. These are examples of asymmetric matrices because the rows and columns are different.\nAsymmetric matrices are more typical of raw biological data, but they can also be used to calculate symmetric matrices in which rows and columns are the same. These matrices are symmetric because the same values are mirrored above and below the diagonal. Some popular symmetric matrices in biology are variance-covariance matrices, correlation matrices, distance (or dissimilarity) matrices, and similarity matrices. If you find this explanation hard to follow, don’t worry, we’ll cover an example in the next section and in future books including the R STATS Crash Course and the R Machine Learning Crash Course.\nMultivariate analyses are very useful for comparing samples and characteristics. For example, if we want to compare our samples to see which are similar to each other, then we set up a data frame with each row as a different sample and each column as a characteristic of the sample.\n\n\n\nWe will start with an example to illustrate the difference between symmetric and asymmetric matrices, and how they may be related. Imagine you have four samples and three characteristics. Choose your favourite biological example to motivate your understanding here – samples could be four individuals or tissues or habitats, and characteristics could be expression of three genes or three phenotypic measurements, or the abundances of three plants (or microbes).\n\n\n\nAsymDat&lt;-data.frame(Char1=c(1,2,1,5),\n                    Char2=c(2,4,1,9),\n                    Char3=c(3,9,4,16))\nrow.names(AsymDat)&lt;-c(\"Samp1\",\"Samp2\",\"Samp3\",\"Samp4\")\nprint(AsymDat)\n\n      Char1 Char2 Char3\nSamp1     1     2     3\nSamp2     2     4     9\nSamp3     1     1     4\nSamp4     5     9    16\n\n\nThis is an asymmetric matrix because the rows and columns are different. Each row is a unique sample and each column is a unique characteristic of the sample.\nFrom these data, we can calculate the variance of each characteristic across the four samples. For example, we can calculate the variance for the first characteristic as:\n\nvar(AsymDat$Char1)\n\n[1] 3.583333\n\n\nWe can also calculate the covariance between any pair of samples\n\ncov(AsymDat$Char1,AsymDat$Char2)\n\n[1] 6.666667\n\n\nWe can continue to calculate the variance for each characteristic and the covariance between each pair of characteristics to create a new matrix.\n\n\n\nA matrix is symmetric if its values are mirrored above and below the diagonal. Following along with the above example, we can create a new matrix called the variance-covariance matrix:\n\ncov(AsymDat)\n\n          Char1     Char2    Char3\nChar1  3.583333  6.666667 11.00000\nChar2  6.666667 12.666667 20.66667\nChar3 11.000000 20.666667 35.33333\n\n\nCompare these values with the individual examples above. The diagonal values are the variances of the characteristics (3.58 for Char1, 12.67 for Char2, and 35.33 for Char3). The off-diagonals are the covariances between each pair of characteristics (e.g., 6.67 for the covariance between Char1 and Char2). The matrix is symmetric because the values are mirrored along the diagonal. That is, the covariance between row 1 (Char1) and column 2 (Char2) is the same as the covariance between row 2 (Char2) and column 1 (Char1).\nThere are many other types of matrices, but once you realize that they are just representations of data, it helps to demystify more complex analyses. Matrices also have mathematical operations but these can be a bit counter-intuitive because there can be many ways to apply equations to two sets of numbers.\n\n\n\n\nR is pretty handy for matrix calculations that would be very time-consuming to do by hand or even in a spreadsheet program.\nAs an example, let’s create some numeric vectors that we can play with. First, a simple vector object called X containing the numbers 1 through 10.\n\nX&lt;-c(1:10)\nX\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nSecond, a vector called Y containing the numbers 0.5 to 5 in 0.5 increments. Note how we can do this using some simple math:\n\nY&lt;-c((1:10)*0.5)\nY\n\n [1] 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\n\n\nNote the extra brackets (1:10) to help us understand that each number in the vector 1 through 10 is multiplied by 0.5, not just the last number.\n\n\nProbably the most common calculation for these X and Y objects is just to cycle through each element of each vector and multiply them together. For example, if X is a vector of leaf length measurements and Y is a vector of leaf width measurements, then we might want to estimate leaf area by multiplying each length by its corresponding width.\nIn R we just use the standard multiplication operator * on a vector, just like we would do for two individual numbers.\n\nX * Y\n\n [1]  0.5  2.0  4.5  8.0 12.5 18.0 24.5 32.0 40.5 50.0\n\n\nAddition, subtraction, division, and exponents are similar.\n\nX + Y\n\n [1]  1.5  3.0  4.5  6.0  7.5  9.0 10.5 12.0 13.5 15.0\n\nX / Y\n\n [1] 2 2 2 2 2 2 2 2 2 2\n\nX ^ Y\n\n [1] 1.000000e+00 2.000000e+00 5.196152e+00 1.600000e+01 5.590170e+01\n [6] 2.160000e+02 9.074927e+02 4.096000e+03 1.968300e+04 1.000000e+05\n\n\nJust as we apply operators to vectors, we can also apply functions to vectors. When we do this, the same function is applied to each individual cell of the vector.\n\nlog(X)\n\n [1] 0.0000000 0.6931472 1.0986123 1.3862944 1.6094379 1.7917595 1.9459101\n [8] 2.0794415 2.1972246 2.3025851\n\nexp(Y)\n\n [1]   1.648721   2.718282   4.481689   7.389056  12.182494  20.085537\n [7]  33.115452  54.598150  90.017131 148.413159\n\n\n\n\n\nVectors, matrices, and higher-order arrays have multiple elements. Because of this, there are more than one ways to multiply the elements in one object with the elements in the other. This may seem a bit abstract but matrix multiplication has broad applications in biology, from gene expression and molecular biology to community ecology and image analysis.\nThere are more options than simply multiplying each corresponding element. For example, we can multiply each element in the vector X by each element in the vector Y. This will create a matrix. Let’s make an example with the first 4 elements of X and the first 3 elements of Y.\n\n\n\nIn the outer product we work across columns of the first object, multiplying by rows of the second object. It’s easier to understand by example:\n\nZ&lt;-X[1:4] %o% Y[1:3]\nZ\n\n     [,1] [,2] [,3]\n[1,]  0.5    1  1.5\n[2,]  1.0    2  3.0\n[3,]  1.5    3  4.5\n[4,]  2.0    4  6.0\n\n\nNote how the first column is each value of X (1-4) multiplied by the first value of Y (0.5), and the second column is multiplied by the second value of Y (1). Similarly, the first row is each value of Y multiplied by the first value of X (1), etc. What happens if we reverse the order?\n\nYoX&lt;-Y[1:3] %o% X[1:4]\nYoX\n\n     [,1] [,2] [,3] [,4]\n[1,]  0.5    1  1.5    2\n[2,]  1.0    2  3.0    4\n[3,]  1.5    3  4.5    6\n\n\n\nQuestion: How does the Z matrix object differ from YoX?\n\nAnswer: We have switched the rows and columns, which is called a transpose\n\n\n\nIn R, we can transpose matrices with the t() function\n\nt(YoX)\n\n     [,1] [,2] [,3]\n[1,]  0.5    1  1.5\n[2,]  1.0    2  3.0\n[3,]  1.5    3  4.5\n[4,]  2.0    4  6.0\n\n\nTo multiply two vectors together with the outer product, we arrange the first vector as rows, and the second vector as columns, and then multiply each pair of values together to fill in the matrix.\nWe can extend this to multiply two objects that are 2-dimensional matrices instead of 1-dimensional vectors. However, this gets tricky for the outer product because instead of generating a 2-D matrix from two 1-D vectors, we will generate a 4-D array from the outer product of two 2-D matrices.\n\n\n\nAnother way to multiply two vectors is with the dot product. To do this, we match the element of each row in the first object with each column in the second object, and sum them together: (e.g. X[1]*Y[1]+X[2]*Y[2]...).\nIt’s easy to extend from two vectors to two matrices, just by multiplying out elements in each row of the first object by elements in the second object.\n\nX %*% Y\n\n      [,1]\n[1,] 192.5\n\nsum(X*Y) == X %*% Y\n\n     [,1]\n[1,] TRUE\n\n\n\n\n\nThere are a few other important matrix operations that are useful for biological data and modelling/simulations. The cross-product is a complicated formula that is easy to calculate in R\n\ncrossprod(X[1:4],Z) # Cross product\n\n     [,1] [,2] [,3]\n[1,]   15   30   45\n\n\n\ncrossprod(Z) # Cross product of Z and t(Z)\n\n     [,1] [,2] [,3]\n[1,]  7.5   15 22.5\n[2,] 15.0   30 45.0\n[3,] 22.5   45 67.5\n\n\nThe Identity Matrix is a special matrix with 1 on the diagonal and 0 on the off-diagonal. We can create it with the diag() function\n\ndiag(4) # Identity matrix, 4x4 in this case\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    0    0    0\n[2,]    0    1    0    0\n[3,]    0    0    1    0\n[4,]    0    0    0    1\n\n\nWe can also use the diag() function on an existing matrix, to pull out all of the values on the diagonal, resulting in a vector\n\ndiag(Z) # Diagonal elements of Z \n\n[1] 0.5 2.0 4.5\n\n\nSome of these calculations can get a bit tricky – especially when we move to 2D matrices instead of vectors. You’ll want to consult or review a matrix algebra textbook if you are going to apply these, but that’s getting too advanced for this book. For now, the important thing is just to know that these options are available if you need them in the future.\n\n\n\n\n\nOperator\nName\n\n\n\n\n*\nMultiply elements\n\n\n%*%\nDot Product\n\n\n%o%\nOuter product\n\n\nt()\nTranspose\n\n\ncrossprod()\nCross-product\n\n\ndiag(4)\nIdentity of 4x4 matrix\n\n\ndiag(M)\nDiagonal elements of matrix M\n\n\n\n\n\n\n\n\nOne popular use-case for matrix calculation is the principal components analysis (PCA). The PCA is covered in much more detail in the PCA Chapter in the book R STATS Crash Course for Biologists.\nBriefly, PCA is a form of unsupervised machine learning. It uses matrix math to re-scale a bunch of correlated vectors (e.g. measurements) so that they can be mapped to an equal number of independent PC axes. For example, if you measure tail fin lengths and body lengths of 100 fish, then you can code the data as two vectors. These values will probably be correlated with bigger fish having bigger tails. We can re-scale these two dependent (i.e. correlated) vectors as two independent (i.e. uncorrelated) principal components.\n\n\n\nPCA of fish size\n\n\nIn the example shown in the figure, PC1 is a measure of fish AND tail size, whereas PC2 is a measure of tail fin size relative to body size.\nPCA and similar ordination methods are widely used in biology, from community ecology and microbiome studies to morphometrics, population genetics, metagenomics and gene expression. Of course there are many applications outside of biology too! For now, just know that it is easy to run a PCA using the princomp() function. In most cases, we would want to scale the vectors to have a mean of 0 and standard deviation of 1. Equivalently, we can use the cor=T parameter to use the correlation matrix in the calculations.\n\nprincomp(Z, cor=T) \n\nCall:\nprincomp(x = Z, cor = T)\n\nStandard deviations:\n      Comp.1       Comp.2       Comp.3 \n1.732051e+00 4.214685e-08 0.000000e+00 \n\n 3  variables and  4 observations.\n\n\n\n\n\n\nPro-tip: Many analysis functions in R output as lists, including some very useful functions like lm() for linear models, which are covered in the R STATS Crash Course for Biologists.\n\nFor example, a princomp output list contains several useful objects.\n\nprincomp(Z) \n\nCall:\nprincomp(x = Z)\n\nStandard deviations:\n      Comp.1       Comp.2       Comp.3 \n2.091650e+00 2.980232e-08 0.000000e+00 \n\n 3  variables and  4 observations.\n\nnames(princomp(Z))\n\n[1] \"sdev\"     \"loadings\" \"center\"   \"scale\"    \"n.obs\"    \"scores\"   \"call\"    \n\nprincomp(Z)$center\n\n[1] 1.25 2.50 3.75\n\nprincomp(Z)$scale\n\n[1] 1 1 1\n\n\nLook at the help ?princomp and scroll down to the Value subheading. Note how the subheadings correspond to names(princomp(z))? These values are stored as a list object with each element corresponding to a part of the list object denoted by $."
  },
  {
    "objectID": "matrices.html#introduction",
    "href": "matrices.html#introduction",
    "title": "Matrices",
    "section": "",
    "text": "In this chapter, we add a few more tools to your R coding toolkit. This may be your first exposure to multivariate methods, but they are widely used in the analysis of biological data. Some populat examples include community composition data (e.g., plants, animals, microbes), environmental characteristics (e.g., temperature, precipitation, salinity), phenotypes (e.g., height, weight, age), cell and molecular characteristics (e.g., transcriptome, proteome, metabolome). Biologically, these are very different datasets. However, from the R programming perspective they are all very similar because they can be represented by a two-dimensional matrix of rows and columns. These are examples of asymmetric matrices because the rows and columns are different.\nAsymmetric matrices are more typical of raw biological data, but they can also be used to calculate symmetric matrices in which rows and columns are the same. These matrices are symmetric because the same values are mirrored above and below the diagonal. Some popular symmetric matrices in biology are variance-covariance matrices, correlation matrices, distance (or dissimilarity) matrices, and similarity matrices. If you find this explanation hard to follow, don’t worry, we’ll cover an example in the next section and in future books including the R STATS Crash Course and the R Machine Learning Crash Course.\nMultivariate analyses are very useful for comparing samples and characteristics. For example, if we want to compare our samples to see which are similar to each other, then we set up a data frame with each row as a different sample and each column as a characteristic of the sample."
  },
  {
    "objectID": "matrices.html#matrix-examples",
    "href": "matrices.html#matrix-examples",
    "title": "Matrices",
    "section": "",
    "text": "We will start with an example to illustrate the difference between symmetric and asymmetric matrices, and how they may be related. Imagine you have four samples and three characteristics. Choose your favourite biological example to motivate your understanding here – samples could be four individuals or tissues or habitats, and characteristics could be expression of three genes or three phenotypic measurements, or the abundances of three plants (or microbes).\n\n\n\nAsymDat&lt;-data.frame(Char1=c(1,2,1,5),\n                    Char2=c(2,4,1,9),\n                    Char3=c(3,9,4,16))\nrow.names(AsymDat)&lt;-c(\"Samp1\",\"Samp2\",\"Samp3\",\"Samp4\")\nprint(AsymDat)\n\n      Char1 Char2 Char3\nSamp1     1     2     3\nSamp2     2     4     9\nSamp3     1     1     4\nSamp4     5     9    16\n\n\nThis is an asymmetric matrix because the rows and columns are different. Each row is a unique sample and each column is a unique characteristic of the sample.\nFrom these data, we can calculate the variance of each characteristic across the four samples. For example, we can calculate the variance for the first characteristic as:\n\nvar(AsymDat$Char1)\n\n[1] 3.583333\n\n\nWe can also calculate the covariance between any pair of samples\n\ncov(AsymDat$Char1,AsymDat$Char2)\n\n[1] 6.666667\n\n\nWe can continue to calculate the variance for each characteristic and the covariance between each pair of characteristics to create a new matrix.\n\n\n\nA matrix is symmetric if its values are mirrored above and below the diagonal. Following along with the above example, we can create a new matrix called the variance-covariance matrix:\n\ncov(AsymDat)\n\n          Char1     Char2    Char3\nChar1  3.583333  6.666667 11.00000\nChar2  6.666667 12.666667 20.66667\nChar3 11.000000 20.666667 35.33333\n\n\nCompare these values with the individual examples above. The diagonal values are the variances of the characteristics (3.58 for Char1, 12.67 for Char2, and 35.33 for Char3). The off-diagonals are the covariances between each pair of characteristics (e.g., 6.67 for the covariance between Char1 and Char2). The matrix is symmetric because the values are mirrored along the diagonal. That is, the covariance between row 1 (Char1) and column 2 (Char2) is the same as the covariance between row 2 (Char2) and column 1 (Char1).\nThere are many other types of matrices, but once you realize that they are just representations of data, it helps to demystify more complex analyses. Matrices also have mathematical operations but these can be a bit counter-intuitive because there can be many ways to apply equations to two sets of numbers."
  },
  {
    "objectID": "matrices.html#matrix-algebra",
    "href": "matrices.html#matrix-algebra",
    "title": "Matrices",
    "section": "",
    "text": "R is pretty handy for matrix calculations that would be very time-consuming to do by hand or even in a spreadsheet program.\nAs an example, let’s create some numeric vectors that we can play with. First, a simple vector object called X containing the numbers 1 through 10.\n\nX&lt;-c(1:10)\nX\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nSecond, a vector called Y containing the numbers 0.5 to 5 in 0.5 increments. Note how we can do this using some simple math:\n\nY&lt;-c((1:10)*0.5)\nY\n\n [1] 0.5 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0\n\n\nNote the extra brackets (1:10) to help us understand that each number in the vector 1 through 10 is multiplied by 0.5, not just the last number.\n\n\nProbably the most common calculation for these X and Y objects is just to cycle through each element of each vector and multiply them together. For example, if X is a vector of leaf length measurements and Y is a vector of leaf width measurements, then we might want to estimate leaf area by multiplying each length by its corresponding width.\nIn R we just use the standard multiplication operator * on a vector, just like we would do for two individual numbers.\n\nX * Y\n\n [1]  0.5  2.0  4.5  8.0 12.5 18.0 24.5 32.0 40.5 50.0\n\n\nAddition, subtraction, division, and exponents are similar.\n\nX + Y\n\n [1]  1.5  3.0  4.5  6.0  7.5  9.0 10.5 12.0 13.5 15.0\n\nX / Y\n\n [1] 2 2 2 2 2 2 2 2 2 2\n\nX ^ Y\n\n [1] 1.000000e+00 2.000000e+00 5.196152e+00 1.600000e+01 5.590170e+01\n [6] 2.160000e+02 9.074927e+02 4.096000e+03 1.968300e+04 1.000000e+05\n\n\nJust as we apply operators to vectors, we can also apply functions to vectors. When we do this, the same function is applied to each individual cell of the vector.\n\nlog(X)\n\n [1] 0.0000000 0.6931472 1.0986123 1.3862944 1.6094379 1.7917595 1.9459101\n [8] 2.0794415 2.1972246 2.3025851\n\nexp(Y)\n\n [1]   1.648721   2.718282   4.481689   7.389056  12.182494  20.085537\n [7]  33.115452  54.598150  90.017131 148.413159\n\n\n\n\n\nVectors, matrices, and higher-order arrays have multiple elements. Because of this, there are more than one ways to multiply the elements in one object with the elements in the other. This may seem a bit abstract but matrix multiplication has broad applications in biology, from gene expression and molecular biology to community ecology and image analysis.\nThere are more options than simply multiplying each corresponding element. For example, we can multiply each element in the vector X by each element in the vector Y. This will create a matrix. Let’s make an example with the first 4 elements of X and the first 3 elements of Y.\n\n\n\nIn the outer product we work across columns of the first object, multiplying by rows of the second object. It’s easier to understand by example:\n\nZ&lt;-X[1:4] %o% Y[1:3]\nZ\n\n     [,1] [,2] [,3]\n[1,]  0.5    1  1.5\n[2,]  1.0    2  3.0\n[3,]  1.5    3  4.5\n[4,]  2.0    4  6.0\n\n\nNote how the first column is each value of X (1-4) multiplied by the first value of Y (0.5), and the second column is multiplied by the second value of Y (1). Similarly, the first row is each value of Y multiplied by the first value of X (1), etc. What happens if we reverse the order?\n\nYoX&lt;-Y[1:3] %o% X[1:4]\nYoX\n\n     [,1] [,2] [,3] [,4]\n[1,]  0.5    1  1.5    2\n[2,]  1.0    2  3.0    4\n[3,]  1.5    3  4.5    6\n\n\n\nQuestion: How does the Z matrix object differ from YoX?\n\nAnswer: We have switched the rows and columns, which is called a transpose\n\n\n\nIn R, we can transpose matrices with the t() function\n\nt(YoX)\n\n     [,1] [,2] [,3]\n[1,]  0.5    1  1.5\n[2,]  1.0    2  3.0\n[3,]  1.5    3  4.5\n[4,]  2.0    4  6.0\n\n\nTo multiply two vectors together with the outer product, we arrange the first vector as rows, and the second vector as columns, and then multiply each pair of values together to fill in the matrix.\nWe can extend this to multiply two objects that are 2-dimensional matrices instead of 1-dimensional vectors. However, this gets tricky for the outer product because instead of generating a 2-D matrix from two 1-D vectors, we will generate a 4-D array from the outer product of two 2-D matrices.\n\n\n\nAnother way to multiply two vectors is with the dot product. To do this, we match the element of each row in the first object with each column in the second object, and sum them together: (e.g. X[1]*Y[1]+X[2]*Y[2]...).\nIt’s easy to extend from two vectors to two matrices, just by multiplying out elements in each row of the first object by elements in the second object.\n\nX %*% Y\n\n      [,1]\n[1,] 192.5\n\nsum(X*Y) == X %*% Y\n\n     [,1]\n[1,] TRUE\n\n\n\n\n\nThere are a few other important matrix operations that are useful for biological data and modelling/simulations. The cross-product is a complicated formula that is easy to calculate in R\n\ncrossprod(X[1:4],Z) # Cross product\n\n     [,1] [,2] [,3]\n[1,]   15   30   45\n\n\n\ncrossprod(Z) # Cross product of Z and t(Z)\n\n     [,1] [,2] [,3]\n[1,]  7.5   15 22.5\n[2,] 15.0   30 45.0\n[3,] 22.5   45 67.5\n\n\nThe Identity Matrix is a special matrix with 1 on the diagonal and 0 on the off-diagonal. We can create it with the diag() function\n\ndiag(4) # Identity matrix, 4x4 in this case\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    0    0    0\n[2,]    0    1    0    0\n[3,]    0    0    1    0\n[4,]    0    0    0    1\n\n\nWe can also use the diag() function on an existing matrix, to pull out all of the values on the diagonal, resulting in a vector\n\ndiag(Z) # Diagonal elements of Z \n\n[1] 0.5 2.0 4.5\n\n\nSome of these calculations can get a bit tricky – especially when we move to 2D matrices instead of vectors. You’ll want to consult or review a matrix algebra textbook if you are going to apply these, but that’s getting too advanced for this book. For now, the important thing is just to know that these options are available if you need them in the future.\n\n\n\n\n\nOperator\nName\n\n\n\n\n*\nMultiply elements\n\n\n%*%\nDot Product\n\n\n%o%\nOuter product\n\n\nt()\nTranspose\n\n\ncrossprod()\nCross-product\n\n\ndiag(4)\nIdentity of 4x4 matrix\n\n\ndiag(M)\nDiagonal elements of matrix M"
  },
  {
    "objectID": "matrices.html#pca",
    "href": "matrices.html#pca",
    "title": "Matrices",
    "section": "",
    "text": "One popular use-case for matrix calculation is the principal components analysis (PCA). The PCA is covered in much more detail in the PCA Chapter in the book R STATS Crash Course for Biologists.\nBriefly, PCA is a form of unsupervised machine learning. It uses matrix math to re-scale a bunch of correlated vectors (e.g. measurements) so that they can be mapped to an equal number of independent PC axes. For example, if you measure tail fin lengths and body lengths of 100 fish, then you can code the data as two vectors. These values will probably be correlated with bigger fish having bigger tails. We can re-scale these two dependent (i.e. correlated) vectors as two independent (i.e. uncorrelated) principal components.\n\n\n\nPCA of fish size\n\n\nIn the example shown in the figure, PC1 is a measure of fish AND tail size, whereas PC2 is a measure of tail fin size relative to body size.\nPCA and similar ordination methods are widely used in biology, from community ecology and microbiome studies to morphometrics, population genetics, metagenomics and gene expression. Of course there are many applications outside of biology too! For now, just know that it is easy to run a PCA using the princomp() function. In most cases, we would want to scale the vectors to have a mean of 0 and standard deviation of 1. Equivalently, we can use the cor=T parameter to use the correlation matrix in the calculations.\n\nprincomp(Z, cor=T) \n\nCall:\nprincomp(x = Z, cor = T)\n\nStandard deviations:\n      Comp.1       Comp.2       Comp.3 \n1.732051e+00 4.214685e-08 0.000000e+00 \n\n 3  variables and  4 observations."
  },
  {
    "objectID": "matrices.html#list-output",
    "href": "matrices.html#list-output",
    "title": "Matrices",
    "section": "",
    "text": "Pro-tip: Many analysis functions in R output as lists, including some very useful functions like lm() for linear models, which are covered in the R STATS Crash Course for Biologists.\n\nFor example, a princomp output list contains several useful objects.\n\nprincomp(Z) \n\nCall:\nprincomp(x = Z)\n\nStandard deviations:\n      Comp.1       Comp.2       Comp.3 \n2.091650e+00 2.980232e-08 0.000000e+00 \n\n 3  variables and  4 observations.\n\nnames(princomp(Z))\n\n[1] \"sdev\"     \"loadings\" \"center\"   \"scale\"    \"n.obs\"    \"scores\"   \"call\"    \n\nprincomp(Z)$center\n\n[1] 1.25 2.50 3.75\n\nprincomp(Z)$scale\n\n[1] 1 1 1\n\n\nLook at the help ?princomp and scroll down to the Value subheading. Note how the subheadings correspond to names(princomp(z))? These values are stored as a list object with each element corresponding to a part of the list object denoted by $."
  },
  {
    "objectID": "Notes.html",
    "href": "Notes.html",
    "title": "NOTES:",
    "section": "",
    "text": "!!Disable Dropbox!! It interferes with permissions when knitting books\n\n\nhttps://quarto.org/docs/reference/formats/pdf.html\n\n\n\nhttps://quarto.org/docs/books/\n\nBook Structure delves into different ways to structure a book (numbered and unnumbered chapters/sections, creating multiple parts, adding appendices, etc.)\nBook Crossrefs explains how to create cross references to sections, figures, tables, equations and more within books.\nBook Output covers customizing the style and appearance of your book in the various output format as well as how to provide navigation and other tools for readers.\nBook Options provides a comprehensive reference to all of the available book options.\nCode Execution provides tips for optimizing the rendering of books with large numbers of documents or expensive computations.\nPublishing Websites enumerates the various options for publishing your book as a website including GitHub Pages, Netlify, and RStudio Connect.\n\n\n\n\n\n\n\nuse system fonts in brackets, e.g.: pdf: mainfont: “Century Schoolbook” monofont: “Consolas” sansfont: “Arial Black” mathfont: “Lucida Bright”\n\n\n\n\n\n\n\npdf: documentclass: scrreprt include-in-header: text: |\ninclude-after-body: text: | \nBUT: Requires adding after every word to index :(\n\n\n\nFix an issue with section numbers overlapping section titles in the table of contents.\npdf: include-in-header: text: |\n\n\n\nremove headers/footers from pages between chapters by adding this text to the include-in-header command:"
  },
  {
    "objectID": "Notes.html#quarto-pdf-options",
    "href": "Notes.html#quarto-pdf-options",
    "title": "NOTES:",
    "section": "",
    "text": "https://quarto.org/docs/reference/formats/pdf.html"
  },
  {
    "objectID": "Notes.html#quarto-book-instructions",
    "href": "Notes.html#quarto-book-instructions",
    "title": "NOTES:",
    "section": "",
    "text": "https://quarto.org/docs/books/\n\nBook Structure delves into different ways to structure a book (numbered and unnumbered chapters/sections, creating multiple parts, adding appendices, etc.)\nBook Crossrefs explains how to create cross references to sections, figures, tables, equations and more within books.\nBook Output covers customizing the style and appearance of your book in the various output format as well as how to provide navigation and other tools for readers.\nBook Options provides a comprehensive reference to all of the available book options.\nCode Execution provides tips for optimizing the rendering of books with large numbers of documents or expensive computations.\nPublishing Websites enumerates the various options for publishing your book as a website including GitHub Pages, Netlify, and RStudio Connect."
  },
  {
    "objectID": "Notes.html#pandoc-formatting-options-httpspandoc.orgmanual.htmltemplate-syntax",
    "href": "Notes.html#pandoc-formatting-options-httpspandoc.orgmanual.htmltemplate-syntax",
    "title": "NOTES:",
    "section": "",
    "text": "use system fonts in brackets, e.g.: pdf: mainfont: “Century Schoolbook” monofont: “Consolas” sansfont: “Arial Black” mathfont: “Lucida Bright”\n\n\n\n\n\n\n\npdf: documentclass: scrreprt include-in-header: text: |\ninclude-after-body: text: | \nBUT: Requires adding after every word to index :(\n\n\n\nFix an issue with section numbers overlapping section titles in the table of contents.\npdf: include-in-header: text: |\n\n\n\nremove headers/footers from pages between chapters by adding this text to the include-in-header command:"
  },
  {
    "objectID": "regex.html",
    "href": "regex.html",
    "title": "Regular Expressions I",
    "section": "",
    "text": "Regular Expressions, also known as regex and regexp are special text-based functions that act as run complex find-and-replace functions. I didn’t learn regular expressions until I was a postdoc working at Duke University, but I wish I had learned about them much earlier! This remains one of the most useful programming tools I have ever used. It is absolutely essential for working with any kind of large text files or large data sets. I’ll explain.\nA lot of programming tools in biology use input text files that require very specific formatting (e.g. .txt, .csv, .fasta, .nex). Sometimes, you might need to reorganize or recode data in a large text file or in many separate text files. This can be a big time sink, it can introduce errors, and it’s not reproducible if you do it manually. But regular expressions can automate the process.\nHere’s one example. As a PhD student I co-founded a project called the Global Garlic Mustard Field Survey (GGMFS) with collaborator Dr. Oliver Bossdorf at the University of Tübingen – yes the same Dr. Bossdorf mentioned in the Quick Visualizations Chapter. We were fortunate to have over 100 collaborators across Europe and North America who helped to collect samples for the project. Details of the project were published in the Journal Neobiota: https://neobiota.pensoft.net/article/1270/ but one BIG problem is the way that each of these 100+ collaborators entered their data online. For example, latitudes and longitudes were entered in a variety of different formats. Regular expressions allowed me to write a small program to automatically convert all of these different formats to a common, decimal format that we could use for the analysis. This saved a huge amount of time and prevented errors that could have been introduced if we tried to edit these values by hand.\nOften when you work with large datasets, you will need to automate some of your error correction, and regular expressions can be a big help here. For example, imagine a simple online survey that includes a place for people to simply type “yes” or “no” in response to a question. This should be coded as a binary variable (1 or 0) for analysis, but you might find a variety of inputs such as: “YES”, “Y”, “yes”, and “Yes”. These all mean the same thing, yet if you try to analyze the raw output, R will treat these as different categories. Here again, regular expressions can be used to quickly change all the different examples to a common “Y” or to a Boolean variable TRUE.\nOne final example, is pattern matching, which is common for the analysis of DNA, proteins or other large strings of data. You may want to find a particular sequence of data, possibly with a few variable sites: e.g. TCTA or TCAA or TCGA. This is another area where regular expressions can help.\n\n\nRegular expressions are a universal language that extends to many other programming languages, including C/C#/C++, Python, Unix/Linux, and Perl. We focus here on R but most of the syntax is mantained across programming languages.\n\n\n\nWARNING! There is a very steep learning curve here, and the only way to really learn this is to drown yourself in examples. There are lots of exercises you can do for practice online. You should also try to apply these whenever you can, just like you should with all of your other R skills.\n\n\n\n\nThere are four main functions that use regular expressions in R.\ngrep() and grepl() are equivalent to ‘find’ in your favorite word processor. They have the general form:\n\ngsub(\"find\", in.this.object)\n\ngrep() outputs a vector with all of the address locations (i.e. numbers) that match. Thus the output length is equal to the number of matches.\ngrepl() outputs a vector of TRUE (match) and FALSE (no match). Thus, the output length is equal to the length of the input object.\nsub() and gsub() are equivalent to ‘find and replace’. They have the general form:\n\ngrep(\"find\", \"replace\", in.this.object)\n\nsub() replaces only the first match, whereas gsub() replaces all of the matches.\nSome specific examples are provided below to help you understand these similarities and differences. As always, you should take the time to try these out and make sure you get the same input. If you don’t, then it’s a good learning opportunity to find out what you did differently!\nThere are two other more advanced functions in R. These aren’t covered in this tutorial, but may be of use once you are more comfortable with the above functions.\nregexpr() provides more detailed information about the first match.\ngregexpr() provides more detailed results about all matches.\n\nSee ?regexpr and ?gregexpr for more info\n\n\n\nSome examples can help to understand the differences among the four main functions. Let’s start with a simple data frame of species names.\n\nSpecies&lt;-c(\"petiolata\", \"verticillatus\", \"salicaria\", \"minor\")\nprint(Species)\n\n[1] \"petiolata\"     \"verticillatus\" \"salicaria\"     \"minor\"        \n\n\n\n\n\nThis returns cell addresses matching the query string.\n\ngrep(\"a\",Species)\n\n[1] 1 2 3\n\n\nNote the vector length compared to the input vector. Instead of the cell number, we can get R to return the specific values in each matching cell with the value=T parameter\n\ngrep(\"a\",Species, value=T)\n\n[1] \"petiolata\"     \"verticillatus\" \"salicaria\"    \n\n\n\n\n\nThis returns a vector of TRUE (match) and FALSE (no match). Compare this output with the same parameters in the grep() function.\n\ngrepl(\"a\",Species)\n\n[1]  TRUE  TRUE  TRUE FALSE\n\n\n\n\n\nThis replaces the first match (in each cell)\n\nsub(\"l\",\"L\",Species)\n\n[1] \"petioLata\"     \"verticiLlatus\" \"saLicaria\"     \"minor\"        \n\n\n\n\n\nThis replaces all matches (in each cell). Compare this output to sub().\n\ngsub(\"l\",\"L\",Species)\n\n[1] \"petioLata\"     \"verticiLLatus\" \"saLicaria\"     \"minor\"        \n\n\n\nQuestion: Did you see the difference?\n\nHint: Look at “Verticillatus”.\n\n\n\n\n\n\nThe backslash is a special character. It’s called the ‘escape’ character because it is used to escape from the literal interpretation of the next character to the right. For example, \\. applies the escape to the period character. The specific meaning depends on the context, which is much easier to understand by examples, as shown below.\n\n\n\nIn the introduction, we discussed the universality of regular expressions in the sense that a similar syntax is used by many different programming langagues. But now here is one exception. In R, the double-escape is usually needed, whereas other programming languages typically use just one. The reason is a bit meta – it’s because we are running regular expressions within R object. So the first \\ is used to escape special characters in R, applying it to the second \\, which is itself the special character that needs to be escaped to pass through the function. The second slash is followed by the ‘escaped’ character. Some examples are provided below.\nIf that isn’t clear. Just remember that you need two backslashes when writing regular experssions in R, but just one backslash for most other languages.\n\n\n\nInstead of finding the letter w, the \\\\w is a wildcard character that represents any letter or digit. It also includes underscore _ for some reason.\n\nsub(\"w\",\"X\",\"...which 1-100 words get replaced?\")\n\n[1] \"...Xhich 1-100 words get replaced?\"\n\ngsub(\"w\",\"X\",\"...which 1-100 words get replaced?\")\n\n[1] \"...Xhich 1-100 Xords get replaced?\"\n\nsub(\"\\\\w\",\"X\",\"...which 1-100 words get replaced?\")\n\n[1] \"...Xhich 1-100 words get replaced?\"\n\ngsub(\"\\\\w\",\"X\",\"...which 1-100 words get replaced?\")\n\n[1] \"...XXXXX X-XXX XXXXX XXX XXXXXXXX?\"\n\n\nAgain, note the differences between the sub() and gsum() functions. We’ll stick to gsub() for the remainder of the examples in this chapter, but you should also run sub() yourself. Each time, take a moment to try to predict how the output will differ before running it. This will help you develop an understanding of regular expressions much more quickly.\n\n\n\nThe capital W is the inverse of \\\\w find a character that is NOT a letter or number.\n\ngsub(\"\\\\W\",\"X\",\"...which 1-100  words get replaced?\")\n\n[1] \"XXXwhichX1X100XXwordsXgetXreplacedX\"\n\n\n\n\n\nThis represents a space\n\ngsub(\"\\\\s\",\"X\",\"...which 1-100  words get replaced?\")\n\n[1] \"...whichX1-100XXwordsXgetXreplaced?\"\n\n\n\n\n\nThis is a tab character. A lot of data files stored as text are tab-delimited (.tsv) as well as comma-delimited (.csv)\n\ngsub(\"\\\\t\",\"X\",\"...which 1-100 \\t words get replaced?\")\n\n[1] \"...which 1-100 X words get replaced?\"\n\n\nRemember that \\t is a tab character.\n\ncat(\"A\\t\\t\\tB C\")\n\nA           B C\n\n\n\n\n\nd for digits. This is the wild card for numeric characters.\n\ngsub(\"\\\\d\",\"X\",\"...which 1-100  words get replaced?\")\n\n[1] \"...which X-XXX  words get replaced?\"\n\n\n\n\n\nNon-digit characters\n\ngsub(\"\\\\D\",\"X\",\"...which 1-100  words get replaced?\")\n\n[1] \"XXXXXXXXX1X100XXXXXXXXXXXXXXXXXXXXX\"\n\n\n\n\n\n\nThere are two special characters that indicate new lines in a text file.\n\n\nThis is the ‘carriage return’ special character\n\n\n\nThis is the ‘newline’ special character\n\n\n\nOne or both of these may be generated when you press the ‘enter’ key while writing a text file. The difference depends on which operating system you are using. These also add a source of headache and confusion when working with text files because:\n\nUnix and MacOS text files use lines that end with \\n only\nWindows and DOS text files use lines end with \\r\\n\n\n\nQuestion: Do you know how this difference originated?\n\nAnswer: The reason goes back to the early days of programming, when programmers were moving from mechanical typewriters to computer programs. Mechanical typewriters are hard to find these days, but they would hold a piece of paper in place on a cylinder called a carriage. The\nThe \\n stands for ‘new line’, and the \\r stands for return. When you reach the end of a line of text on a typewriter, you would typically return the carriage back to the starting position, and then move to the next line, thus the \\r\\n. The Unix operating system decided that the \\r wasn’t needed, whereas the DOS operating system decided to include it.\nThis difference can cause problems when moving text files across operating systems. Programs like FileZilla will automatically translate these end-of-line characters when moving across systems.\n\n\n\n\nIn addition to special characters that use the escape \\\\, there are a number of other special characters that don’t use the escape, but have a special meaning.\nNote that if you want to search for the characters below you would have to use the escape character. E.g., use \\\\. to search for the period character (.).\n\n\nThe period is a wild card that means ‘anything’. This includes all of the \\\\w characters but also other characters like puncutation marks.\n\ngsub(\".\",\"X\",\"...which 1-100  words get replaced?\")\n\n[1] \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n\n\nSo how to search for a period .? As noted above, we have to use the escape character\n\ngsub(\"\\\\.\",\"X\",\"...which 1-100  words get replaced?\")\n\n[1] \"XXXwhich 1-100  words get replaced?\"\n\n\n\n\n\nThis is sometimes called the pipe character, and it simply means ‘or’. For example, we can search for w or e.\n\ngsub(\"w|e\",\"X\",\"...which 1-100  words get replaced?\")\n\n[1] \"...Xhich 1-100  Xords gXt rXplacXd?\"\n\n\n\n\n\nThese special characters refer to details about the kind of search that we are trying to conduct. Look at these examples carefully, and remember that sub replaces the first match while gsub replaces all of the matches.\n\nsub(\"\\\\w\",\"X\",\"...which 1-100 words get replaced?\")\n\n[1] \"...Xhich 1-100 words get replaced?\"\n\ngsub(\"\\\\w\",\"X\",\"...which 1-100 words get replaced?\")\n\n[1] \"...XXXXX X-XXX XXXXX XXX XXXXXXXX?\"\n\n\nNow let’s apply some of these special characters to see how they work.\n\n\n\nFinds ‘one or more’ matches (i.e. at least one match)\n\nsub(\"\\\\w+\",\"X\",\"...which 1-100 words get replaced?\")\n\n[1] \"...X 1-100 words get replaced?\"\n\ngsub(\"\\\\w+\",\"X\",\"...which 1-100 words get replaced?\")\n\n[1] \"...X X-X X X X?\"\n\n\nCompare this match to the one above. Notice how we have replaced groups of letters instead of single letters. The algorithm works like this:\n\nStart at the left and move to the right, one character at a time\nCheck if the character is a letter or number (\\\\w).\nIf NO, move to the next character\nIf YES, check the next character. If it is also a \\\\w then go to the next character. Repeat until the next character is not \\\\w, and replace the entire string of characters.\n\nWhen run in the sub() function, the algorithm does the above and then stops. When run with the gsub() function, it continues to the next character, and then starts over.\n\n\n\nThis is a greedy search matches 0 or more in a row. Again, this is easier to understand by exploring examples.\n\nsub(\"\\\\w*\",\"X\",\"...which 1-100 words get replaced?\")\n\n[1] \"X...which 1-100 words get replaced?\"\n\ngsub(\"\\\\w*\",\"X\",\"...which 1-100 words get replaced?\")\n\n[1] \"X.X.X.X X-X X X X?X\"\n\n\nIn the sub() function, it detects a period (.) as the first character, indicating no match. It replaces the ‘null’ or 0 match at the beginning, which has the effect of adding a character. In the gsub() function it repeats this again before each period (.). It then continues until it finds the letter w. Then it finds a group of \\\\w matches, replacing all of them with a single X. Then a space, which is skipped, then a -, which is another null match, prompting another insert.\n\n\n\nThis is the restrained search, which matches zero or one time.\n\nsub(\"\\\\w?\",\"X\",\"...which 1-100 words get replaced?\")\n\n[1] \"X...which 1-100 words get replaced?\"\n\ngsub(\"\\\\w?\",\"X\",\"...which 1-100 words get replaced?\")\n\n[1] \"X.X.X.XXXXX X-XXX XXXXX XXX XXXXXXXX?X\"\n\n\nCompare this to the * above. The ? character behaves in a similar way, except it is constrained in the sense that each each letter is replaced individually, instead of replacing entire words.\n\n\n\nThis is the lazy version of +\n\nsub(\"\\\\w+?\",\"X\",\"...which 1-100 words get replaced?\")\n\n[1] \"...Xhich 1-100 words get replaced?\"\n\ngsub(\"\\\\w+?\",\"X\",\"...which 1-100 words get replaced?\")\n\n[1] \"...XXXXX X-XXX XXXXX XXX XXXXXXXX?\"\n\n\nNote the difference in sub(), which replaces on the the first letter here but the whole word when + is used alone in the earlier example. In the gsub() example we end up replacing every letter instead of whole words. Remember, sub() runs the algorithm once and then stops, while gsub() cycles through the algorithm until it reaches the end of the line.\n\n\n\nSimilarly, we can combine these characters for the ‘lazy’ version of *\n\nsub(\"\\\\w*?\",\"X\",\"...which 1-100 words get replaced?\")\n\n[1] \"X...which 1-100 words get replaced?\"\n\n\n\ngsub(\"\\\\w*?\",\"X\",\"...which 1-100 words get replaced?\")\n\n\n\n[1] \"X.X.X.XwXhXiXcXhX X1X-X1X0X0X XwXoXrXdXsX XgXeXtX XrXeXpXlXaXcXeXdX?X\"\n\n\nTry using +*.\n\nQuestion: Why do you get an error message?\n\nAnswer: The * and ? find the same characters, but have competing replacement rules (greedy or restrained).\n\n\n\nCurly brackets are used to specify a number of matches, expanding on the options even further.\n\n\n\nFind between \\(n\\) to \\(m\\) matches\n\ngsub(\"\\\\w{3,4}\",\"X\",\"...which 1-100 words get replaced?\")\n\n[1] \"...Xh 1-X Xs X XX?\"\n\n\n\n\n\nFind exactly \\(n\\) matches\n\ngsub(\"\\\\w{3}\",\"X\",\"...which 1-100 words get replaced?\")\n\n[1] \"...Xch 1-X Xds X XXed?\"\n\n\n\n\n\nFind \\(n\\) or more matches\n\ngsub(\"\\\\w{4,}\",\"X\",\"...which 1-100 words get replaced?\")\n\n[1] \"...X 1-100 X get X?\"\n\n\n\n\n\nAs above, we can use ? for the ‘lazy’ versions of these searches\n\ngsub(\"\\\\w{4,}?\",\"X\",\"...which 1-100 words get replaced?\")\n\n[1] \"...Xh 1-100 Xs get XX?\"\n\n\n\n\n\n\nSquare brackets allow us to define a set, which is a group of characters from which we want to match any. Within a set, we can use the dash - to specify a range of numbers or letters.\n\ngsub(\"[aceihw-z]\",\"X\",\"...which 1-100 words get replaced?\")\n\n[1] \"...XXXXX 1-100 Xords gXt rXplXXXd?\"\n\n\nIn the above example, we search for 1 of any of the listed letters: a, c, e, i h, w, x, y, z. Note that x and y are included in the w-z statement.\n\nQuestion: What if we want to find 1 or more of these characters in a row to replace with X?\n\n\ngsub(\"[aceihw-z]+\",\"X\",\"...which 1-100 words get replaced?\")\n\n[1] \"...X 1-100 Xords gXt rXplXd?\"\n\n\n\n\n\nUse these characters to specify searches at the start ^ or end $ of the input string.\n\n\nHow do we find which species start with the letter a? Use the start character:\n\ngrep(\"^a\",Species)\n\ninteger(0)\n\n\n\n\n\nThe same character (^) has a different meaning if used with a set []. In those cases, it negates, or finds the opposite.\nFor example, find species containing any character other than a:\n\ngrep(\"[^a]\",Species)\n\n[1] 1 2 3 4\n\n\nReplace every letter except a or l\n\ngsub(\"[^al]\",\"X\",Species)\n\n[1] \"XXXXXlaXa\"     \"XXXXXXXllaXXX\" \"XalXXaXXa\"     \"XXXXX\"        \n\n\n\n\n\nFind species that end with a\n\ngrep(\"a$\",Species)\n\n[1] 1 3\n\n\n\n\n\n\nRegular parentheses are used to ‘capture’ text, which can then be specified in the replacement string using \\\\1. Or you can capture multiple pieces of text and reorganize them by using the corresponding number – \\\\1 for the first set of(), \\\\2 for the second set of (), etc. Some examples should help.\nReplace each word with its first letter\n\ngsub(\"(\\\\w)\\\\w+\",\"\\\\1\",\n     \"...which 1-100 words get replaced?\")\n\n[1] \"...w 1-1 w g r?\"\n\n\nPull out only the numbers and reverse their order\n\ngsub(\".*([0-9]+)-([0-9]+).*\",\n     \"\\\\2-\\\\1\",\"...which 1-100 words get replaced?\")\n\n[1] \"100-1\"\n\n\nReverse first two letters of each word\n\ngsub(\"(\\\\w)(\\\\w)(\\\\w+)\",\"\\\\2\\\\1\\\\3\",\n     \"...which 1-100 words get replaced?\")\n\n[1] \"...hwich 1-010 owrds egt erplaced?\""
  },
  {
    "objectID": "regex.html#introduction",
    "href": "regex.html#introduction",
    "title": "Regular Expressions I",
    "section": "",
    "text": "Regular Expressions, also known as regex and regexp are special text-based functions that act as run complex find-and-replace functions. I didn’t learn regular expressions until I was a postdoc working at Duke University, but I wish I had learned about them much earlier! This remains one of the most useful programming tools I have ever used. It is absolutely essential for working with any kind of large text files or large data sets. I’ll explain.\nA lot of programming tools in biology use input text files that require very specific formatting (e.g. .txt, .csv, .fasta, .nex). Sometimes, you might need to reorganize or recode data in a large text file or in many separate text files. This can be a big time sink, it can introduce errors, and it’s not reproducible if you do it manually. But regular expressions can automate the process.\nHere’s one example. As a PhD student I co-founded a project called the Global Garlic Mustard Field Survey (GGMFS) with collaborator Dr. Oliver Bossdorf at the University of Tübingen – yes the same Dr. Bossdorf mentioned in the Quick Visualizations Chapter. We were fortunate to have over 100 collaborators across Europe and North America who helped to collect samples for the project. Details of the project were published in the Journal Neobiota: https://neobiota.pensoft.net/article/1270/ but one BIG problem is the way that each of these 100+ collaborators entered their data online. For example, latitudes and longitudes were entered in a variety of different formats. Regular expressions allowed me to write a small program to automatically convert all of these different formats to a common, decimal format that we could use for the analysis. This saved a huge amount of time and prevented errors that could have been introduced if we tried to edit these values by hand.\nOften when you work with large datasets, you will need to automate some of your error correction, and regular expressions can be a big help here. For example, imagine a simple online survey that includes a place for people to simply type “yes” or “no” in response to a question. This should be coded as a binary variable (1 or 0) for analysis, but you might find a variety of inputs such as: “YES”, “Y”, “yes”, and “Yes”. These all mean the same thing, yet if you try to analyze the raw output, R will treat these as different categories. Here again, regular expressions can be used to quickly change all the different examples to a common “Y” or to a Boolean variable TRUE.\nOne final example, is pattern matching, which is common for the analysis of DNA, proteins or other large strings of data. You may want to find a particular sequence of data, possibly with a few variable sites: e.g. TCTA or TCAA or TCGA. This is another area where regular expressions can help.\n\n\nRegular expressions are a universal language that extends to many other programming languages, including C/C#/C++, Python, Unix/Linux, and Perl. We focus here on R but most of the syntax is mantained across programming languages.\n\n\n\nWARNING! There is a very steep learning curve here, and the only way to really learn this is to drown yourself in examples. There are lots of exercises you can do for practice online. You should also try to apply these whenever you can, just like you should with all of your other R skills."
  },
  {
    "objectID": "regex.html#functions",
    "href": "regex.html#functions",
    "title": "Regular Expressions I",
    "section": "",
    "text": "There are four main functions that use regular expressions in R.\ngrep() and grepl() are equivalent to ‘find’ in your favorite word processor. They have the general form:\n\ngsub(\"find\", in.this.object)\n\ngrep() outputs a vector with all of the address locations (i.e. numbers) that match. Thus the output length is equal to the number of matches.\ngrepl() outputs a vector of TRUE (match) and FALSE (no match). Thus, the output length is equal to the length of the input object.\nsub() and gsub() are equivalent to ‘find and replace’. They have the general form:\n\ngrep(\"find\", \"replace\", in.this.object)\n\nsub() replaces only the first match, whereas gsub() replaces all of the matches.\nSome specific examples are provided below to help you understand these similarities and differences. As always, you should take the time to try these out and make sure you get the same input. If you don’t, then it’s a good learning opportunity to find out what you did differently!\nThere are two other more advanced functions in R. These aren’t covered in this tutorial, but may be of use once you are more comfortable with the above functions.\nregexpr() provides more detailed information about the first match.\ngregexpr() provides more detailed results about all matches.\n\nSee ?regexpr and ?gregexpr for more info\n\n\n\nSome examples can help to understand the differences among the four main functions. Let’s start with a simple data frame of species names.\n\nSpecies&lt;-c(\"petiolata\", \"verticillatus\", \"salicaria\", \"minor\")\nprint(Species)\n\n[1] \"petiolata\"     \"verticillatus\" \"salicaria\"     \"minor\"        \n\n\n\n\n\nThis returns cell addresses matching the query string.\n\ngrep(\"a\",Species)\n\n[1] 1 2 3\n\n\nNote the vector length compared to the input vector. Instead of the cell number, we can get R to return the specific values in each matching cell with the value=T parameter\n\ngrep(\"a\",Species, value=T)\n\n[1] \"petiolata\"     \"verticillatus\" \"salicaria\"    \n\n\n\n\n\nThis returns a vector of TRUE (match) and FALSE (no match). Compare this output with the same parameters in the grep() function.\n\ngrepl(\"a\",Species)\n\n[1]  TRUE  TRUE  TRUE FALSE\n\n\n\n\n\nThis replaces the first match (in each cell)\n\nsub(\"l\",\"L\",Species)\n\n[1] \"petioLata\"     \"verticiLlatus\" \"saLicaria\"     \"minor\"        \n\n\n\n\n\nThis replaces all matches (in each cell). Compare this output to sub().\n\ngsub(\"l\",\"L\",Species)\n\n[1] \"petioLata\"     \"verticiLLatus\" \"saLicaria\"     \"minor\"        \n\n\n\nQuestion: Did you see the difference?\n\nHint: Look at “Verticillatus”."
  },
  {
    "objectID": "regex.html#wildcards",
    "href": "regex.html#wildcards",
    "title": "Regular Expressions I",
    "section": "",
    "text": "The backslash is a special character. It’s called the ‘escape’ character because it is used to escape from the literal interpretation of the next character to the right. For example, \\. applies the escape to the period character. The specific meaning depends on the context, which is much easier to understand by examples, as shown below.\n\n\n\nIn the introduction, we discussed the universality of regular expressions in the sense that a similar syntax is used by many different programming langagues. But now here is one exception. In R, the double-escape is usually needed, whereas other programming languages typically use just one. The reason is a bit meta – it’s because we are running regular expressions within R object. So the first \\ is used to escape special characters in R, applying it to the second \\, which is itself the special character that needs to be escaped to pass through the function. The second slash is followed by the ‘escaped’ character. Some examples are provided below.\nIf that isn’t clear. Just remember that you need two backslashes when writing regular experssions in R, but just one backslash for most other languages.\n\n\n\nInstead of finding the letter w, the \\\\w is a wildcard character that represents any letter or digit. It also includes underscore _ for some reason.\n\nsub(\"w\",\"X\",\"...which 1-100 words get replaced?\")\n\n[1] \"...Xhich 1-100 words get replaced?\"\n\ngsub(\"w\",\"X\",\"...which 1-100 words get replaced?\")\n\n[1] \"...Xhich 1-100 Xords get replaced?\"\n\nsub(\"\\\\w\",\"X\",\"...which 1-100 words get replaced?\")\n\n[1] \"...Xhich 1-100 words get replaced?\"\n\ngsub(\"\\\\w\",\"X\",\"...which 1-100 words get replaced?\")\n\n[1] \"...XXXXX X-XXX XXXXX XXX XXXXXXXX?\"\n\n\nAgain, note the differences between the sub() and gsum() functions. We’ll stick to gsub() for the remainder of the examples in this chapter, but you should also run sub() yourself. Each time, take a moment to try to predict how the output will differ before running it. This will help you develop an understanding of regular expressions much more quickly.\n\n\n\nThe capital W is the inverse of \\\\w find a character that is NOT a letter or number.\n\ngsub(\"\\\\W\",\"X\",\"...which 1-100  words get replaced?\")\n\n[1] \"XXXwhichX1X100XXwordsXgetXreplacedX\"\n\n\n\n\n\nThis represents a space\n\ngsub(\"\\\\s\",\"X\",\"...which 1-100  words get replaced?\")\n\n[1] \"...whichX1-100XXwordsXgetXreplaced?\"\n\n\n\n\n\nThis is a tab character. A lot of data files stored as text are tab-delimited (.tsv) as well as comma-delimited (.csv)\n\ngsub(\"\\\\t\",\"X\",\"...which 1-100 \\t words get replaced?\")\n\n[1] \"...which 1-100 X words get replaced?\"\n\n\nRemember that \\t is a tab character.\n\ncat(\"A\\t\\t\\tB C\")\n\nA           B C\n\n\n\n\n\nd for digits. This is the wild card for numeric characters.\n\ngsub(\"\\\\d\",\"X\",\"...which 1-100  words get replaced?\")\n\n[1] \"...which X-XXX  words get replaced?\"\n\n\n\n\n\nNon-digit characters\n\ngsub(\"\\\\D\",\"X\",\"...which 1-100  words get replaced?\")\n\n[1] \"XXXXXXXXX1X100XXXXXXXXXXXXXXXXXXXXX\""
  },
  {
    "objectID": "regex.html#new-lines",
    "href": "regex.html#new-lines",
    "title": "Regular Expressions I",
    "section": "",
    "text": "There are two special characters that indicate new lines in a text file.\n\n\nThis is the ‘carriage return’ special character\n\n\n\nThis is the ‘newline’ special character\n\n\n\nOne or both of these may be generated when you press the ‘enter’ key while writing a text file. The difference depends on which operating system you are using. These also add a source of headache and confusion when working with text files because:\n\nUnix and MacOS text files use lines that end with \\n only\nWindows and DOS text files use lines end with \\r\\n\n\n\nQuestion: Do you know how this difference originated?\n\nAnswer: The reason goes back to the early days of programming, when programmers were moving from mechanical typewriters to computer programs. Mechanical typewriters are hard to find these days, but they would hold a piece of paper in place on a cylinder called a carriage. The\nThe \\n stands for ‘new line’, and the \\r stands for return. When you reach the end of a line of text on a typewriter, you would typically return the carriage back to the starting position, and then move to the next line, thus the \\r\\n. The Unix operating system decided that the \\r wasn’t needed, whereas the DOS operating system decided to include it.\nThis difference can cause problems when moving text files across operating systems. Programs like FileZilla will automatically translate these end-of-line characters when moving across systems."
  },
  {
    "objectID": "regex.html#special-characters",
    "href": "regex.html#special-characters",
    "title": "Regular Expressions I",
    "section": "",
    "text": "In addition to special characters that use the escape \\\\, there are a number of other special characters that don’t use the escape, but have a special meaning.\nNote that if you want to search for the characters below you would have to use the escape character. E.g., use \\\\. to search for the period character (.).\n\n\nThe period is a wild card that means ‘anything’. This includes all of the \\\\w characters but also other characters like puncutation marks.\n\ngsub(\".\",\"X\",\"...which 1-100  words get replaced?\")\n\n[1] \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\"\n\n\nSo how to search for a period .? As noted above, we have to use the escape character\n\ngsub(\"\\\\.\",\"X\",\"...which 1-100  words get replaced?\")\n\n[1] \"XXXwhich 1-100  words get replaced?\"\n\n\n\n\n\nThis is sometimes called the pipe character, and it simply means ‘or’. For example, we can search for w or e.\n\ngsub(\"w|e\",\"X\",\"...which 1-100  words get replaced?\")\n\n[1] \"...Xhich 1-100  Xords gXt rXplacXd?\"\n\n\n\n\n\nThese special characters refer to details about the kind of search that we are trying to conduct. Look at these examples carefully, and remember that sub replaces the first match while gsub replaces all of the matches.\n\nsub(\"\\\\w\",\"X\",\"...which 1-100 words get replaced?\")\n\n[1] \"...Xhich 1-100 words get replaced?\"\n\ngsub(\"\\\\w\",\"X\",\"...which 1-100 words get replaced?\")\n\n[1] \"...XXXXX X-XXX XXXXX XXX XXXXXXXX?\"\n\n\nNow let’s apply some of these special characters to see how they work.\n\n\n\nFinds ‘one or more’ matches (i.e. at least one match)\n\nsub(\"\\\\w+\",\"X\",\"...which 1-100 words get replaced?\")\n\n[1] \"...X 1-100 words get replaced?\"\n\ngsub(\"\\\\w+\",\"X\",\"...which 1-100 words get replaced?\")\n\n[1] \"...X X-X X X X?\"\n\n\nCompare this match to the one above. Notice how we have replaced groups of letters instead of single letters. The algorithm works like this:\n\nStart at the left and move to the right, one character at a time\nCheck if the character is a letter or number (\\\\w).\nIf NO, move to the next character\nIf YES, check the next character. If it is also a \\\\w then go to the next character. Repeat until the next character is not \\\\w, and replace the entire string of characters.\n\nWhen run in the sub() function, the algorithm does the above and then stops. When run with the gsub() function, it continues to the next character, and then starts over.\n\n\n\nThis is a greedy search matches 0 or more in a row. Again, this is easier to understand by exploring examples.\n\nsub(\"\\\\w*\",\"X\",\"...which 1-100 words get replaced?\")\n\n[1] \"X...which 1-100 words get replaced?\"\n\ngsub(\"\\\\w*\",\"X\",\"...which 1-100 words get replaced?\")\n\n[1] \"X.X.X.X X-X X X X?X\"\n\n\nIn the sub() function, it detects a period (.) as the first character, indicating no match. It replaces the ‘null’ or 0 match at the beginning, which has the effect of adding a character. In the gsub() function it repeats this again before each period (.). It then continues until it finds the letter w. Then it finds a group of \\\\w matches, replacing all of them with a single X. Then a space, which is skipped, then a -, which is another null match, prompting another insert.\n\n\n\nThis is the restrained search, which matches zero or one time.\n\nsub(\"\\\\w?\",\"X\",\"...which 1-100 words get replaced?\")\n\n[1] \"X...which 1-100 words get replaced?\"\n\ngsub(\"\\\\w?\",\"X\",\"...which 1-100 words get replaced?\")\n\n[1] \"X.X.X.XXXXX X-XXX XXXXX XXX XXXXXXXX?X\"\n\n\nCompare this to the * above. The ? character behaves in a similar way, except it is constrained in the sense that each each letter is replaced individually, instead of replacing entire words.\n\n\n\nThis is the lazy version of +\n\nsub(\"\\\\w+?\",\"X\",\"...which 1-100 words get replaced?\")\n\n[1] \"...Xhich 1-100 words get replaced?\"\n\ngsub(\"\\\\w+?\",\"X\",\"...which 1-100 words get replaced?\")\n\n[1] \"...XXXXX X-XXX XXXXX XXX XXXXXXXX?\"\n\n\nNote the difference in sub(), which replaces on the the first letter here but the whole word when + is used alone in the earlier example. In the gsub() example we end up replacing every letter instead of whole words. Remember, sub() runs the algorithm once and then stops, while gsub() cycles through the algorithm until it reaches the end of the line.\n\n\n\nSimilarly, we can combine these characters for the ‘lazy’ version of *\n\nsub(\"\\\\w*?\",\"X\",\"...which 1-100 words get replaced?\")\n\n[1] \"X...which 1-100 words get replaced?\"\n\n\n\ngsub(\"\\\\w*?\",\"X\",\"...which 1-100 words get replaced?\")\n\n\n\n[1] \"X.X.X.XwXhXiXcXhX X1X-X1X0X0X XwXoXrXdXsX XgXeXtX XrXeXpXlXaXcXeXdX?X\"\n\n\nTry using +*.\n\nQuestion: Why do you get an error message?\n\nAnswer: The * and ? find the same characters, but have competing replacement rules (greedy or restrained).\n\n\n\nCurly brackets are used to specify a number of matches, expanding on the options even further.\n\n\n\nFind between \\(n\\) to \\(m\\) matches\n\ngsub(\"\\\\w{3,4}\",\"X\",\"...which 1-100 words get replaced?\")\n\n[1] \"...Xh 1-X Xs X XX?\"\n\n\n\n\n\nFind exactly \\(n\\) matches\n\ngsub(\"\\\\w{3}\",\"X\",\"...which 1-100 words get replaced?\")\n\n[1] \"...Xch 1-X Xds X XXed?\"\n\n\n\n\n\nFind \\(n\\) or more matches\n\ngsub(\"\\\\w{4,}\",\"X\",\"...which 1-100 words get replaced?\")\n\n[1] \"...X 1-100 X get X?\"\n\n\n\n\n\nAs above, we can use ? for the ‘lazy’ versions of these searches\n\ngsub(\"\\\\w{4,}?\",\"X\",\"...which 1-100 words get replaced?\")\n\n[1] \"...Xh 1-100 Xs get XX?\""
  },
  {
    "objectID": "regex.html#set",
    "href": "regex.html#set",
    "title": "Regular Expressions I",
    "section": "",
    "text": "Square brackets allow us to define a set, which is a group of characters from which we want to match any. Within a set, we can use the dash - to specify a range of numbers or letters.\n\ngsub(\"[aceihw-z]\",\"X\",\"...which 1-100 words get replaced?\")\n\n[1] \"...XXXXX 1-100 Xords gXt rXplXXXd?\"\n\n\nIn the above example, we search for 1 of any of the listed letters: a, c, e, i h, w, x, y, z. Note that x and y are included in the w-z statement.\n\nQuestion: What if we want to find 1 or more of these characters in a row to replace with X?\n\n\ngsub(\"[aceihw-z]+\",\"X\",\"...which 1-100 words get replaced?\")\n\n[1] \"...X 1-100 Xords gXt rXplXd?\""
  },
  {
    "objectID": "regex.html#negate-or-start-and-end",
    "href": "regex.html#negate-or-start-and-end",
    "title": "Regular Expressions I",
    "section": "",
    "text": "Use these characters to specify searches at the start ^ or end $ of the input string.\n\n\nHow do we find which species start with the letter a? Use the start character:\n\ngrep(\"^a\",Species)\n\ninteger(0)\n\n\n\n\n\nThe same character (^) has a different meaning if used with a set []. In those cases, it negates, or finds the opposite.\nFor example, find species containing any character other than a:\n\ngrep(\"[^a]\",Species)\n\n[1] 1 2 3 4\n\n\nReplace every letter except a or l\n\ngsub(\"[^al]\",\"X\",Species)\n\n[1] \"XXXXXlaXa\"     \"XXXXXXXllaXXX\" \"XalXXaXXa\"     \"XXXXX\"        \n\n\n\n\n\nFind species that end with a\n\ngrep(\"a$\",Species)\n\n[1] 1 3"
  },
  {
    "objectID": "regex.html#capture",
    "href": "regex.html#capture",
    "title": "Regular Expressions I",
    "section": "",
    "text": "Regular parentheses are used to ‘capture’ text, which can then be specified in the replacement string using \\\\1. Or you can capture multiple pieces of text and reorganize them by using the corresponding number – \\\\1 for the first set of(), \\\\2 for the second set of (), etc. Some examples should help.\nReplace each word with its first letter\n\ngsub(\"(\\\\w)\\\\w+\",\"\\\\1\",\n     \"...which 1-100 words get replaced?\")\n\n[1] \"...w 1-1 w g r?\"\n\n\nPull out only the numbers and reverse their order\n\ngsub(\".*([0-9]+)-([0-9]+).*\",\n     \"\\\\2-\\\\1\",\"...which 1-100 words get replaced?\")\n\n[1] \"100-1\"\n\n\nReverse first two letters of each word\n\ngsub(\"(\\\\w)(\\\\w)(\\\\w+)\",\"\\\\2\\\\1\\\\3\",\n     \"...which 1-100 words get replaced?\")\n\n[1] \"...hwich 1-010 owrds egt erplaced?\""
  },
  {
    "objectID": "rmarkdown.html",
    "href": "rmarkdown.html",
    "title": "R Markdown",
    "section": "",
    "text": "By now you have mastered the fundamentals of base R, visualizations, and data science!\nR Markdown is a powerful format for quickly making high-quality reports of your analysis. You can embed code and all kinds of output, including graphs, and output them to a Word Document, PDF or website. In fact, this entire book was written in R Markdown!\nHere we’ll cover just the basics, but a complete guide to R Markdown is available online from Yihui Xie, J. J. Allaire and Garrett Grolemund (https://bookdown.org/yihui/rmarkdown/). You can also check out the R Markdown documents that we use to make our tutorial websites on our GitHub Pages (the website files have .html extension and the R Markdown files have the same name with .Rmd extensions):\n\nColautti Lab Resources Website: (https://colauttilab.github.io/)\nColautti Lab GitHub Repository: (https://github.com/ColauttiLab/ColauttiLab.github.io)\n\n\n\n\nBefore beginning this tutorial, make sure you have installed these packages:\n\ninstall.packages('rmarkdown')\ninstall.packages('dplyr')\ninstall.packages('knitr')\n\nThese should be installed with R Studio, but you may want to re-install them if you are working with an older version. You may have to quit and restart R Studio a few times during this process.\n\n\n\nThere is a very handy 2-page ‘cheat sheet’ https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf, which you can also access through R Studio under the Help menu: Help -&gt; Cheatsheets. There are several other links here too, including ggplot and dplyr.\n\n\n\nSo far, you’ve been working through the command line in a .R program file. As discussed in the R Fundamentals Chapter, the benefit of programming in a .R file is that we can share the program and associated files with other computers, including high-performance servers.\nAn R Markdown file is another special text file, with a .Rmd extension. It’s useful for generating reports with embedded code and visualizations. I use it as a virtual notepad when analyzing new data, and later convert it to Online Supplementary Material for published papers. The utility of the .Rmd file is easiest to understand by example.\nBecause an R Markdown file is just a text file with a .Rmd extension, we could just make a new text file and save it with the .Rmd extension. However, in RStudio, we can make a new R markdown file from the menu: File-&gt; New-&gt; R Markdown\nChoose Document from the left-hand side menu, give it a title, and make sure html is selected.\nThen click OK\nVery few elements are needed for a basic markdown file, and examples of these elements are provided within the R Markdown file that R Studio sets up for you.\nAt the top of this window is a little icon that says ‘Knit’. The Knit icon, knits together your text file into a rendered html document.\nTry clicking the knit icon to generate a report using the default text in the R Markdown file. Note that you may have to save a copy of your R Markdown file first. A new file will open in a web browser. Take a moment to compare the output file with the input R Markdown.\nIf you want to output this as a pdf file, then you can simply choose Print, and then Print to PDF in your web browser. You can also make pdf files directly from R Studio, but you might run into problems depending on your file content and format.\nWe’ll stick with the html version for now, and walk through some of the main components available to you.\n\n\n\nEvery R Markdown file starts with a YAML header, which contains some basic information about the file. A YAML header is generated automatically when you make a new .Rmd file in RStudio, but not all elements are needed. Depending on what options you choose, it might look something like this:\n\n---\ntitle: \"Untitled\"\nauthor: \"Robert I. Colautti\"\ndate: \"January 20, 2019\"\noutput: html_document\n---\n\nThere are other options available for YAML, and you can includes a separate _output.yml to set other aspects of the layout.\nHere are some additional formatting options. Replace the output: html_document line above:\n\noutput:\n  html_document: # Add options for html output\n    toc: true # Add table of contents (TOC)\n    number_sections: true # Add section numbers\n    toc_float: # Have TOC floating at the side\n      collapsed: false # Expand subsections\n\n\n\n\nThe Markdown in R Markdown refers to the Markdown protocol (https://en.wikipedia.org/wiki/Markdown)\nThis is a non-proprietary system that was designed to quickly and easily encode formatted documents and websites in a simple text document.\nThe main advantage of R Markdown (.Rmd) over regular Markdown (.md) is the ability to easily print, format, and execute embedded R code for graphs, tables, and calculations.\nWe’ll look at some basic Markdown elements and then we’ll see how to embed R code for professional, reproducible reports.\n\n\n\n\n\nPlain text is converted into paragraph format.\nTo start a new paragraph, press enter twice. This is important – if you only press enter once, then the two paragraphs will knit together into the same paragraph.\nSimilarly, if you inlcude more than two lines between paragraphs, these will be ignored when you render the R Markdown document.\nTry adding paragraphs of text separated by pressing enter 1, 2 and 3 times. Then, knit to html to see how these are rendered in the final output.\n\n\n\nYou can format text with * or _\n*italics* or _italics_: italics\n**bold** or __bold__: bold\nUse greater-than sign for block quotes, eg. &gt; TIP: quote\n\nTIP: quote\n\n\n\n\n\nAdd headers with up to six hash marks (#). Each additional # denotes a sub-heading of the previous (sub)heading.\n\n\n\n\n\n\n\nUse two dashes (--) for short–dash (a.k.a. ‘n-dash’).\nUse three dashes (---) for long — dash (a.k.a. m-dash).\n\n\n\nLinks have a special format. The text you want the user to see goes in square brackets, followed immediately by the file or html link in regular brackets, with no space in between. You can use both web links and relative path links.\n[Colautti Lab Website](https://colauttilab.github.io/)\nThis should produce a link if you are reading this electronically:\nColautti Lab Website\nYou can also use this with relative path names, for example to link a file in a folder called images inside of the project folder:\n[Linked .png file](./images/ColauttiLabLogo.png)\nAgain, this should produce a link if you are reading this electronically. However, you will get an error if you try to include this in your R Markdown file.\n\nQuestion: Why will you get an error?\n\nAnswer: This file is not in your working directory. You will need to create an image and save it as ColauttiLabLogo.png inside a directory called images, which is also inside of the working directory that contains your .Rmd file.\n\n\n\nInstead of linking, you can embed the image directly by adding an exclamation point. The text in square brackets becomes the figure caption.\n![Linked .png file](./images/ColauttiLabLogo.png):\n\n\n\nLinked .png file\n\n\nNote that R Markdown added a figure number for me, based on the chapter and the number of previous images. This doesn’t include the graphs that were created with embedded R code, only images that were embedded with ![]().\n\n\n\nLists are easy to create, simply start a line with * or + for unordered lists or a number for ordered lists. Add tab characters for sub-lists:\n\n+ Unordered list item 1  \n* Item 2  \n  + sub item 2.1  \n  * sub item 2.2  \n* Item 3  \n\n\nUnordered list item 1\nItem 2\n\nsub item 2.1\nsub item 2.2\n\nItem 3\n\n\n1. Ordered list item 1  \n2. Item 2  \n  + sub item 2.1  \n  * sub item 2.2  \n3. Item 3  \n\n\nOrdered list item 1\n\nItem 2\n\n\n\nsub item 2.1\n\nsub item 2.2\n\n\n\nItem 3\n\nThe fun thing about ordered lists is the numbers you use don’t really matter – R Markdown will automatically start at 1 and increase for each item.\n\n1. Ordered list item 1  \n1. Item 2  \n  + sub item 2.1  \n  * sub item 2.2  \n1. Item 3  \n\n\nOrdered list item 1\n\nItem 2\n\n\n\nsub item 2.1\n\nsub item 2.2\n\n\n\nItem 3\n\nThis is a nice feature in early drafts of your R Markdown, to which you might later add, rename, or reorganize the order. You can just leave the numbers and not waste effort renumber each time.\n\n\n\nTables are added using a line of horizontal dashes to separate the title of the table, then a row of header names separated by pipes to define the header row. Finally, we add another line of dashes with pipes to indicate the relative column widths. The data goes underneath, with pipes separating each column.\n\nTables \n-----------------------\nDate  | Length  | Width\n------|---------|------\n09/09/09 | 14 | 27\n10/09/09 | 15 | 29\n11/09/09 | 16 | 31\n\nProduces this output:\n\n\n\n\n\n\nDate\nLength\nWidth\n\n\n\n\n09/09/09\n14\n27\n\n\n10/09/09\n15\n29\n\n\n11/09/09\n16\n31\n\n\n\nNow that we have written some basic markdown elements, we can generate our R markdown report to see what they look like. Click on the knit button and compare the input with what you have typed in your R mardown file.\n\n\n\nYou can format text to look like code using the back-tick character.\n\n# `Use single tick mark to invoke code formatted text.`\n\nThe Back Tick is a strange looking character usually located on the same key as the tilde (~) on English keyboards. Don’t confuse the back tick with the single quotation mark.\nYou can incorporate blocks of R code using three back ticks with r in curly brackets. Write some code, then add three more tick marks to signal the end of the code chunk. By defaule, your code will run when you convert your R Markdown to html, showing the output. This is a great way to include graphs and the output of statistical models.\n\n#   ```{r}\n#   &lt;your code goes here&gt;&gt;\n#   ```\n\nCtl-Alt-i is a nice shortcut in R Studio for adding code chunks quickly.\n\n\nYou can name your code chunks by adding a name right after the r separated only by a space. Naming code chunks is very handy for troubleshooting. When you knit your file, the name of each chunk is listed in the Render tab in R Studio. If there is an error, you can see which code chunk is causing the error. Note that the name cannot contain spaces, and it can be followed by a comma to specify options for the code chunk.\n\n# ```{r code-chunk-name, eval=F}`\n\n\n\n\nYou can use different options for your R code chunks, as shown on the R Markdown cheat sheet. The default is to both print the code AND show the output, but these can be changed with options. The three options I use most commonly are:\n\neval=F – Don’t evaluate the code. The code will be shown, but it won’t be run so no objects or output will be created.\necho=F – Don’t echo the code. Don’t show the code, but run it and include any output, plots, messages and warnings.\ninclude=F – Don’t include the code in the rendered document. Run the code to create any objects and load libraries, but don’t show the code or any output, plots, messages or warnings.\n\n\n\n\n\nMaking tables from data is a bit more complicated. For example, if we wanted to summarize the FallopiaData.csv data, we could read in the file and then summarize with dplyr as we did in the Data Science Chapter.\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.2\n\nFallo&lt;-read.csv(\n  \"https://colauttilab.github.io/RCrashCourse/FallopiaData.csv\")\n\nSumTable&lt;-Fallo %&gt;% \n  group_by(Taxon,Scenario,Nutrients) %&gt;%\n  summarize(Mean=mean(Total), SD=sd(Total)) %&gt;%\n  arrange(desc(Mean))\n\nprint(SumTable)\n\n# A tibble: 10 × 5\n# Groups:   Taxon, Scenario [10]\n   Taxon Scenario     Nutrients  Mean    SD\n   &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 bohem high         high       60.3  8.68\n 2 japon gradual      high       59.7  9.57\n 3 bohem fluctuations high       58.4  9.20\n 4 bohem extreme      high       58.3  7.34\n 5 bohem gradual      high       57.5  9.34\n 6 japon extreme      high       57.2 10.9 \n 7 japon high         high       56.4  8.20\n 8 japon fluctuations high       56.4 13.7 \n 9 japon low          low        52.0  8.29\n10 bohem low          low        48.0  8.86\n\n\nThe output is legible but not very attractive for a final report. To make it look better, we can use the kable function from the knitr package:\n\nlibrary(knitr)\n\nWarning: package 'knitr' was built under R version 4.3.2\n\nkable(SumTable, caption = \"Summary Table\")\n\n\nSummary Table\n\n\nTaxon\nScenario\nNutrients\nMean\nSD\n\n\n\n\nbohem\nhigh\nhigh\n60.28091\n8.677075\n\n\njapon\ngradual\nhigh\n59.72917\n9.565376\n\n\nbohem\nfluctuations\nhigh\n58.36455\n9.202334\n\n\nbohem\nextreme\nhigh\n58.30917\n7.337015\n\n\nbohem\ngradual\nhigh\n57.46154\n9.338311\n\n\njapon\nextreme\nhigh\n57.23643\n10.903133\n\n\njapon\nhigh\nhigh\n56.44833\n8.204091\n\n\njapon\nfluctuations\nhigh\n56.43692\n13.724906\n\n\njapon\nlow\nlow\n52.02917\n8.287938\n\n\nbohem\nlow\nlow\n47.98077\n8.862164\n\n\n\n\n\nThe output of this code is shown in the Summary Table. Contrast the formatting from the kable() function with the standard R output from print().\n\n\n\nUse R code to embed graphs.\n\nggplot() +\n  geom_histogram(aes(rnorm(100)), binwidth=0.2)\n\n\n\n\n\n\n\n\n\n\n\nRendering an html has additional features that won’t show up in a pdf. The code below will produce tabs in your knit html file.\n\n## Tabs by Group {.tabset}\n\n### Treatment Group\n\n1. Treatment 1 explanation\n2. Treatment 2 explanation\n3. Treatment 3 explanation\n\n### Histogram\n\n#```{r}\n#library(ggplot2)\n#ggplot() + \n#  geom_histogram(aes(rnorm(100)),binwidth=0.2)\n#```\n\nThe output of this code is shown in the figure.\n{fig-alt=“Tabs created in html using {.tabset} with ## headers. The”Plot 1” tab contains embedded R code to produce a histogram graph. Both the code and graph are shown by default”}\n\n\n\nInsert equations using LaTeX equations. LaTeX is a system for converting basic text files to formated documents.\nUse single dollar signs for in-line equations, like $Y = X$, which will print as \\(Y = X\\) on the same line as the text.\n\nUse double dollar signs on a new line for full-line equations, like $$Y = X$$:\n\n\\[Y = X \\]\nwhich isolates the equation on its own line, and centers it.\nYou can use a variety of Greek letters by using the backslash character \\. For upper-case Greek letters, just use an upper-case letter in the spelling. For example:\n\n\\lambda(\\(\\lambda\\)) OR \\Lambda (\\(\\Lambda\\))\n\\gamma(\\(\\gamma\\)) OR \\Gamma(\\(\\Gamma\\))\n\\delta(\\(\\delta\\)) OR \\Delta(\\(\\Delta\\))\n\nOmicron can be \\omicron or simply the letter o, with no backslash o (\\(o\\)). Note that some LaTeX characters, like omicron, do not have capital versions (e.g., \\Omicron does not produce a Greek character).\nThere are numerous other options but below is a quick rundown of some of the commonly used scripts.\n\n\n\n\n\n\n\n\n\nScript\nDescription\nCode\nExample\n\n\n\n\n\\infty\nInfinity\n\\infty\n\\(\\infty\\)\n\n\n_\nSubscript\nX_i\n\\(X_i\\)\n\n\n^\nSuperscript\nX^2\n\\(X^2\\)\n\n\n'\nFirst order derivative\nf'(x)\n\\(f'(x)\\)\n\n\n''\nSecond order derivative\ns'(x)\n\\(s'(x)\\)\n\n\n\\sim\nPredict\nY \\sim X\n\\(Y \\sim X\\)\n\n\n\\times\nMultiply\nX \\times Y\n\\(X \\times Y\\)\n\n\n\\pm\nPlus or minus\nX \\pm Y\n\\(X \\pm Y\\)\n\n\n\\neq\nNote equal\nX \\neq Y\n\\(X \\neq Y\\)\n\n\n\\leq\nLess than or equal\nX \\leq Y\n\\(X \\leq Y\\)\n\n\n\\geq\nGreater than or equal\nX \\geq Y\n\\(X \\geq Y\\)\n\n\n{}\nGroup together\nX_{subscript}\n\\(X_{subscript}\\)\n\n\n\\sqrt\nSquare root\n\\sqrt{x^2y^2}\n\\(\\sqrt{x^2y^2}\\)\n\n\n\\frac\nFraction\n\\frac{X+1}{X-1}\n\\(\\frac{X+1}{X-1}\\)\n\n\n\\sum\nSum\n\\sum_{x=1}^{K}\n\\(\\sum_{x=1}^{K}\\)\n\n\n\\prod\nProduct\n\\prod_{x=1}^{K}\n\\(\\prod_{x=1}^{K}\\)\n\n\n\\int\nIntegral\n\\int_{0}^{\\infty}\n\\(\\int_{0}^{\\infty}\\)\n\n\n\\lim\nLimit\n\\lim_{x \\to \\infty}\n\\(\\lim_{x \\to \\infty}\\)\n\n\n\nNote in particular, the use of curly brackets to group items together in superscripts, subscripts, fractions and square root. Also note the simulate (\\sim) character, which is the tilde (~) used in statistical models and other R functions like facet_grid() and aggregate().\nHere are some more sophisticated examples to show how to create more complex equations. Again, try reproducing these in R markdown. If you don’t get the same output, then check to see what is different with your code.\n\n$$Y_i \\sim \\alpha + \\beta_1 X_i + \\epsilon_i$$ \n\nwill produce a linear model equation:\n\\[Y_i \\sim \\alpha + \\beta_1 X_i + \\epsilon_i\\]\nand\n\n$$sum_{n=1}^{\\infty} 2^{-n} = 1$$\n\nwill produce:\n\\[\\sum_{n=1}^{\\infty} 2^{-n} = 1\\]\nNote the use of special characters with the backslash \\, along with subscripts _ and superscripts ^ with text in curly brackets {}.\nThat’s all you need to know to produce professional reports with R and R Markdown!\n\n\n\nR Notebook files are a unique type of R Markdown file, but include subtle differences that probably won’t matter much to you at this point in your coding journey. You could create a notebook file in similar ways to R markdown, the easiest being to select File --&gt; New File --&gt; R Notebook from the R Studio menu. However, I suggest you don’t, for reasons that will become clear below.\nSaving an R notebook file adds the extension .nb to tell R Studio that it’s a notebook file. For example, if you had an R markdown file called MyAnalysis.Rmd the R notebook version would save as MyAnalysis.Rmd.nb. Additionally, an html file is created whenever you save the notebook file, whereas you have to manually ‘knit’ an R markdown file to produce an html output.\nWhen you knit a .Rmd document, you re-run all of the code, whereas the .Rmd.nb only updates the code that has changed. This can cause confusion if you change one part of the code but forget to update it, whereas knitting the whole document is a safer approach for new coders, and established coders like me who are very prone to typos and other errors. The disadvantage of knitting the whole document each time is that it takes longer. However, you can also break up a large analysis into separate Rmd files. There is also a better alternative to notebooks, as described in the next section.\n\n\n\nQuarto is a recent addition to R Studio that also is based in R markdown but has a number of additional features. Again, I suggest you avoid using at this point because it has features that hide the underlying R Markdown code, which will make it harder for you to remember. As with R Markdown and R Notebooks, you can create a quarto file from the File --&gt; New File menu. However, you have two types of Quarto files: Documents and Presentations.\nAlthough they are based in R Markdown, they have additional commands that make quarto files different enough to get their own file extension: .qmd instead of .Rmd. The main advantage of Quarto is that it provides real-time rendering of the document inside of the R Studio window. When you open a Quarto file in R Studio, you will see options for Source and Visual at the top of the text editor window. Source shows the underlying code, whereas Visual gives the real-time document rendering. This means that you get an idea of the format and appearance of the document without having to save or knit the file each time.\nAt this stage of your learning, I suggest you avoid using Quarto and focus on R Markdown. Once you master R Markdown and the rest of this book, then you might want to move to Quarto and learn the additional coding options. At least for now, avoid using the Visual option with real-time rendering while you are coding a document. If you aren’t seeing the underlying code, then you are going to have trouble learning and retaining it.\n\nMy advice at this stage is to keep it simple. Focus on learning R Markdown first. As you get more comfortable coding, you could move to more advanced tools like Quarto."
  },
  {
    "objectID": "rmarkdown.html#introduction",
    "href": "rmarkdown.html#introduction",
    "title": "R Markdown",
    "section": "",
    "text": "By now you have mastered the fundamentals of base R, visualizations, and data science!\nR Markdown is a powerful format for quickly making high-quality reports of your analysis. You can embed code and all kinds of output, including graphs, and output them to a Word Document, PDF or website. In fact, this entire book was written in R Markdown!\nHere we’ll cover just the basics, but a complete guide to R Markdown is available online from Yihui Xie, J. J. Allaire and Garrett Grolemund (https://bookdown.org/yihui/rmarkdown/). You can also check out the R Markdown documents that we use to make our tutorial websites on our GitHub Pages (the website files have .html extension and the R Markdown files have the same name with .Rmd extensions):\n\nColautti Lab Resources Website: (https://colauttilab.github.io/)\nColautti Lab GitHub Repository: (https://github.com/ColauttiLab/ColauttiLab.github.io)"
  },
  {
    "objectID": "rmarkdown.html#setup",
    "href": "rmarkdown.html#setup",
    "title": "R Markdown",
    "section": "",
    "text": "Before beginning this tutorial, make sure you have installed these packages:\n\ninstall.packages('rmarkdown')\ninstall.packages('dplyr')\ninstall.packages('knitr')\n\nThese should be installed with R Studio, but you may want to re-install them if you are working with an older version. You may have to quit and restart R Studio a few times during this process."
  },
  {
    "objectID": "rmarkdown.html#cheat-sheet",
    "href": "rmarkdown.html#cheat-sheet",
    "title": "R Markdown",
    "section": "",
    "text": "There is a very handy 2-page ‘cheat sheet’ https://www.rstudio.com/wp-content/uploads/2015/02/rmarkdown-cheatsheet.pdf, which you can also access through R Studio under the Help menu: Help -&gt; Cheatsheets. There are several other links here too, including ggplot and dplyr."
  },
  {
    "objectID": "rmarkdown.html#create",
    "href": "rmarkdown.html#create",
    "title": "R Markdown",
    "section": "",
    "text": "So far, you’ve been working through the command line in a .R program file. As discussed in the R Fundamentals Chapter, the benefit of programming in a .R file is that we can share the program and associated files with other computers, including high-performance servers.\nAn R Markdown file is another special text file, with a .Rmd extension. It’s useful for generating reports with embedded code and visualizations. I use it as a virtual notepad when analyzing new data, and later convert it to Online Supplementary Material for published papers. The utility of the .Rmd file is easiest to understand by example.\nBecause an R Markdown file is just a text file with a .Rmd extension, we could just make a new text file and save it with the .Rmd extension. However, in RStudio, we can make a new R markdown file from the menu: File-&gt; New-&gt; R Markdown\nChoose Document from the left-hand side menu, give it a title, and make sure html is selected.\nThen click OK\nVery few elements are needed for a basic markdown file, and examples of these elements are provided within the R Markdown file that R Studio sets up for you.\nAt the top of this window is a little icon that says ‘Knit’. The Knit icon, knits together your text file into a rendered html document.\nTry clicking the knit icon to generate a report using the default text in the R Markdown file. Note that you may have to save a copy of your R Markdown file first. A new file will open in a web browser. Take a moment to compare the output file with the input R Markdown.\nIf you want to output this as a pdf file, then you can simply choose Print, and then Print to PDF in your web browser. You can also make pdf files directly from R Studio, but you might run into problems depending on your file content and format.\nWe’ll stick with the html version for now, and walk through some of the main components available to you."
  },
  {
    "objectID": "rmarkdown.html#yaml-header",
    "href": "rmarkdown.html#yaml-header",
    "title": "R Markdown",
    "section": "",
    "text": "Every R Markdown file starts with a YAML header, which contains some basic information about the file. A YAML header is generated automatically when you make a new .Rmd file in RStudio, but not all elements are needed. Depending on what options you choose, it might look something like this:\n\n---\ntitle: \"Untitled\"\nauthor: \"Robert I. Colautti\"\ndate: \"January 20, 2019\"\noutput: html_document\n---\n\nThere are other options available for YAML, and you can includes a separate _output.yml to set other aspects of the layout.\nHere are some additional formatting options. Replace the output: html_document line above:\n\noutput:\n  html_document: # Add options for html output\n    toc: true # Add table of contents (TOC)\n    number_sections: true # Add section numbers\n    toc_float: # Have TOC floating at the side\n      collapsed: false # Expand subsections"
  },
  {
    "objectID": "rmarkdown.html#markdown-elements",
    "href": "rmarkdown.html#markdown-elements",
    "title": "R Markdown",
    "section": "",
    "text": "The Markdown in R Markdown refers to the Markdown protocol (https://en.wikipedia.org/wiki/Markdown)\nThis is a non-proprietary system that was designed to quickly and easily encode formatted documents and websites in a simple text document.\nThe main advantage of R Markdown (.Rmd) over regular Markdown (.md) is the ability to easily print, format, and execute embedded R code for graphs, tables, and calculations.\nWe’ll look at some basic Markdown elements and then we’ll see how to embed R code for professional, reproducible reports."
  },
  {
    "objectID": "rmarkdown.html#basic-elements",
    "href": "rmarkdown.html#basic-elements",
    "title": "R Markdown",
    "section": "",
    "text": "Plain text is converted into paragraph format.\nTo start a new paragraph, press enter twice. This is important – if you only press enter once, then the two paragraphs will knit together into the same paragraph.\nSimilarly, if you inlcude more than two lines between paragraphs, these will be ignored when you render the R Markdown document.\nTry adding paragraphs of text separated by pressing enter 1, 2 and 3 times. Then, knit to html to see how these are rendered in the final output.\n\n\n\nYou can format text with * or _\n*italics* or _italics_: italics\n**bold** or __bold__: bold\nUse greater-than sign for block quotes, eg. &gt; TIP: quote\n\nTIP: quote"
  },
  {
    "objectID": "rmarkdown.html#headers",
    "href": "rmarkdown.html#headers",
    "title": "R Markdown",
    "section": "",
    "text": "Add headers with up to six hash marks (#). Each additional # denotes a sub-heading of the previous (sub)heading."
  },
  {
    "objectID": "rmarkdown.html#other-elements",
    "href": "rmarkdown.html#other-elements",
    "title": "R Markdown",
    "section": "",
    "text": "Use two dashes (--) for short–dash (a.k.a. ‘n-dash’).\nUse three dashes (---) for long — dash (a.k.a. m-dash)."
  },
  {
    "objectID": "rmarkdown.html#links",
    "href": "rmarkdown.html#links",
    "title": "R Markdown",
    "section": "",
    "text": "Links have a special format. The text you want the user to see goes in square brackets, followed immediately by the file or html link in regular brackets, with no space in between. You can use both web links and relative path links.\n[Colautti Lab Website](https://colauttilab.github.io/)\nThis should produce a link if you are reading this electronically:\nColautti Lab Website\nYou can also use this with relative path names, for example to link a file in a folder called images inside of the project folder:\n[Linked .png file](./images/ColauttiLabLogo.png)\nAgain, this should produce a link if you are reading this electronically. However, you will get an error if you try to include this in your R Markdown file.\n\nQuestion: Why will you get an error?\n\nAnswer: This file is not in your working directory. You will need to create an image and save it as ColauttiLabLogo.png inside a directory called images, which is also inside of the working directory that contains your .Rmd file."
  },
  {
    "objectID": "rmarkdown.html#images",
    "href": "rmarkdown.html#images",
    "title": "R Markdown",
    "section": "",
    "text": "Instead of linking, you can embed the image directly by adding an exclamation point. The text in square brackets becomes the figure caption.\n![Linked .png file](./images/ColauttiLabLogo.png):\n\n\n\nLinked .png file\n\n\nNote that R Markdown added a figure number for me, based on the chapter and the number of previous images. This doesn’t include the graphs that were created with embedded R code, only images that were embedded with ![]()."
  },
  {
    "objectID": "rmarkdown.html#lists",
    "href": "rmarkdown.html#lists",
    "title": "R Markdown",
    "section": "",
    "text": "Lists are easy to create, simply start a line with * or + for unordered lists or a number for ordered lists. Add tab characters for sub-lists:\n\n+ Unordered list item 1  \n* Item 2  \n  + sub item 2.1  \n  * sub item 2.2  \n* Item 3  \n\n\nUnordered list item 1\nItem 2\n\nsub item 2.1\nsub item 2.2\n\nItem 3\n\n\n1. Ordered list item 1  \n2. Item 2  \n  + sub item 2.1  \n  * sub item 2.2  \n3. Item 3  \n\n\nOrdered list item 1\n\nItem 2\n\n\n\nsub item 2.1\n\nsub item 2.2\n\n\n\nItem 3\n\nThe fun thing about ordered lists is the numbers you use don’t really matter – R Markdown will automatically start at 1 and increase for each item.\n\n1. Ordered list item 1  \n1. Item 2  \n  + sub item 2.1  \n  * sub item 2.2  \n1. Item 3  \n\n\nOrdered list item 1\n\nItem 2\n\n\n\nsub item 2.1\n\nsub item 2.2\n\n\n\nItem 3\n\nThis is a nice feature in early drafts of your R Markdown, to which you might later add, rename, or reorganize the order. You can just leave the numbers and not waste effort renumber each time."
  },
  {
    "objectID": "rmarkdown.html#tables",
    "href": "rmarkdown.html#tables",
    "title": "R Markdown",
    "section": "",
    "text": "Tables are added using a line of horizontal dashes to separate the title of the table, then a row of header names separated by pipes to define the header row. Finally, we add another line of dashes with pipes to indicate the relative column widths. The data goes underneath, with pipes separating each column.\n\nTables \n-----------------------\nDate  | Length  | Width\n------|---------|------\n09/09/09 | 14 | 27\n10/09/09 | 15 | 29\n11/09/09 | 16 | 31\n\nProduces this output:"
  },
  {
    "objectID": "rmarkdown.html#tables-1",
    "href": "rmarkdown.html#tables-1",
    "title": "R Markdown",
    "section": "",
    "text": "Date\nLength\nWidth\n\n\n\n\n09/09/09\n14\n27\n\n\n10/09/09\n15\n29\n\n\n11/09/09\n16\n31\n\n\n\nNow that we have written some basic markdown elements, we can generate our R markdown report to see what they look like. Click on the knit button and compare the input with what you have typed in your R mardown file."
  },
  {
    "objectID": "rmarkdown.html#embed-r-code",
    "href": "rmarkdown.html#embed-r-code",
    "title": "R Markdown",
    "section": "",
    "text": "You can format text to look like code using the back-tick character.\n\n# `Use single tick mark to invoke code formatted text.`\n\nThe Back Tick is a strange looking character usually located on the same key as the tilde (~) on English keyboards. Don’t confuse the back tick with the single quotation mark.\nYou can incorporate blocks of R code using three back ticks with r in curly brackets. Write some code, then add three more tick marks to signal the end of the code chunk. By defaule, your code will run when you convert your R Markdown to html, showing the output. This is a great way to include graphs and the output of statistical models.\n\n#   ```{r}\n#   &lt;your code goes here&gt;&gt;\n#   ```\n\nCtl-Alt-i is a nice shortcut in R Studio for adding code chunks quickly.\n\n\nYou can name your code chunks by adding a name right after the r separated only by a space. Naming code chunks is very handy for troubleshooting. When you knit your file, the name of each chunk is listed in the Render tab in R Studio. If there is an error, you can see which code chunk is causing the error. Note that the name cannot contain spaces, and it can be followed by a comma to specify options for the code chunk.\n\n# ```{r code-chunk-name, eval=F}`\n\n\n\n\nYou can use different options for your R code chunks, as shown on the R Markdown cheat sheet. The default is to both print the code AND show the output, but these can be changed with options. The three options I use most commonly are:\n\neval=F – Don’t evaluate the code. The code will be shown, but it won’t be run so no objects or output will be created.\necho=F – Don’t echo the code. Don’t show the code, but run it and include any output, plots, messages and warnings.\ninclude=F – Don’t include the code in the rendered document. Run the code to create any objects and load libraries, but don’t show the code or any output, plots, messages or warnings."
  },
  {
    "objectID": "rmarkdown.html#dynamic-tables",
    "href": "rmarkdown.html#dynamic-tables",
    "title": "R Markdown",
    "section": "",
    "text": "Making tables from data is a bit more complicated. For example, if we wanted to summarize the FallopiaData.csv data, we could read in the file and then summarize with dplyr as we did in the Data Science Chapter.\n\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.2\n\nFallo&lt;-read.csv(\n  \"https://colauttilab.github.io/RCrashCourse/FallopiaData.csv\")\n\nSumTable&lt;-Fallo %&gt;% \n  group_by(Taxon,Scenario,Nutrients) %&gt;%\n  summarize(Mean=mean(Total), SD=sd(Total)) %&gt;%\n  arrange(desc(Mean))\n\nprint(SumTable)\n\n# A tibble: 10 × 5\n# Groups:   Taxon, Scenario [10]\n   Taxon Scenario     Nutrients  Mean    SD\n   &lt;chr&gt; &lt;chr&gt;        &lt;chr&gt;     &lt;dbl&gt; &lt;dbl&gt;\n 1 bohem high         high       60.3  8.68\n 2 japon gradual      high       59.7  9.57\n 3 bohem fluctuations high       58.4  9.20\n 4 bohem extreme      high       58.3  7.34\n 5 bohem gradual      high       57.5  9.34\n 6 japon extreme      high       57.2 10.9 \n 7 japon high         high       56.4  8.20\n 8 japon fluctuations high       56.4 13.7 \n 9 japon low          low        52.0  8.29\n10 bohem low          low        48.0  8.86\n\n\nThe output is legible but not very attractive for a final report. To make it look better, we can use the kable function from the knitr package:\n\nlibrary(knitr)\n\nWarning: package 'knitr' was built under R version 4.3.2\n\nkable(SumTable, caption = \"Summary Table\")\n\n\nSummary Table\n\n\nTaxon\nScenario\nNutrients\nMean\nSD\n\n\n\n\nbohem\nhigh\nhigh\n60.28091\n8.677075\n\n\njapon\ngradual\nhigh\n59.72917\n9.565376\n\n\nbohem\nfluctuations\nhigh\n58.36455\n9.202334\n\n\nbohem\nextreme\nhigh\n58.30917\n7.337015\n\n\nbohem\ngradual\nhigh\n57.46154\n9.338311\n\n\njapon\nextreme\nhigh\n57.23643\n10.903133\n\n\njapon\nhigh\nhigh\n56.44833\n8.204091\n\n\njapon\nfluctuations\nhigh\n56.43692\n13.724906\n\n\njapon\nlow\nlow\n52.02917\n8.287938\n\n\nbohem\nlow\nlow\n47.98077\n8.862164\n\n\n\n\n\nThe output of this code is shown in the Summary Table. Contrast the formatting from the kable() function with the standard R output from print()."
  },
  {
    "objectID": "rmarkdown.html#embed-graphs",
    "href": "rmarkdown.html#embed-graphs",
    "title": "R Markdown",
    "section": "",
    "text": "Use R code to embed graphs.\n\nggplot() +\n  geom_histogram(aes(rnorm(100)), binwidth=0.2)"
  },
  {
    "objectID": "rmarkdown.html#content-as-tabs",
    "href": "rmarkdown.html#content-as-tabs",
    "title": "R Markdown",
    "section": "",
    "text": "Rendering an html has additional features that won’t show up in a pdf. The code below will produce tabs in your knit html file.\n\n## Tabs by Group {.tabset}\n\n### Treatment Group\n\n1. Treatment 1 explanation\n2. Treatment 2 explanation\n3. Treatment 3 explanation\n\n### Histogram\n\n#```{r}\n#library(ggplot2)\n#ggplot() + \n#  geom_histogram(aes(rnorm(100)),binwidth=0.2)\n#```\n\nThe output of this code is shown in the figure.\n{fig-alt=“Tabs created in html using {.tabset} with ## headers. The”Plot 1” tab contains embedded R code to produce a histogram graph. Both the code and graph are shown by default”}"
  },
  {
    "objectID": "rmarkdown.html#equations",
    "href": "rmarkdown.html#equations",
    "title": "R Markdown",
    "section": "",
    "text": "Insert equations using LaTeX equations. LaTeX is a system for converting basic text files to formated documents.\nUse single dollar signs for in-line equations, like $Y = X$, which will print as \\(Y = X\\) on the same line as the text.\n\nUse double dollar signs on a new line for full-line equations, like $$Y = X$$:\n\n\\[Y = X \\]\nwhich isolates the equation on its own line, and centers it.\nYou can use a variety of Greek letters by using the backslash character \\. For upper-case Greek letters, just use an upper-case letter in the spelling. For example:\n\n\\lambda(\\(\\lambda\\)) OR \\Lambda (\\(\\Lambda\\))\n\\gamma(\\(\\gamma\\)) OR \\Gamma(\\(\\Gamma\\))\n\\delta(\\(\\delta\\)) OR \\Delta(\\(\\Delta\\))\n\nOmicron can be \\omicron or simply the letter o, with no backslash o (\\(o\\)). Note that some LaTeX characters, like omicron, do not have capital versions (e.g., \\Omicron does not produce a Greek character).\nThere are numerous other options but below is a quick rundown of some of the commonly used scripts.\n\n\n\n\n\n\n\n\n\nScript\nDescription\nCode\nExample\n\n\n\n\n\\infty\nInfinity\n\\infty\n\\(\\infty\\)\n\n\n_\nSubscript\nX_i\n\\(X_i\\)\n\n\n^\nSuperscript\nX^2\n\\(X^2\\)\n\n\n'\nFirst order derivative\nf'(x)\n\\(f'(x)\\)\n\n\n''\nSecond order derivative\ns'(x)\n\\(s'(x)\\)\n\n\n\\sim\nPredict\nY \\sim X\n\\(Y \\sim X\\)\n\n\n\\times\nMultiply\nX \\times Y\n\\(X \\times Y\\)\n\n\n\\pm\nPlus or minus\nX \\pm Y\n\\(X \\pm Y\\)\n\n\n\\neq\nNote equal\nX \\neq Y\n\\(X \\neq Y\\)\n\n\n\\leq\nLess than or equal\nX \\leq Y\n\\(X \\leq Y\\)\n\n\n\\geq\nGreater than or equal\nX \\geq Y\n\\(X \\geq Y\\)\n\n\n{}\nGroup together\nX_{subscript}\n\\(X_{subscript}\\)\n\n\n\\sqrt\nSquare root\n\\sqrt{x^2y^2}\n\\(\\sqrt{x^2y^2}\\)\n\n\n\\frac\nFraction\n\\frac{X+1}{X-1}\n\\(\\frac{X+1}{X-1}\\)\n\n\n\\sum\nSum\n\\sum_{x=1}^{K}\n\\(\\sum_{x=1}^{K}\\)\n\n\n\\prod\nProduct\n\\prod_{x=1}^{K}\n\\(\\prod_{x=1}^{K}\\)\n\n\n\\int\nIntegral\n\\int_{0}^{\\infty}\n\\(\\int_{0}^{\\infty}\\)\n\n\n\\lim\nLimit\n\\lim_{x \\to \\infty}\n\\(\\lim_{x \\to \\infty}\\)\n\n\n\nNote in particular, the use of curly brackets to group items together in superscripts, subscripts, fractions and square root. Also note the simulate (\\sim) character, which is the tilde (~) used in statistical models and other R functions like facet_grid() and aggregate().\nHere are some more sophisticated examples to show how to create more complex equations. Again, try reproducing these in R markdown. If you don’t get the same output, then check to see what is different with your code.\n\n$$Y_i \\sim \\alpha + \\beta_1 X_i + \\epsilon_i$$ \n\nwill produce a linear model equation:\n\\[Y_i \\sim \\alpha + \\beta_1 X_i + \\epsilon_i\\]\nand\n\n$$sum_{n=1}^{\\infty} 2^{-n} = 1$$\n\nwill produce:\n\\[\\sum_{n=1}^{\\infty} 2^{-n} = 1\\]\nNote the use of special characters with the backslash \\, along with subscripts _ and superscripts ^ with text in curly brackets {}.\nThat’s all you need to know to produce professional reports with R and R Markdown!"
  },
  {
    "objectID": "rmarkdown.html#r-notebooks",
    "href": "rmarkdown.html#r-notebooks",
    "title": "R Markdown",
    "section": "",
    "text": "R Notebook files are a unique type of R Markdown file, but include subtle differences that probably won’t matter much to you at this point in your coding journey. You could create a notebook file in similar ways to R markdown, the easiest being to select File --&gt; New File --&gt; R Notebook from the R Studio menu. However, I suggest you don’t, for reasons that will become clear below.\nSaving an R notebook file adds the extension .nb to tell R Studio that it’s a notebook file. For example, if you had an R markdown file called MyAnalysis.Rmd the R notebook version would save as MyAnalysis.Rmd.nb. Additionally, an html file is created whenever you save the notebook file, whereas you have to manually ‘knit’ an R markdown file to produce an html output.\nWhen you knit a .Rmd document, you re-run all of the code, whereas the .Rmd.nb only updates the code that has changed. This can cause confusion if you change one part of the code but forget to update it, whereas knitting the whole document is a safer approach for new coders, and established coders like me who are very prone to typos and other errors. The disadvantage of knitting the whole document each time is that it takes longer. However, you can also break up a large analysis into separate Rmd files. There is also a better alternative to notebooks, as described in the next section."
  },
  {
    "objectID": "rmarkdown.html#quarto",
    "href": "rmarkdown.html#quarto",
    "title": "R Markdown",
    "section": "",
    "text": "Quarto is a recent addition to R Studio that also is based in R markdown but has a number of additional features. Again, I suggest you avoid using at this point because it has features that hide the underlying R Markdown code, which will make it harder for you to remember. As with R Markdown and R Notebooks, you can create a quarto file from the File --&gt; New File menu. However, you have two types of Quarto files: Documents and Presentations.\nAlthough they are based in R Markdown, they have additional commands that make quarto files different enough to get their own file extension: .qmd instead of .Rmd. The main advantage of Quarto is that it provides real-time rendering of the document inside of the R Studio window. When you open a Quarto file in R Studio, you will see options for Source and Visual at the top of the text editor window. Source shows the underlying code, whereas Visual gives the real-time document rendering. This means that you get an idea of the format and appearance of the document without having to save or knit the file each time.\nAt this stage of your learning, I suggest you avoid using Quarto and focus on R Markdown. Once you master R Markdown and the rest of this book, then you might want to move to Quarto and learn the additional coding options. At least for now, avoid using the Visual option with real-time rendering while you are coding a document. If you aren’t seeing the underlying code, then you are going to have trouble learning and retaining it.\n\nMy advice at this stage is to keep it simple. Focus on learning R Markdown first. As you get more comfortable coding, you could move to more advanced tools like Quarto."
  }
]