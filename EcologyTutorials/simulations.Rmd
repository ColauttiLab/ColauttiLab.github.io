---
title: |-
  Simulations in R
  <br> Bootstrap, Null Model & Permutation
author: "Rob Colautti"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 1. Introduction

A bootstrap model gets its name from the old 19th century expression to "lift yourself up by your own bootstraps", as a metaphor for accomplishing a task without external forces. In the context of data analysis, the term bootstrap refers to the generation of 'new' data from existing data. 

A very common use of a bootstrap model is to generate a null distribution from the data, to test whether the observed mean or variance is different than what we would expect by chance.

You may not realize it, but you have already taken the first step towards a bootstrap model in R by using the `sample()` function. 

Let's look at the data from the Global Garlic Mustard Field Survey. The data are available [here](../EcologyTutorials/GGMFS_Teaching_Data.csv). Details on this dataset are covered in the previous tutorial introducting climate models: [link](../EcologyTutorials/GGMFS_climate.html):

```{r}
GMdat<-read.csv("../EcologyTutorials/GGMFS_Teaching_Data.csv",header=T)
GMsubDat<-GMdat[,c("Longitude","Latitude","Region","Pop_Size")] # Subset to columns of interest
GMsubDat<-GMsubDat[complete.cases(GMsubDat),] # Remove missing data
```

## Parametric Assumptions

Let's calculate the __Mean__ and __95% CI__ for `Pop_Size`. In classic frequentist statistics, we might assume a normal distribution, and then calculate the __Mean__ and __Standard Error (SE)__. You may recall from intro stats that the mean of a random sample drawn from a Gaussian distribution will fall within ~1.96x the standard error of the mean, 95% of the time. This is a complicated way of saying that the parametric 95% CI of a random sample is Mean +/- 1.96*SE. You may also recall that the SE of a sample mean is __Standard Deviation (SD)__ divided by the square root of the sample size: $$\frac{SD}{\sqrt{(N-1)}}$$.
```{r}
# Calculate mean, SE and CI from parametric model
X<-mean(GMsubDat$Pop_Size)
SE<-sd(GMsubDat$Pop_Size)/sqrt(nrow(GMsubDat)-1)
CI<-c(X-SE*1.96,X+SE*1.96)

# Round and print values
X<-round(X)
CI<-round(CI)
paste("Raw mean is:",X)
paste("Raw 95% CI is:",CI[1],"to",CI[2])
```

This assumes a normal distribution Let's take a look at the distribution of `Pop_Size`, and then add the mean and 95% CI
```{r}
library(ggplot2)
qplot(Pop_Size,data=GMsubDat)+theme_bw()+geom_vline(aes(xintercept=X),colour="red")+geom_vline(aes(xintercept=CI[1]),colour="blue")+geom_vline(aes(xintercept=CI[2]),colour="blue")
```

The data highly right-skewed, ranging from near-zero to almost 1,000,000. They are clearly not normal, violating the assumption of the parametric estimate.

> How can we reduce the skew and improve assumptions of normality in the data?

## Transform

We can try to apply a transformation to make the data look more like a normal distribution:
```{r}
# Calculate mean and 95% CI on transformed data
GMsubDat$log_Size<-log(GMsubDat$Pop_Size)
Xlog<-mean(GMsubDat$log_Size)
SElog<-sd(GMsubDat$log_Size)/sqrt(nrow(GMsubDat)-1)
CIlog<-c(Xlog-SElog*1.96,Xlog+SElog*1.96)

# Plot data + mean + 95% CI
qplot(log_Size,data=GMsubDat)+theme_bw()+geom_vline(aes(xintercept=Xlog),colour="red")+geom_vline(aes(xintercept=CIlog[1]),colour="blue")+geom_vline(aes(xintercept=CIlog[2]),colour="blue")
```

Much better, but still a bit skewed. Let's back-transform the mean and CI calculated on the log-transformed data, and compare with the original
```{r}
# Original
cat("Raw mean is:",X,"\n",
    "Raw 95% CI is:",CI[1],"to",CI[2],"\n")

# Back-Transform from log-scale to original measurement scale
Xlog<-exp(Xlog)
CIlog<-exp(CIlog)

# Round 
Xlog<-round(Xlog)
CIlog<-round(CIlog)
cat("Transformed mean is:",Xlog,"\n",
    "Transformed 95% CI is:",CIlog[1],"to",CIlog[2],"\n")
```

Compare the raw vs. log-transformed values. That's more than an order of magnitude difference in the mean, and a much smaller 95% CI!

# 1. Bootstrap Basics

We can also use a bootstrap model estimate the mean and 95% CI directly from the data. The beauty of this approach is that it is not biased by the distribution of the data. To do this we:

  1. Resample the data, with replacement
  2. Calculate the mean 
  3. Repeat N times
  4. Calculate the mean of the N replicated sample means. This is the __bootstrap mean__.
  5. The 0.025*Nth smallest mean is the lower 95% CI
  6. The 0.975*Nth smallest mean is the upper 95% CI

We can do this fairly easily in R using `sample()` in a `for` loop and saving the output in a vector

```{r}
Iters<-999 # Number of iterations == the number ot times the mean is resampled 
BootOut<-rep(NA,Iters) # Make an empty vector with # cells = number of iterations
for (i in 1:Iters){
  # Resample the data, calculate the mean, and save it in a vector; repeat
  BootOut[i]<-mean(sample(GMsubDat$Pop_Size,nrow(GMsubDat),replace=T)) 
}
head(BootOut)
```

Now calculate the mean and CI from the sorted data (steps 4-6). Note that the values are drawn directly from the data.
```{r}
Xboot<-mean(BootOut)
CIlow<-sort(BootOut)[floor(length(BootOut)*0.025)] # Lower 2.5% CI
CIhigh<-sort(BootOut)[ceiling(length(BootOut)*0.975)] # Upper 2.5% CI
CIboot<-c(CIlow,CIhigh)
```

Plot for comparison. 
```{r}
# Plot
qplot(BootOut)+theme_bw()+geom_vline(aes(xintercept=Xboot),colour="red")+geom_vline(aes(xintercept=CIlow),colour="blue")+geom_vline(aes(xintercept=CIhigh),colour="blue")
```

> Why does the 95% CI seem so much wider on this plot than the previous two?

This is a plot of estimated population means from the bootstrap iterations, whereas the previous graphs plot the raw data. Therefore the x-axis scales are quite different.

## Compare methods

Let's look at all of them together:
```{r}
# Transform bootstrap values
Xboot<-round(Xboot)
CIboot<-round(CIboot)

cat("Raw mean is:",X,"\n",
    "Raw 95% CI is:",CI[1],"to",CI[2],"\n",
    "Transformed mean is:",Xlog,"\n",
    "Transformed 95% CI is:",CIlog[1],"to",CIlog[2],"\n",
    "Bootstrap mean is:",Xboot,"\n",
    "Bootstrap 95% CI is:",CIboot[1],"to",CIboot[2],"\n")
```

> Which method is most accurate and why?

# 2. Bootstrap: Effect Size

We can also use a bootstrap simulation as a non-parametric statistic. In the dataset on _Alliaria petiolata_ (garlic mustard) from the Global Garlic Mustard Field Survey there is a factor called __Region__, which tells us which continent the population is on. Let's test if the North American populations are larger than populations in Europe.

## Parametric Models

```{r}
# Raw data
RawMod<-lm(Pop_Size~Region,data=GMsubDat)
(RawCoef<-summary(RawMod)$coefficients)

# Log-transformed
LogMod<-lm(log_Size~Region,data=GMsubDat)
(LogCoef<-summary(LogMod)$coefficients)
```

Recall: The 'RegionNorthAm' parameter is the estimated difference in the mean of North American population size relative to Europe. The 2nd model is on a log-scale so it needs to be back-transformed.
```{r}
# Calculate effect size and parametric 95%CI, and round
RawES<-round(RawCoef[1])
RawCI<-round(c(RawCoef[1]-1.96*RawCoef[4],RawCoef[1]+1.96*RawCoef[4]))

# Calculate for log-transformed
LogES<-exp(LogCoef[1])
LogES<-round(LogES)
LogCI<-exp(c(LogCoef[1]-1.96*LogCoef[4],LogCoef[1]+1.96*LogCoef[4]))
LogCI<-round(LogCI)

cat("The effect size based on raw data is:",RawES,"\n",
    "with parametric 95% CI of",RawCI[1],"to",RawCI[2],"\n",
    "The effect size based on transformed data is:","\n",
    LogES,"with parametric 95% CI of",LogCI[1],"to",LogCI[2],"\n")
```

> What does the range of CI values tell you about the hypothesis that introduced populations (North America) are larger than populations in the native range (Europe)?

## Bootstrap model

We sample, with replacement, just as we did with the above bootstrap model. However, this time we sample separately for each region and calculate the difference in the mean each time

  1. Resample the data __from each region__, with replacement
  2. Calculate the __difference between means__ 
  3. Repeat N times
  4. Calculate the mean of the N replicated samples. This is the __bootstrap effect size__.
  5. The 0.025*Nth smallest mean is the lower 95% CI
  6. The 0.975*Nth smallest mean is the upper 95% CI

```{r}
Iters<-999 # Number of iterations == the number ot times the mean is resampled 
BootESOut<-rep(NA,Iters) # Make an empty vector with # cells = number of iterations
# Make separate EU/NA groups for resampling
EUpops<-GMsubDat$Pop_Size[grep("Europe",GMsubDat$Region)]
NApops<-GMsubDat$Pop_Size[grep("NorthAm",GMsubDat$Region)]
for (i in 1:Iters){
  # Resample the data, calculate the mean, and save it in a vector; repeat
  BootESOut[i]<-
    mean(sample(NApops,length(NApops),replace=T)) -
    mean(sample(EUpops,length(EUpops),replace=T))
}
head(BootESOut)
```

Look at the frequency histogram. 
```{r}
qplot(BootESOut)+theme_bw()
```

Compare the results

```{r}
# Calculate mean & 95% CI
XbootES<-round(mean(BootESOut))
CIlowES<-sort(BootESOut)[floor(length(BootESOut)*0.025)] # Lower 2.5% CI
CIhighES<-sort(BootESOut)[ceiling(length(BootESOut)*0.975)] # Upper 2.5% CI
CIbootES<-round(c(CIlow,CIhigh))

# Compare results
cat("The effect size based on raw data is:",RawES,"\n",
    "with parametric 95% CI of",RawCI[1],"to",RawCI[2],"\n",
    "The effect size based on transformed data is:",LogES,"\n",
    "with parametric 95% CI of",LogCI[1],"to",LogCI[2],"\n",
    "The effect size of the bootstrap model is:",XbootES,"\n",
    "with 95% CI of",CIbootES[1],"to",RawCI[2],"\n")
```

> What do the x- and y-axis represent, and why does the distribution have this shape?

# 3. Bootstrap Null Models

In the above model, we know our effect size is significant -- i.e. North American populations are significantly bigger than native populations. But, there are two problems:

  1. We don't know the exact P-value. 
  2. The above model wouldn't work for __constrained data__. For example, what if we wanted to test the significance of eigenvalues from a principal components model? Eigenvalues are __constrained__ to be greater than zero. Since we calculate 95% CI from resampled data, rather than a parametric model, we can't just check if our 95% CI overlap zero.
  
Here's where a null model comes in handy. We first define and simulate a null model, then compare our observed value(s) to the null model.

Defining the null model is a little more tricky -- you have to think in terms of the null hypothesis. In our example, the null hypothesis is that there is no difference in the size of European and North American populations. To generate a null distribution:

  1. Resample the __region codes__, __with replacement__
  2. Calculate the __difference between means__ 
  3. Repeat N times
  4. Compare the observed effect size to the null distribution
  5. The p-value is proportion of values in the null model greater/less than the observed effect size (or divide by 2 for a 2-tailed test)
  
```{r}
Iters<-999 # Number of iterations == the number ot times the mean is resampled 
PermOut<-rep(NA,Iters) # Make an empty vector with # cells = number of iterations
SimDat<-GMsubDat # Make a new dataset for resampling
for (i in 1:Iters){
  # 1. Reshuffle Region labels (sample, without replacement)
  SimDat$Region<-sample(SimDat$Region,replace=F)
  # 2. Calculate effect size and save in output vector
  PermOut[i]<-
    mean(SimDat$Pop_Size[grep("NorthAm",SimDat$Region)]) -
    mean(SimDat$Pop_Size[grep("Europe",SimDat$Region)])
}
head(PermOut)
```

## Plot Null

```{r}
# Calculate mean difference of raw data
(RawX<-aggregate(GMsubDat$Pop_Size,list(GMsubDat$Region),mean))
RawDiff<-RawX$x[2]-RawX$x[1]
qplot(PermOut)+theme_bw()+geom_vline(aes(xintercept=RawDiff),colour="red")
```

## Calculate p-value

The p-value is the probability of obtaining the observed value by chance. In this case the observed value is the observed difference in the mean `RawDiff`, above. The null model simulates 'observed values obtained by chance'. So the estimated p-value is the proportion of values less than the observed value. 

```{r}
p<-sum(sort(PermOut)>RawDiff)/length(PermOut)
paste("The estimated p-value is:",p)
```

If the observed value is lower than all of the simulated values, then the best we can say is: 
$$p < \frac{1}{N_{iters}}$$ 
    
where `Niters` is the total number of iterations. So p < 0.001 for 1001 iterations

# 4. Bootstrap Caveats

It's important to think carefully about your bootstrap and null models and the implicit assumptions you make when you run them. For example, we assume that each observation has the same probability of being chosen in each iteration. This wouldn't be true if there was some sort of inherent correlation among data points. For example, the data we are working with were sampled non-randomly.
```{r}
qplot(Longitude,Latitude,data=GMdat,alpha=I(0.3))+theme_bw()
```

We may also have two or more factors in a model that are correlated. For example, multiple traits measured on the same individuals or the same traits measured on the same individuals but at different time points. In this kind of 'repeated measures' experiments, should run the bootstrap and null model tests at the level of individual, not observation. That is, reshuffle ID tags for a null model, leaving the rest of the row/column data untouched.

# 5. Simulating data

A simulation is a very handy method to use when generating toy data for generating model predictions or when
trying to understand the performance of statistical models. In fact, modern statistics publications often use simulations to test the validity of statistical models under different assumptions of the data (e.g. non-random sampling, heteroskedasticity).

Let's consider an experiment testing the effect of garlic mustard on the root colonization beneficial mycorrhizal fungi, conducted at Queen's University Biological Station (QUBS). In this experiment, 6 different native __species__ were found to grow <strong>in</strong>side or <strong>out</strong>side of garlic mustard populations, at each of 11 __locations__. Roots from each species were harvested to look at colonization by beneficial <strong>Myco</strong>rrhizae and <strong>path</strong>ogens.

Our goal here is to simulate data, which we will use first to generate model predictions, and then to test the performance of generalized linear mixed models.

## Model Predictions

### Hypothesis 1

__Hypothesis 1:__ Mycorrhizal colonization protects roots from lesions at the point of colonization, resulting in a negative correlation between the two. 

To create a model, we will simulate roots of individual plants. %Myc and %Path are calculated by examining 100 root cross-sections. We can simulate each cross-section with a probability of finding (or not finding) the other as a joint probability.

```{r}
NXsec<-100 # Number of cross-sections per plant
NPlants<-1000 # Number of plants
Pm<-0.2 # Probability of Mycorrhizae colonizing any given x-section
Pp<-Pm # Probability of Pathogens colonizing any give x-section 
Pc<-0.5 # Probability of finding Pathogens given Mycorrhizae have colonized at the same cross section. Interference if Pc < 1, facilitation if Pc > 1. The value of 0.5 means Pathogens have only half the expected colonization rate when Mycorrhizae are present.
Root<-data.frame(Myc=rep(NA,NPlants),Path=rep(NA,NPlants)) # Data for saving output
for(Pl in 1:NPlants){
  tempMyc<-sample(1:0,NXsec,replace=T,prob=c(Pm,1-Pm)) # Generate Myc colonization
  tempPath<-rep(NA,NXsec) # Empty vector of pathogen infection

  # Simulate root cross-sections. 
  if(length(tempPath[tempMyc==0])>0){
    tempPath[tempMyc==0]<-sample(1:0,length(tempPath[tempMyc==0]),replace=T,prob=c(Pp,1-Pp))  
  }
  if(length(tempPath[tempMyc==1])>0){
    tempPath[tempMyc==1]<-sample(1:0,length(tempPath[tempMyc==1]),replace=T,prob=c(Pp*Pc,1-Pp*Pc))
  }

  # Calculate % colonization
  Root$Myc[Pl]<-sum(tempMyc)
  Root$Path[Pl]<-sum(tempPath)
  
  # Clear temporary vectors to avoid errors in future iterations
  tempMyc<-tempPath<-NA
}
head(Root)
```

Compare means and run a linear model.
```{r}
mean(Root$Myc)
mean(Root$Path)
summary(lm(Path~Myc,data=Root))
```

Plot the % colonization of the simulated plants
```{r}
qplot(Myc,Path,data=Root,alpha=I(0.3))+theme_bw()+geom_smooth(method="lm")
```

Note that the slope of the line at the whole-plant level is much less steep than in our model at the individual point of colonization within a plant's root. In our original model, pathogen colonization was reduced by half (20%-->1%) if Mycorrhizae had already colonized the same root cross-section. Therefore we might predict that a 1% increase in Mycorrhizal colonization at the whole-plant level would reduce Pathogen colonization by 0.5%. Compare the slope of the linear model (above) to the prediction of -0.5

### Hypothesis 2

Garlic mustard reduces mycorrhizal colonization.

In terms of our simulation model, this means:

  * GM reduces the colonization probability of Mycorrhizae but not Pathogens (i.e. Pm reduced but not Pp)

By changing these parameters we can simulate 'high' garlic mustard densities vs. 'low' density or absence. The easiest way to do this is to move the code above into a function, add a 'return' statement, and then run it for different parameters representing the two cases:
```{r}
GMsim<-function(NXsec=100,NPlants=1000,Pm=0.2,Pp=0.2,Pc=0.5){
  Root<-data.frame(Myc=rep(NA,NPlants),Path=rep(NA,NPlants)) # Data for saving output
  for(i in 1:NPlants){
    tempMyc<-sample(1:0,NXsec,replace=T,prob=c(Pm,1-Pm)) # Generate Myc colonization
    tempPath<-rep(NA,NXsec) # Empty vector of pathogen infection

    # Simulate root cross-sections. 
    if(length(tempPath[tempMyc==0])>0){
      tempPath[tempMyc==0]<-sample(1:0,length(tempPath[tempMyc==0]),replace=T,prob=c(Pp,1-Pp))  
    }
    if(length(tempPath[tempMyc==1])>0){
      tempPath[tempMyc==1]<-sample(1:0,length(tempPath[tempMyc==1]),replace=T,prob=c(Pp*Pc,1-Pp*Pc))
    }

    # Calculate % colonization
    Root$Myc[i]<-sum(tempMyc)
    Root$Path[i]<-sum(tempPath)
    
    # Clear temporary vectors to avoid errors in future iterations
    tempMyc<-tempPath<-NA
  }
  return(Root) ## Don't forget to return this to the user!
}
```

Now it's a simple matter of calling the function with new parameters. Since we put the original values as default parameters, we only need to change Pm.

```{r}
GMlow<-GMsim()
GMhigh<-GMsim(Pm=0.1)
# Add labels and combine for analysis & visualization
GMhigh$Dens<-"High"
GMlow$Dens<-"Low"
simDat<-rbind(GMlow,GMhigh)
head(simDat)
```

> Predict what the plot will look like (means and slope for each group)

Analyze results and plot. 
```{r}
summary(lm(Path~Myc*Dens,data=simDat))
qplot(Myc,Path,data=simDat,colour=Dens,alpha=I(0.3))+theme_bw()+geom_smooth(method="lm")
```

> Note the significant slope, but no significant effect of density. Why?

### Hypothesis 3

Garlic mustard reduces mycorrhizal colonization and reduces localized protection.

In terms of our simulation model, there are actually two sub-hypotheses here:

  1. GM reduces colonization for Mycorrhizae but not Pathogens (i.e. __Pm__ reduced but not __Pp__), as above.
  2. When Mycorrhizae are present, pathogens have higher probability (i.e. __Pc__ closer to __Pp__)

Again, this is easily accomplished by tweaking parameters. We keep the same __Pm__ values from the previous model and increase (double) the __Pc__ for the high-density GM model.
```{r}
GMlow<-GMsim(Pm=0.2,Pc=0.5) # Same parameters as above
GMhigh<-GMsim(Pm=0.1,Pc=1) # When GM is high: Myc colonize at 50% the rate and don't affect Pathogen colonization
# Add labels and combine for analysis & visualization
GMhigh$Dens<-"High"
GMlow$Dens<-"Low"
simDat<-rbind(GMlow,GMhigh)
# Analyze and plot
summary(lm(Path~Myc*Dens,data=simDat))
qplot(Myc,Path,data=simDat,colour=Dens,alpha=I(0.3))+theme_bw()+geom_smooth(method="lm")
```

You should see different slopes, and depeding on your simulated values you may even see a positive slope in the high-density case.

# 6. Simulations for Statistical Model

Now what if you wanted to check the __performance__ of the simple linear model we have been using? By __performance__ I mean the ability to detect an effect when present and reject when not present? For example, using the P < 0.05 convention we might want to check if the Type I error rate is <= 0.05% while also minimizing the Type II error rate for a range of parameter values. We can do this with a nested simulation:

  1. Run our simulated data for a given set of parameter values
  2. Run the linear model and extract the relvant P-value
  3. Repeat N times
  4. Examine the distribution of P-values  
  and/or
  5. Sort P-values and check # simulations where P < 0.05
  
To do this for the parameters/model above, we put the code into another `for` loop and save the P-values to a data.frame. 

> WARNING: This could take a while to run. Explain why.

```{r}
Loops<-100
Pout<-data.frame(Myc=rep(NA,Loops),Dens=rep(NA,Loops),Int=rep(NA,Loops))
for(i in 1:Loops){
GMlow<-GMsim(Pm=0.2,Pc=0.5)
GMhigh<-GMsim(Pm=0.1,Pc=1)
  # Add labels and combine for analysis & visualization
  GMhigh$Dens<-"High"
  GMlow$Dens<-"Low"
  simDat<-rbind(GMlow,GMhigh)
  # Analyze and save
  Pout[i,]<-summary(lm(Path~Myc*Dens,data=simDat))$coefficients[14:16]
}
head(Pout)
```

## Type II Error Rates

Calculate false-negative rates as the proportion of simulations with P > 0.05
```{r}
sum(Pout$Myc>0.05)/length(Pout$Myc) # Mycorrhizae effect
sum(Pout$Dens>0.05)/length(Pout$Dens) # GM Density Effect
sum(Pout$Int>0.05)/length(Pout$Int) # Interaction Effect
```

> What do these error rates mean? What does this tell us about the lm() model we are using

Plot the distribution of P-values from each factor, with a reference line at P < 0.05
```{r}
qplot(Myc,data=Pout)+geom_vline(aes(xintercept=0.05),colour="red")+theme_bw() # Distribution of P-values for Myc slope
qplot(Dens,data=Pout)+geom_vline(aes(xintercept=0.05),colour="red")+theme_bw() # Distribution of P-values for GM density effect
qplot(Int,data=Pout)+geom_vline(aes(xintercept=0.05),colour="red")+theme_bw() # Distribution of P-values for density*Myc interaction
```

Our Type II error rates tell us the `lm()` model is failing to reject the null for the interaction term about 50% of the time and Myc and Dens main effects about 90% the time!  Without any parameter differences we should get a P-value greater than 0.05 about 95% of the time. The `lm()` model is essentially garbage for detecting the effect that we are simulating.

> How would we test Type I Error Rates?

## Type I Error Rates

Type I error occurs when a statistical model rejects a null hypothesis even though it is true. By convention, it is more desirable for statistical tests to decrease Type I error at the expense of increasing Type II error.

To test Type I error, we simply set parameters to values that represent no effect. That is, Pm=Pc and the same parameters for GMhigh and GMlow
```{r}
Loops<-100
Pout<-data.frame(Myc=rep(NA,Loops),Dens=rep(NA,Loops),Int=rep(NA,Loops))
for(i in 1:Loops){
  GMhigh<-GMsim(Pm=0.2,Pc=0.5)
  GMlow<-GMsim(Pm=0.1,Pc=1)
  # Add labels and combine for analysis & visualization
  GMhigh$Dens<-"High"
  GMlow$Dens<-"Low"
  simDat<-rbind(GMlow,GMhigh)
  # Analyze and save
  Pout[i,]<-summary(lm(Path~Myc*Dens,data=simDat))$coefficients[14:16]
}
head(Pout)
```

The Type I Error rates occur when P < 0.05, which we expect to occur in about 5% of simulations, just by chance.

```{r}
sum(Pout$Myc<0.05)/length(Pout$Myc) # Mycorrhizae effect
sum(Pout$Dens<0.05)/length(Pout$Dens) # GM Density Effect
sum(Pout$Int<0.05)/length(Pout$Int) # Interaction Effect
```

> What do these error rates mean? What does this tell us about the lm() model we are using?

These Type I Error rates are about what we expect.

## Nested(Nested(Sim))

Now imagine you wanted to test the Type II error rates of the `lm()` across a range of parameter values. For example, we might want to test:

  1. Is it better to measure more individual plants, or more cross-sections per plant?
  2. Are Type II errors more common when overall colonization rates (Pm==Pp) are low?
  3. Do interactions increase Type II error rates?

These are just 3 examples that we may come up with. To answer these, we can wrap our simulated simulations inside another simulation. Let's do this for the 3rd example. We could try to put the above simulation inside another function, and run again for a range of paramter values. But let's keep it simple and just compare cases with and without interactions

```{r}
Loops<-100
PoutNoInt<-data.frame(Myc=rep(NA,Loops),Dens=rep(NA,Loops),Int=rep(NA,Loops))
for(i in 1:Loops){
  GMlow<-GMsim(Pm=0.2,Pc=0.5)
  GMhigh<-GMsim(Pm=0.1,Pc=0.5)
  # Add labels and combine for analysis & visualization
  GMhigh$Dens<-"High"
  GMlow$Dens<-"Low"
  simDat<-rbind(GMlow,GMhigh)
  # Analyze and save
  PoutNoInt[i,]<-summary(lm(Path~Myc*Dens,data=simDat))$coefficients[14:16]
}
Loops<-100
PoutInt<-data.frame(Myc=rep(NA,Loops),Dens=rep(NA,Loops),Int=rep(NA,Loops))
for(i in 1:Loops){
  GMlow<-GMsim(Pm=0.2,Pc=0.5)
  GMhigh<-GMsim(Pm=0.1,Pc=1)
  # Add labels and combine for analysis & visualization
  GMhigh$Dens<-"High"
  GMlow$Dens<-"Low"
  simDat<-rbind(GMlow,GMhigh)
  # Analyze and save
  PoutInt[i,]<-summary(lm(Path~Myc*Dens,data=simDat))$coefficients[14:16]
}
```

Now compare the Type II Error rates. 
```{r}
cat("Error rates for No-Interaction Model\n",
      "Myc:",sum(PoutNoInt$Myc>0.05)/length(PoutNoInt$Myc),"\n",
      "Dens:",sum(PoutNoInt$Dens>0.05)/length(PoutNoInt$Dens),"\n",
    "Error rates for Interaction Model\n",
      "I-Myc:",sum(PoutInt$Myc>0.05)/length(PoutInt$Myc),"\n",
      "I-Dens:",sum(PoutInt$Dens>0.05)/length(PoutInt$Dens),"\n",
      "I-Int:",sum(PoutInt$Int>0.05)/length(PoutInt$Int),"\n")
```

Here we can see that the `lm()` test still under-performs (Type II >> 0.05), but also that the biological interaction reduces the ability of `lm()` to detect the Mycorrhizal effect (I-Myc >> Myc). 

A nice next-step could be to calculate error rates across a range of sample sizes and effect sizes and visualize the results. For example, instead of just interactions vs no-interaction you might imagine plots of error rates separately for each of the three parameters (Myc, Dens, Int), with each graph having:

  * __x-axis:__ Difference or ratio of Pm in high vs. low GM density
  * __y-axis:__ Difference or ratio of Pc in high vs. low GM density
  * __z-axis:__ (i.e. colour or point size): Error rate

# 7. Better Statistics

Now that simulations show us that we should avoid `lm()` models, we might want to rethink our analysis. One option would be to try more complicated models like Generalized Linear Models (GLM) or Generalized Mixed Models (GLMM). But those can take a long time to run -- to long to try our simulations. It would help to first think about why `lm()` is performing so poorly.

By collapsing all of the root cross-sections to a single value (% colonization), we lose the associations at the root intersection level. Therefore, instead of measuring % colonization, we can try to infer colonization probabilities and the interference term directly from the data.

Consider a 2x2 contingency table of presence/absence probabilities under the assumption of no interactions between Mycorrhizae and Pathogens:

![Probabilites](./contingency_prob.svg)

If there is an interaction between Mycorrhizal and pathogen colonization, then we expect to see a deviation due to interference of Mycorrhizae on Pathogens, but we might also want to consider the reverse -- interference of pathogens on Mycorrhizal colonization. We can roughly estimate an interference parameter (I) as a deviation in the number of co-occurrance observations from the Pp*Pm prediction. We can then test whether I is significantly different from zero, and whether it differs in high vs. low GM patches.

There are some nuanced problems with this approach, but we can always use simulation models to test the performance of this new model to see if we are missing something important.

## Modified function

We have to modify our custom function to keep track of I instead of % colonization of roots:
```{r}
GMIsim<-function(NXsec=100,NPlants=1000,Pm=0.2,Pp=0.2,Pc=0.5){
  Ivals<-data.frame(I=rep(NA,NXsec)) # Vector for saving output
  for(i in 1:NPlants){
    tempMyc<-sample(1:0,NXsec,replace=T,prob=c(Pm,1-Pm)) # Generate Myc colonization
    tempPath<-rep(NA,NXsec) # Empty vector of pathogen infection

    # Simulate root cross-sections. 
    if(length(tempPath[tempMyc==0])>0){
      tempPath[tempMyc==0]<-sample(1:0,length(tempPath[tempMyc==0]),replace=T,prob=c(Pp,1-Pp))  
    }
    if(length(tempPath[tempMyc==1])>0){
      tempPath[tempMyc==1]<-sample(1:0,length(tempPath[tempMyc==1]),replace=T,prob=c(Pp*Pc,1-Pp*Pc))
    }

    # Count # categories
    tempMPobs<-sum(tempMyc==1 & tempPath==1) # Count of observed co-occurrences
    # Number of prediced co-occrrences is a bit tricky. 
    ## 1. Sum each sole occurrence and divide by total observations to calculate individual probabilities
    ## 2. Multiply probabilities together to calculate predicted joint probability
    ## 3. Multiply by total N observations to calculate predicted number of co-occurrences
    tempMPpred<-sum(tempMyc==1 & tempPath==0)*sum(tempMyc==0 & tempPath==1)/NXsec 
    # Now calculate I as the deviation of observed from predicted
    
    # Calculate I
    if(tempMPpred > 0){ # Avoid errors when tempMPpred == 0
      Ivals[i,]<-tempMPobs/tempMPpred  
    }
    
    
    # Clear temporary vectors to avoid errors in future iterations
    tempMyc<-tempPath<-tempMPobs<-tempMPpred<-NA
  }
  return(Ivals)
}
```

## Type II Error Rates

Let's test the Type II Error Rates for the new model, given how poorly the old `lm()` version performed. Note that we have to modify the new `lm()` for the interaction parameter (I), then run the loop and check the Type II error rate. If it still doesn't look good, we can write a bootstrap/null model for the I-statistic.
```{r}
Loops<-100
Pout<-data.frame(Int0=rep(NA,Loops),Dens=rep(NA,Loops))
for(i in 1:Loops){
  GMhigh<-GMIsim(Pm=0.1,Pc=0.5)
  GMlow<-GMIsim(Pm=0.2,Pc=1)
  # Add labels and combine for analysis & visualization
  GMhigh$Dens<-"High"
  GMlow$Dens<-"Low"
  simDat<-rbind(GMlow,GMhigh)
  # Analyze and save
  Pout[i,]<-summary(lm(I~Dens,data=simDat))$coefficients[7:8]
}
head(Pout)
```

Calculate false-negative rates as the proportion of simulations with P > 0.05. 
```{r}
sum(Pout$Int0>0.05)/length(Pout$Int0) # Test of whether I is different from zero
sum(Pout$Dens>0.05)/length(Pout$Dens) # Test if I differs for high vs low GM
```

Now we are getting no Type II errors at all -- no false negatives. That means the Type II error rate is < 0.01 (less than 1 in 100 simulated results). We could increase the number of simulations to 1000 or 10000 but it will take a while to run, so we won't do it here.

> IMPORTANT: The interepretation of these parameters is different from the previous lm(). Explain why.

Plot the distribution of P-values from each factor, with a reference line at P < 0.05
```{r}
qplot(Int0,data=Pout)+geom_vline(aes(xintercept=0.05),colour="red")+theme_bw() # Distribution of P-values
qplot(Dens,data=Pout)+geom_vline(aes(xintercept=0.05),colour="red")+theme_bw() # Distribution of P-values
```

## Type I Error Rates

We should check Type I error rates too. 
```{r}
Loops<-100
Pout<-data.frame(Int0=rep(NA,Loops),Dens=rep(NA,Loops))
for(i in 1:Loops){
  GMhigh<-GMIsim(Pc=1)
  GMlow<-GMIsim(Pc=1)
  # Add labels and combine for analysis & visualization
  GMhigh$Dens<-"High"
  GMlow$Dens<-"Low"
  simDat<-rbind(GMlow,GMhigh)
  # Analyze and save
  Pout[i,]<-summary(lm(I~Dens,data=simDat))$coefficients[7:8]
}
head(Pout)
```

Calculate false-positive rates as the proportion of simulations with P < 0.05. 
```{r}
sum(Pout$Int0<0.05)/length(Pout$Int0) # Test of whether I is different from zero
sum(Pout$Dens<0.05)/length(Pout$Dens) # Test if I differs for high vs low GM
```

Now there is a low false-positive rate when testing whether I differs, but a high rate when testing whether I is different from zero. 

Plot the distribution of P-values from each factor, with a reference line at P < 0.05
```{r}
qplot(Int0,data=Pout)+geom_vline(aes(xintercept=0.05),colour="red")+theme_bw() # Distribution of P-values
qplot(Dens,data=Pout)+geom_vline(aes(xintercept=0.05),colour="red")+theme_bw() # Distribution of P-values
```

This approach with `lm()` appears to work pretty well if we want to test whether I differs between populations, but not if we want to check if I is different from 0. For that, we may want to write a bootstrap/null model function instead.

## Bootstrap null model

```{r}
Mboot<-function(simDatI=NA,Iters=1000){
  nullI<-rep(NA,Iters)
  for (i in 1:Iters){
    nullI[i]<-mean(sample(simDatI,length(simDatI),replace=T))
  }
  # 2-tailed test
  P<-sum(abs(nullI) > mean(simDatI))/Iters
  if(P > 0){
    return(P)
  } else {
    return(1/Iters)
  }
}
```

Now run the bootstrap in place of the `lm()`. This may be a bit slow given the number of iterations in all of the simulations.
```{r}
Loops<-100
Pout<-data.frame(Int0=rep(NA,Loops))
for(i in 1:Loops){
  GMhigh<-GMIsim(Pc=1)
  GMlow<-GMIsim(Pc=1)
  # Add labels and combine for analysis & visualization
  GMhigh$Dens<-"High"
  GMlow$Dens<-"Low"
  simDat<-rbind(GMlow,GMhigh)
  # Analyze and save
  Pout[i,]<-Mboot(simDatI=simDat$I)
}
head(Pout)
```
```{r}
sum(Pout$Int0<0.05)/length(Pout$Int0) # Test of whether I is different from zero
qplot(Int0,data=Pout)+geom_vline(aes(xintercept=0.05),colour="red")+theme_bw() # Distribution of P-values
```

With the bootstrap model, we now have a Type I error rate < 0.01. Let's think about why the `lm()` didn't work but the bootstrap did. 

It will help to take a look at the raw data, by running one iteration of the loop and examinig the raw data (mean I values) instead of the P values from the `lm()`.

```{r}
GMhigh<-GMIsim(Pc=1)
GMlow<-GMIsim(Pc=1)
# Add labels and combine for analysis & visualization
GMhigh$Dens<-"High"
GMlow$Dens<-"Low"
simDat<-rbind(GMlow,GMhigh)
# Analyze and save

qplot(I,data=simDat,fill=Dens)+theme_bw()
```

Re-run the above script a few times to look at distributions from the simulations.

> What do you notice about the distribution? What does it suggest about testing the significance of I with linear models vs. bootstrap?

# 8. Constrained values

Recall that ___I___ is a ratio (observed/predicted). This is a good example of __constrained data__ because a ratio of count data can never be less than zero. By generating a null distribution above, we circumvented this problem. Another approach is to take the log of the ratio (log-odds ratio). This is equivalent to log(observed) - log(predicted) and since log of values < 1 are negative, the distribution should have better properties. We can either do this in a new function, or just transform the values after calling the function.

```{r}
qplot(log(I+1)-1,data=simDat,fill=Dens)+theme_bw() # Note log(I+1) to account for obs=0, and then subtract 1 to centre mean back at zero
```

> Exercise: Write a simulation to test the Type I and Type II errors on this new log-scale variable: log(I)

# 9. Permutation Test

The permutation test is a good alternative to the null distribution. The principal is very similar, but instead of resampling the raw data and re-calculating a mean or effect size, we resample the data and run a statistical model each time. The null distribution is then built from the F-value or other parameter from the statistical test.

First, generate 1 iteration of random data:
```{r}
GMhigh<-GMIsim(Pc=1)
GMlow<-GMIsim(Pc=1)
# Add labels and combine for analysis & visualization
GMhigh$Dens<-"High"
GMlow$Dens<-"Low"
simDat<-rbind(GMlow,GMhigh)
```

Permute by re-shuffling Density labels, running `lm()` and recording the F-statistic
```{r}
X<-summary(lm(I~Dens,data=simDat))$fstatistic[1]
Iters<-100
Xperm<-rep(NA,Iters)
for(i in 1:Iters){
  simDens<-sample(simDat$Dens,nrow(simDat),replace=F)
  Xperm[i]<-summary(lm(simDat$I~simDens))$fstatistic[1]
}

sum(X>Xperm)/Iters # P-value
qplot(Xperm)+geom_vline(aes(xintercept=0.05),colour="red")+theme_bw()
```

The `lmPerm` package does this for `lm()`, saving a few lines of code:
```{r}
library(lmPerm)
X<-summary(lm(I~Dens,data=simDat))$fstatistic[1]
Xperm<-summary(lmp(I~Dens,data=simDat),perm="Prob")
```

> Exercise: Test the Type I and Type II errors for the the two ways of doing a permutation test described above. Compare the errors from the two methods.

# 10. Make it faster

Once you understand the basic coding for bootraps and permutation tests, you can look into available packages to reduce the amount of code you need to write and have them run faster. 

The `lmPerm` package is the permutation equivalent of `lm()`

The `boot` package has tools for writing bootstrap models

The `foreach` and `doParallel` packages allow you to use multithreading in your functions and for-loops.






